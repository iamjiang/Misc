{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e7a5b02",
   "metadata": {},
   "source": [
    "<span style=\"font-weight: bold; font-size: 18px;\">**Multi-Label Posture Classification: Model Development Strategy**<br><br>\n",
    "\n",
    "We propose a comparative evaluation of two complementary modeling approaches to address the multi-label posture prediction task, each offering distinct advantages for legal document classification.\n",
    "\n",
    "**Baseline Approach: Bag-of-Words Models**<br>\n",
    "\n",
    "Our initial baseline leverages traditional bag-of-words representations (TF-IDF, BM25) combined with multi-label classifiers, justified by several key factors:\n",
    "\n",
    "<div style=\"margin-left: 20px;\"><b>‚Ä¢ Computational Efficiency:</b> Lightweight architecture enables rapid prototyping and establishes performance baselines without GPU requirements</div>\n",
    "<div style=\"margin-left: 20px;\"><b>‚Ä¢ Statistical Robustness:</b> Word-frequency features provide interpretable, domain-agnostic representations suitable for legal terminology analysis</div>\n",
    "<div style=\"margin-left: 20px;\"><b>‚Ä¢ Multi-Label Compatibility:</b> Well-established integration with multi-label algorithms (One-vs-Rest, Binary Relevance, Label Powerset)</div>\n",
    "<div style=\"margin-left: 20px;\"><b>‚Ä¢ Baseline Establishment:</b> Provides interpretable performance benchmarks for evaluating more complex architectures</div>\n",
    "\n",
    "**Advanced Approach: Transformer-Based Models (ModernBERT)**<br>\n",
    "\n",
    "Our primary model leverages ModernBERT encoder architecture, specifically designed to address the limitations of traditional BERT for our use case:\n",
    "\n",
    "<div style=\"margin-left: 20px;\"><b>‚Ä¢ Extended Context Coverage:</b> ModernBERT's 8,192-token context window accommodates ~90% of our corpus without truncation, preserving critical legal context that may span entire documents</div>\n",
    "\n",
    "<div style=\"margin-left: 20px;\"><b>‚Ä¢ Contextual Understanding:</b> Unlike bag-of-words approaches, transformer architectures capture:\n",
    "  <div style=\"margin-left: 40px;\">- Long-range dependencies between legal arguments</div>\n",
    "  <div style=\"margin-left: 40px;\">- Positional relationships between procedural elements</div>\n",
    "  <div style=\"margin-left: 40px;\">- Semantic nuances distinguishing similar posture categories</div>\n",
    "</div>\n",
    "\n",
    "<div style=\"margin-left: 20px;\"><b>‚Ä¢ Multi-Label Architecture:</b> The encoder's [CLS] token representation can be effectively coupled with multi-label classification heads, enabling simultaneous prediction of multiple postures</div>\n",
    "\n",
    "<div style=\"margin-left: 20px;\"><b>‚Ä¢ Legal Domain Adaptation:</b> Pre-trained language understanding provides superior handling of complex legal terminology and document structure</div>\n",
    "\n",
    "**Comparative Justification:**<br>\n",
    "\n",
    "This dual-approach strategy enables comprehensive evaluation of feature representation impact on multi-label performance, ranging from traditional statistical methods to state-of-the-art contextual understanding, ultimately identifying the optimal balance between computational efficiency and classification accuracy for legal posture prediction.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce6340b",
   "metadata": {},
   "source": [
    "## Data Preparation for ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f537d7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56de0440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents with postures: 17077\n"
     ]
    }
   ],
   "source": [
    "# Prepare the labels - convert postures to a list format\n",
    "def prepare_labels(postures_str):\n",
    "    \"\"\"Convert posture string to list of postures\"\"\"\n",
    "    if pd.isna(postures_str) or postures_str == '':\n",
    "        return []\n",
    "    return [p.strip() for p in postures_str.split(',') if p.strip()]\n",
    "\n",
    "# Apply to dataframe\n",
    "_dir=os.path.join(os.getcwd(),\"processed_data\")\n",
    "df=pd.read_pickle(os.path.join(_dir, \"data.pkl\"))\n",
    "df['posture_list'] = df['postures'].apply(prepare_labels)\n",
    "\n",
    "# Remove documents with no postures\n",
    "df_ml = df[df['posture_list'].apply(len) > 0].copy()\n",
    "print(f\"Documents with postures: {len(df_ml)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b115e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total unique postures: 230\n",
      "\n",
      "Most common postures:\n",
      "On Appeal                                                         9197\n",
      "Appellate Review                                                  4652\n",
      "Review of Administrative Decision                                 2773\n",
      "Motion to Dismiss                                                 1679\n",
      "Sentencing or Penalty Phase Motion or Objection                   1342\n",
      "Trial or Guilt Phase Motion or Objection                          1097\n",
      "Motion for Attorney's Fees                                         612\n",
      "Post-Trial Hearing Motion                                          512\n",
      "Motion for Preliminary Injunction                                  364\n",
      "Motion to Dismiss for Lack of Subject Matter Jurisdiction          343\n",
      "Motion to Compel Arbitration                                       255\n",
      "Motion for New Trial                                               226\n",
      "Petition to Terminate Parental Rights                              219\n",
      "Motion for Judgment as a Matter of Law (JMOL)/Directed Verdict     212\n",
      "Motion for Reconsideration                                         206\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Analyze posture distribution\n",
    "all_postures_ml = []\n",
    "for postures in df_ml['posture_list']:\n",
    "    all_postures_ml.extend(postures)\n",
    "\n",
    "posture_counts = pd.Series(all_postures_ml).value_counts()\n",
    "print(f\"\\nTotal unique postures: {len(posture_counts)}\")\n",
    "print()\n",
    "print(f\"Most common postures:\")\n",
    "print(posture_counts.head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a11f4a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Postures with >= 100 occurrences: 27\n",
      "['On Appeal', 'Appellate Review', 'Review of Administrative Decision', 'Motion to Dismiss', 'Sentencing or Penalty Phase Motion or Objection', 'Trial or Guilt Phase Motion or Objection', \"Motion for Attorney's Fees\", 'Post-Trial Hearing Motion', 'Motion for Preliminary Injunction', 'Motion to Dismiss for Lack of Subject Matter Jurisdiction', 'Motion to Compel Arbitration', 'Motion for New Trial', 'Petition to Terminate Parental Rights', 'Motion for Judgment as a Matter of Law (JMOL)/Directed Verdict', 'Motion for Reconsideration', 'Motion to Dismiss for Lack of Personal Jurisdiction', 'Motion for Costs', 'Juvenile Delinquency Proceeding', 'Motion for Default Judgment/Order of Default', 'Motion to Dismiss for Lack of Standing', 'Motion to Dismiss for Lack of Jurisdiction', 'Motion to Transfer or Change Venue', 'Petition for Divorce or Dissolution', 'Motion for Contempt', 'Motion for Protective Order', 'Motion for Permanent Injunction', 'Motion to Set Aside or Vacate']\n"
     ]
    }
   ],
   "source": [
    "# Filter to most common postures (those appearing in at least 100 documents)\n",
    "min_frequency = 100\n",
    "common_postures = posture_counts[posture_counts >= min_frequency].index.tolist()\n",
    "print(f\"\\nPostures with >= {min_frequency} occurrences: {len(common_postures)}\")\n",
    "print(common_postures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c65e4e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents after filtering to common postures: 16568\n",
      "Label matrix shape: (16568, 27)\n",
      "Labels: ['Appellate Review' 'Juvenile Delinquency Proceeding'\n",
      " \"Motion for Attorney's Fees\" 'Motion for Contempt' 'Motion for Costs'\n",
      " 'Motion for Default Judgment/Order of Default'\n",
      " 'Motion for Judgment as a Matter of Law (JMOL)/Directed Verdict'\n",
      " 'Motion for New Trial' 'Motion for Permanent Injunction'\n",
      " 'Motion for Preliminary Injunction' 'Motion for Protective Order'\n",
      " 'Motion for Reconsideration' 'Motion to Compel Arbitration'\n",
      " 'Motion to Dismiss' 'Motion to Dismiss for Lack of Jurisdiction'\n",
      " 'Motion to Dismiss for Lack of Personal Jurisdiction'\n",
      " 'Motion to Dismiss for Lack of Standing'\n",
      " 'Motion to Dismiss for Lack of Subject Matter Jurisdiction'\n",
      " 'Motion to Set Aside or Vacate' 'Motion to Transfer or Change Venue'\n",
      " 'On Appeal' 'Petition for Divorce or Dissolution'\n",
      " 'Petition to Terminate Parental Rights' 'Post-Trial Hearing Motion'\n",
      " 'Review of Administrative Decision'\n",
      " 'Sentencing or Penalty Phase Motion or Objection'\n",
      " 'Trial or Guilt Phase Motion or Objection']\n"
     ]
    }
   ],
   "source": [
    "## Multi-label Classification Setup\n",
    "\n",
    "# Filter documents to only include those with common postures\n",
    "def filter_common_postures(posture_list, common_postures):\n",
    "    \"\"\"Keep only postures that are in the common_postures list\"\"\"\n",
    "    return [p for p in posture_list if p in common_postures]\n",
    "\n",
    "df_ml['filtered_postures'] = df_ml['posture_list'].apply(\n",
    "    lambda x: filter_common_postures(x, common_postures)\n",
    ")\n",
    "\n",
    "# Remove documents that have no common postures after filtering\n",
    "df_ml = df_ml[df_ml['filtered_postures'].apply(len) > 0].copy()\n",
    "print(f\"Documents after filtering to common postures: {len(df_ml)}\")\n",
    "\n",
    "# Create binary label matrix using MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "y_multilabel = mlb.fit_transform(df_ml['filtered_postures'])\n",
    "\n",
    "print(f\"Label matrix shape: {y_multilabel.shape}\")\n",
    "print(f\"Labels: {mlb.classes_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26a1650d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_df238 caption {\n",
       "  color: red;\n",
       "  font-size: 15px;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_df238\">\n",
       "  <caption>Distribution of num_postures</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_df238_level0_col0\" class=\"col_heading level0 col0\" >count</th>\n",
       "      <th id=\"T_df238_level0_col1\" class=\"col_heading level0 col1\" >percentage</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >num_postures</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_df238_level0_row0\" class=\"row_heading level0 row0\" >1</th>\n",
       "      <td id=\"T_df238_row0_col0\" class=\"data row0 col0\" >7,649</td>\n",
       "      <td id=\"T_df238_row0_col1\" class=\"data row0 col1\" >46.17%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_df238_level0_row1\" class=\"row_heading level0 row1\" >2</th>\n",
       "      <td id=\"T_df238_row1_col0\" class=\"data row1 col0\" >7,567</td>\n",
       "      <td id=\"T_df238_row1_col1\" class=\"data row1 col1\" >45.67%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_df238_level0_row2\" class=\"row_heading level0 row2\" >3</th>\n",
       "      <td id=\"T_df238_row2_col0\" class=\"data row2 col0\" >1,127</td>\n",
       "      <td id=\"T_df238_row2_col1\" class=\"data row2 col1\" >6.80%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_df238_level0_row3\" class=\"row_heading level0 row3\" >4</th>\n",
       "      <td id=\"T_df238_row3_col0\" class=\"data row3 col0\" >189</td>\n",
       "      <td id=\"T_df238_row3_col1\" class=\"data row3 col1\" >1.14%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_df238_level0_row4\" class=\"row_heading level0 row4\" >5</th>\n",
       "      <td id=\"T_df238_row4_col0\" class=\"data row4 col0\" >32</td>\n",
       "      <td id=\"T_df238_row4_col1\" class=\"data row4 col1\" >0.19%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_df238_level0_row5\" class=\"row_heading level0 row5\" >6</th>\n",
       "      <td id=\"T_df238_row5_col0\" class=\"data row5 col0\" >2</td>\n",
       "      <td id=\"T_df238_row5_col1\" class=\"data row5 col1\" >0.01%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_df238_level0_row6\" class=\"row_heading level0 row6\" >7</th>\n",
       "      <td id=\"T_df238_row6_col0\" class=\"data row6 col0\" >2</td>\n",
       "      <td id=\"T_df238_row6_col1\" class=\"data row6 col1\" >0.01%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f7c45d730e0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_counts = df_ml['num_postures'].value_counts(dropna=False)\n",
    "_pct = df_ml['num_postures'].value_counts(dropna=False,normalize=True) \n",
    "\n",
    "pd.DataFrame({\n",
    "    'count': _counts,\n",
    "    'percentage': _pct\n",
    "}).sort_index().style.format({'count':'{:,}','percentage':'{:.2%}'}).set_caption(\"Distribution of num_postures\")\\\n",
    "    .set_table_styles([{'selector': 'caption','props': [('color', 'red'),('font-size', '15px')]}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b3bf28e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 16568\n",
      "Training set: 11597 (70.00%)\n",
      "Validation set: 2485 (15.00%)\n",
      "Test set: 2486 (15.00%)\n"
     ]
    }
   ],
   "source": [
    "# Prepare text data\n",
    "X_text = df_ml['full_text'].values\n",
    "\n",
    "# Split the data\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X_text, y_multilabel, \n",
    "    test_size=0.3, # 30% for temp (which will be split into val and test)\n",
    "    random_state=42, \n",
    "    stratify=None\n",
    ")\n",
    "\n",
    " # Split temp into validation and test (50-50 split of the 30%)\n",
    "# # This gives us 15% each\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp,\n",
    "    test_size=0.5,\n",
    "    random_state=42, \n",
    "    stratify=None\n",
    ")\n",
    "\n",
    "print(f\"Total samples: {len(df_ml)}\")\n",
    "print(f\"Training set: {len(X_train)} ({len(X_train)/len(df_ml):.2%})\")\n",
    "print(f\"Validation set: {len(X_val)} ({len(X_val)/len(df_ml):.2%})\")\n",
    "print(f\"Test set: {len(X_test)} ({len(X_test)/len(df_ml):.2%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3e4c020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label distribution in training set:\n",
      "Appellate Review: 3310 (28.5%)\n",
      "Juvenile Delinquency Proceeding: 103 (0.9%)\n",
      "Motion for Attorney's Fees: 412 (3.6%)\n",
      "Motion for Contempt: 88 (0.8%)\n",
      "Motion for Costs: 121 (1.0%)\n",
      "Motion for Default Judgment/Order of Default: 101 (0.9%)\n",
      "Motion for Judgment as a Matter of Law (JMOL)/Directed Verdict: 147 (1.3%)\n",
      "Motion for New Trial: 156 (1.3%)\n",
      "Motion for Permanent Injunction: 73 (0.6%)\n",
      "Motion for Preliminary Injunction: 254 (2.2%)\n",
      "Motion for Protective Order: 73 (0.6%)\n",
      "Motion for Reconsideration: 145 (1.3%)\n",
      "Motion to Compel Arbitration: 179 (1.5%)\n",
      "Motion to Dismiss: 1155 (10.0%)\n",
      "Motion to Dismiss for Lack of Jurisdiction: 82 (0.7%)\n",
      "Motion to Dismiss for Lack of Personal Jurisdiction: 138 (1.2%)\n",
      "Motion to Dismiss for Lack of Standing: 87 (0.8%)\n",
      "Motion to Dismiss for Lack of Subject Matter Jurisdiction: 231 (2.0%)\n",
      "Motion to Set Aside or Vacate: 73 (0.6%)\n",
      "Motion to Transfer or Change Venue: 88 (0.8%)\n",
      "On Appeal: 6404 (55.2%)\n",
      "Petition for Divorce or Dissolution: 82 (0.7%)\n",
      "Petition to Terminate Parental Rights: 167 (1.4%)\n",
      "Post-Trial Hearing Motion: 372 (3.2%)\n",
      "Review of Administrative Decision: 1940 (16.7%)\n",
      "Sentencing or Penalty Phase Motion or Objection: 953 (8.2%)\n",
      "Trial or Guilt Phase Motion or Objection: 776 (6.7%)\n"
     ]
    }
   ],
   "source": [
    "# Check label distribution\n",
    "train_label_sums = y_train.sum(axis=0)\n",
    "val_label_sums = y_val.sum(axis=0)\n",
    "test_label_sums = y_test.sum(axis=0)\n",
    "\n",
    "print(\"\\nLabel distribution in training set:\")\n",
    "for i, label in enumerate(mlb.classes_):\n",
    "    print(f\"{label}: {train_label_sums[i]} ({train_label_sums[i]/len(y_train)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5177e708",
   "metadata": {},
   "outputs": [],
   "source": [
    "## save preprocess data\n",
    "saved_data=os.path.join(os.getcwd(), 'processed_data')\n",
    "os.makedirs(saved_data, exist_ok=True)\n",
    "# Save using pickle\n",
    "with open(os.path.join(saved_data,'train_arrays.pkl'), 'wb') as f:\n",
    "    pickle.dump({'X_train': X_train, 'y_train': y_train}, f)\n",
    "\n",
    "with open(os.path.join(saved_data,'val_arrays.pkl'), 'wb') as f:\n",
    "    pickle.dump({'X_val': X_val, 'y_val': y_val}, f)\n",
    "\n",
    "with open(os.path.join(saved_data,'test_arrays.pkl'), 'wb') as f:\n",
    "    pickle.dump({'X_test': X_test, 'y_test': y_test}, f)\n",
    "\n",
    "print(\"All arrays saved with pickle!\")\n",
    "\n",
    "# To load later:\n",
    "# with open(os.path.join(saved_data,'train_arrays.pkl'), 'rb') as f:\n",
    "#     train_data = pickle.load(f)\n",
    "#     X_train = train_data['X_train']\n",
    "#     y_train = train_data['y_train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3804c25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4273a31e",
   "metadata": {},
   "source": [
    "## Bag-of-word (TFIDF): Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8244ef62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from lightgbm import early_stopping, log_evaluation\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, hamming_loss\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, f1_score, \n",
    "    roc_auc_score, average_precision_score,\n",
    "    hamming_loss, jaccard_score\n",
    ")\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aef25990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting TF-IDF vectorizer...\n",
      "TF-IDF matrix shape (train): (11597, 10000)\n",
      "TF-IDF matrix shape (val): (2485, 10000)\n",
      "TF-IDF matrix shape (test): (2486, 10000)\n",
      "Vocabulary size: 10000\n",
      "\n",
      "Sample features: ['00' '000' '000 00' '000 000' '001' '01' '010' '02' '020' '03' '030' '04'\n",
      " '040' '05' '06' '07' '08' '09' '10' '10 000']\n",
      "Last features: ['years prior' 'years prison' 'years supervised' 'yes' 'yes sir' 'yield'\n",
      " 'york' 'york city' 'york county' 'york law' 'york state' 'young'\n",
      " 'younger' 'youth' 'zba' 'zero' 'zone' 'zoning' 'zoning board'\n",
      " 'zoning ordinance']\n"
     ]
    }
   ],
   "source": [
    "# Create TF-IDF vectorizer\n",
    "# Using parameters optimized for legal text\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=10000,  # Limit features for computational efficiency\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 2),  # Include unigrams and bigrams\n",
    "    min_df=5,           # Ignore terms that appear in fewer than 5 documents\n",
    "    max_df=0.95,        # Ignore terms that appear in more than 95% of documents\n",
    "    sublinear_tf=True   # Apply sublinear scaling\n",
    ")\n",
    "\n",
    "print(\"Fitting TF-IDF vectorizer...\")\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_val_tfidf = tfidf.transform(X_val)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "print(f\"TF-IDF matrix shape (train): {X_train_tfidf.shape}\")\n",
    "print(f\"TF-IDF matrix shape (val): {X_val_tfidf.shape}\")\n",
    "print(f\"TF-IDF matrix shape (test): {X_test_tfidf.shape}\")\n",
    "print(f\"Vocabulary size: {len(tfidf.vocabulary_)}\")\n",
    "\n",
    "# Show some sample features\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "print(f\"\\nSample features: {feature_names[:20]}\")\n",
    "print(f\"Last features: {feature_names[-20:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92f09a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_evaluation(y_true, y_pred_proba, y_pred_binary=None, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation for multi-label classification with all averaging methods\n",
    "    \"\"\"\n",
    "    if y_pred_binary is None:\n",
    "        y_pred_binary = (y_pred_proba >= threshold).astype(int)\n",
    "    \n",
    "    metrics = {}\n",
    "    \n",
    "    # SAMPLES AVERAGE (per-sample then average across samples)\n",
    "    metrics['precision_samples'] = precision_score(y_true, y_pred_binary, average='samples', zero_division=0)\n",
    "    metrics['recall_samples'] = recall_score(y_true, y_pred_binary, average='samples', zero_division=0)\n",
    "    metrics['f1_samples'] = f1_score(y_true, y_pred_binary, average='samples', zero_division=0)\n",
    "    \n",
    "    # MICRO AVERAGE (global aggregation)\n",
    "    metrics['precision_micro'] = precision_score(y_true, y_pred_binary, average='micro', zero_division=0)\n",
    "    metrics['recall_micro'] = recall_score(y_true, y_pred_binary, average='micro', zero_division=0)\n",
    "    metrics['f1_micro'] = f1_score(y_true, y_pred_binary, average='micro', zero_division=0)\n",
    "    \n",
    "    # MACRO AVERAGE (unweighted average across labels)\n",
    "    metrics['precision_macro'] = precision_score(y_true, y_pred_binary, average='macro', zero_division=0)\n",
    "    metrics['recall_macro'] = recall_score(y_true, y_pred_binary, average='macro', zero_division=0)\n",
    "    metrics['f1_macro'] = f1_score(y_true, y_pred_binary, average='macro', zero_division=0)\n",
    "    \n",
    "    # WEIGHTED AVERAGE (weighted by support/frequency)\n",
    "    metrics['precision_weighted'] = precision_score(y_true, y_pred_binary, average='weighted', zero_division=0)\n",
    "    metrics['recall_weighted'] = recall_score(y_true, y_pred_binary, average='weighted', zero_division=0)\n",
    "    metrics['f1_weighted'] = f1_score(y_true, y_pred_binary, average='weighted', zero_division=0)\n",
    "    \n",
    "    # ROC-AUC (multiple averaging methods)\n",
    "    try:\n",
    "        metrics['roc_auc_macro'] = roc_auc_score(y_true, y_pred_proba, average='macro')\n",
    "        metrics['roc_auc_weighted'] = roc_auc_score(y_true, y_pred_proba, average='weighted')\n",
    "        metrics['roc_auc_samples'] = roc_auc_score(y_true, y_pred_proba, average='samples')\n",
    "    except ValueError as e:\n",
    "        print(f\"ROC-AUC calculation failed: {e}\")\n",
    "        metrics['roc_auc_macro'] = 0.0\n",
    "        metrics['roc_auc_weighted'] = 0.0\n",
    "        metrics['roc_auc_samples'] = 0.0\n",
    "    \n",
    "    # Precision-Recall AUC (multiple averaging methods)\n",
    "    try:\n",
    "        metrics['pr_auc_macro'] = average_precision_score(y_true, y_pred_proba, average='macro')\n",
    "        metrics['pr_auc_weighted'] = average_precision_score(y_true, y_pred_proba, average='weighted')\n",
    "        metrics['pr_auc_samples'] = average_precision_score(y_true, y_pred_proba, average='samples')\n",
    "    except ValueError as e:\n",
    "        print(f\"PR-AUC calculation failed: {e}\")\n",
    "        metrics['pr_auc_macro'] = 0.0\n",
    "        metrics['pr_auc_weighted'] = 0.0\n",
    "        metrics['pr_auc_samples'] = 0.0\n",
    "    \n",
    "    # Hamming Loss (inherently micro-averaged)\n",
    "    metrics['hamming_loss'] = hamming_loss(y_true, y_pred_binary)\n",
    "    \n",
    "    # Jaccard Score (multiple averaging methods)\n",
    "    metrics['jaccard_samples'] = jaccard_score(y_true, y_pred_binary, average='samples', zero_division=0)\n",
    "    metrics['jaccard_macro'] = jaccard_score(y_true, y_pred_binary, average='macro', zero_division=0)\n",
    "    metrics['jaccard_weighted'] = jaccard_score(y_true, y_pred_binary, average='weighted', zero_division=0)\n",
    "    \n",
    "    # Note: micro average for Jaccard in multi-label is not directly supported in sklearn\n",
    "    # but can be calculated manually if needed\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c020c01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define models to test with optimized hyperparameters and validation-aware training\n",
    "# models = {\n",
    "#     'Logistic Regression': OneVsRestClassifier(\n",
    "#         LogisticRegression(\n",
    "#             random_state=42, \n",
    "#             max_iter=1000,\n",
    "#             C=1.0,\n",
    "#             solver='liblinear'\n",
    "#         )\n",
    "#     ),\n",
    "#     'Random Forest': OneVsRestClassifier(\n",
    "#         RandomForestClassifier(\n",
    "#             n_estimators=100, \n",
    "#             random_state=42, \n",
    "#             n_jobs=-1,\n",
    "#             max_depth=10,\n",
    "#             min_samples_split=5,\n",
    "#             min_samples_leaf=2,\n",
    "#             # Additional overfitting control\n",
    "#             min_impurity_decrease=0.0001,\n",
    "#             max_features='sqrt'\n",
    "#         )\n",
    "#     ),\n",
    "#     'XGBoost': OneVsRestClassifier(\n",
    "#         xgb.XGBClassifier(\n",
    "#             random_state=42,\n",
    "#             n_estimators=100,\n",
    "#             max_depth=6,\n",
    "#             learning_rate=0.1,\n",
    "#             subsample=0.8,\n",
    "#             colsample_bytree=0.8,\n",
    "#             eval_metric='logloss',\n",
    "#             verbosity=0,\n",
    "#             # Early stopping will be handled in training loop\n",
    "#             early_stopping_rounds=10\n",
    "#         )\n",
    "#     ),\n",
    "#     'LightGBM': OneVsRestClassifier(\n",
    "#         lgb.LGBMClassifier(\n",
    "#             random_state=42,\n",
    "#             n_estimators=100,\n",
    "#             max_depth=6,\n",
    "#             learning_rate=0.1,\n",
    "#             subsample=0.8,\n",
    "#             colsample_bytree=0.8,\n",
    "#             verbosity=-1,\n",
    "#             # Early stopping will be handled in training loop\n",
    "#             early_stopping_rounds=10\n",
    "#         )\n",
    "#     )\n",
    "# }\n",
    "\n",
    "# # Enhanced training function with validation monitoring\n",
    "# def train_with_validation_control(model, X_train, y_train, X_val, y_val, model_name):\n",
    "#     \"\"\"\n",
    "#     Train model with validation monitoring to control overfitting\n",
    "#     \"\"\"\n",
    "#     print(f\"\\nTraining {model_name} with validation control...\")\n",
    "    \n",
    "#     if model_name in ['XGBoost', 'LightGBM']:\n",
    "#         # For tree-based models, we can use early stopping\n",
    "#         if model_name == 'XGBoost':\n",
    "#             # XGBoost with early stopping\n",
    "#             for i, estimator in enumerate(model.estimators_):\n",
    "#                 print(f\"  Training label {i+1}/{len(model.estimators_)}\")\n",
    "                \n",
    "#                 # Get single label\n",
    "#                 y_train_single = y_train[:, i]\n",
    "#                 y_val_single = y_val[:, i]\n",
    "                \n",
    "#                 # Only train if there are positive samples\n",
    "#                 if y_train_single.sum() > 0:\n",
    "#                     estimator.fit(\n",
    "#                         X_train, y_train_single,\n",
    "#                         eval_set=[(X_val, y_val_single)],\n",
    "#                         verbose=False\n",
    "#                     )\n",
    "#                 else:\n",
    "#                     # For labels with no positive samples, create a dummy classifier\n",
    "#                     estimator.fit(X_train[:10], y_train_single[:10])\n",
    "        \n",
    "#         elif model_name == 'LightGBM':\n",
    "#             # LightGBM with early stopping\n",
    "#             for i, estimator in enumerate(model.estimators_):\n",
    "#                 print(f\"  Training label {i+1}/{len(model.estimators_)}\")\n",
    "                \n",
    "#                 # Get single label\n",
    "#                 y_train_single = y_train[:, i]\n",
    "#                 y_val_single = y_val[:, i]\n",
    "                \n",
    "#                 # Only train if there are positive samples\n",
    "#                 if y_train_single.sum() > 0:\n",
    "#                     estimator.fit(\n",
    "#                         X_train, y_train_single,\n",
    "#                         eval_set=[(X_val, y_val_single)],\n",
    "#                         callbacks=[\n",
    "#                             early_stopping(10, verbose=False),\n",
    "#                             log_evaluation(0)  # No logging\n",
    "#                         ]\n",
    "#                     )\n",
    "#                 else:\n",
    "#                     # For labels with no positive samples, create a dummy classifier\n",
    "#                     estimator.fit(X_train[:10], y_train_single[:10])\n",
    "#     else:\n",
    "#         # For other models, use regular training\n",
    "#         model.fit(X_train, y_train)\n",
    "    \n",
    "#     return model\n",
    "\n",
    "# # Store results with validation tracking\n",
    "# results = {}\n",
    "# validation_scores = {}\n",
    "\n",
    "# print(\"Training and evaluating models with validation control...\")\n",
    "# print(\"=\"*60)\n",
    "# print(\"Models to evaluate:\")\n",
    "# for name in models.keys():\n",
    "#     print(f\"  ‚Ä¢ {name}\")\n",
    "# print()\n",
    "\n",
    "# for name, model in models.items():\n",
    "#     # Train with validation control\n",
    "#     if name in ['XGBoost', 'LightGBM']:\n",
    "#         # For tree-based models, we need to handle OneVsRestClassifier manually\n",
    "#         # to implement early stopping properly\n",
    "#         trained_model = OneVsRestClassifier(\n",
    "#             model.estimator,\n",
    "#             n_jobs=1  # Sequential to handle early stopping\n",
    "#         )\n",
    "#         trained_model.fit(X_train_tfidf, y_train)\n",
    "#     else:\n",
    "#         trained_model = model\n",
    "#         trained_model.fit(X_train_tfidf, y_train)\n",
    "    \n",
    "#     # Make predictions on all sets\n",
    "#     y_pred_train = trained_model.predict(X_train_tfidf)\n",
    "#     y_pred_val = trained_model.predict(X_val_tfidf)\n",
    "#     y_pred_test = trained_model.predict(X_test_tfidf)\n",
    "    \n",
    "#     # Calculate metrics for all sets\n",
    "#     train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "#     val_accuracy = accuracy_score(y_val, y_pred_val)\n",
    "#     test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "    \n",
    "#     train_hamming = hamming_loss(y_train, y_pred_train)\n",
    "#     val_hamming = hamming_loss(y_val, y_pred_val)\n",
    "#     test_hamming = hamming_loss(y_test, y_pred_test)\n",
    "    \n",
    "#     # Calculate F1 scores\n",
    "#     train_f1_micro = f1_score(y_train, y_pred_train, average='micro')\n",
    "#     val_f1_micro = f1_score(y_val, y_pred_val, average='micro')\n",
    "#     test_f1_micro = f1_score(y_test, y_pred_test, average='micro')\n",
    "    \n",
    "#     # Store results\n",
    "#     results[name] = {\n",
    "#         'model': trained_model,\n",
    "#         'train_accuracy': train_accuracy,\n",
    "#         'val_accuracy': val_accuracy,\n",
    "#         'test_accuracy': test_accuracy,\n",
    "#         'train_hamming_loss': train_hamming,\n",
    "#         'val_hamming_loss': val_hamming,\n",
    "#         'test_hamming_loss': test_hamming,\n",
    "#         'train_f1_micro': train_f1_micro,\n",
    "#         'val_f1_micro': val_f1_micro,\n",
    "#         'test_f1_micro': test_f1_micro,\n",
    "#         'y_pred_test': y_pred_test,\n",
    "#         'y_pred_val': y_pred_val\n",
    "#     }\n",
    "    \n",
    "#     # Check for overfitting\n",
    "#     accuracy_gap = train_accuracy - val_accuracy\n",
    "#     f1_gap = train_f1_micro - val_f1_micro\n",
    "    \n",
    "#     overfitting_status = \"‚úÖ Good\" if accuracy_gap < 0.05 else \"‚ö†Ô∏è Moderate\" if accuracy_gap < 0.1 else \"üö® High\"\n",
    "    \n",
    "#     print(f\"\\n{name} Results:\")\n",
    "#     print(f\"  Train Accuracy: {train_accuracy:.4f}\")\n",
    "#     print(f\"  Val Accuracy:   {val_accuracy:.4f}\")\n",
    "#     print(f\"  Test Accuracy:  {test_accuracy:.4f}\")\n",
    "#     print(f\"  Train-Val Gap:  {accuracy_gap:.4f} ({overfitting_status})\")\n",
    "#     print(f\"  Train F1:       {train_f1_micro:.4f}\")\n",
    "#     print(f\"  Val F1:         {val_f1_micro:.4f}\")\n",
    "#     print(f\"  Test F1:        {test_f1_micro:.4f}\")\n",
    "#     print(f\"  F1 Gap:         {f1_gap:.4f}\")\n",
    "\n",
    "# print(\"\\n\" + \"=\"*80)\n",
    "# print(\"Model Comparison with Overfitting Analysis:\")\n",
    "# print(f\"{'Model':<15} | {'Test Acc':<8} | {'Val Acc':<8} | {'Gap':<6} | {'Status':<12} | {'Performance':<12}\")\n",
    "# print(\"-\" * 85)\n",
    "\n",
    "# # Sort results by validation accuracy (better indicator than test accuracy)\n",
    "# sorted_results = sorted(results.items(), key=lambda x: x[1]['val_accuracy'], reverse=True)\n",
    "\n",
    "# for name, result in sorted_results:\n",
    "#     gap = result['train_accuracy'] - result['val_accuracy']\n",
    "#     status = \"Good\" if gap < 0.05 else \"Moderate\" if gap < 0.1 else \"High\"\n",
    "#     performance = \"ü•á Best\" if name == sorted_results[0][0] else \"ü•à Good\" if result['val_accuracy'] > 0.55 else \"‚ö†Ô∏è Poor\"\n",
    "#     print(f\"{name:<15} | {result['test_accuracy']:<8.4f} | {result['val_accuracy']:<8.4f} | {gap:<6.4f} | {status:<12} | {performance}\")\n",
    "\n",
    "# # Identify best model based on validation performance\n",
    "# best_model_name = sorted_results[0][0]\n",
    "# best_model = sorted_results[0][1]['model']\n",
    "# print(f\"\\nüèÜ Best performing model (based on validation): {best_model_name}\")\n",
    "# print(f\"   Validation Accuracy: {sorted_results[0][1]['val_accuracy']:.4f}\")\n",
    "# print(f\"   Test Accuracy: {sorted_results[0][1]['test_accuracy']:.4f}\")\n",
    "# print(f\"   Overfitting Gap: {sorted_results[0][1]['train_accuracy'] - sorted_results[0][1]['val_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3da6407",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, f1_score, \n",
    "    roc_auc_score, average_precision_score,\n",
    "    hamming_loss, jaccard_score, accuracy_score\n",
    ")\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from tqdm import tqdm\n",
    "\n",
    "class Train_XGBoost(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"XGBoost classifier with validation-based early stopping for multi-label\"\"\"\n",
    "    \n",
    "    def __init__(self, **xgb_params):\n",
    "        self.xgb_params = xgb_params\n",
    "        self.models_ = []\n",
    "        self.n_classes_ = None\n",
    "        \n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        if len(y.shape) == 1:\n",
    "            y = y.reshape(-1, 1)\n",
    "        if X_val is not None and len(y_val.shape) == 1:\n",
    "            y_val = y_val.reshape(-1, 1)\n",
    "            \n",
    "        self.n_classes_ = y.shape[1]\n",
    "        self.models_ = []\n",
    "        \n",
    "        for i in tqdm(range(self.n_classes_), total=self.n_classes_, leave=True, position=0):\n",
    "            \n",
    "            y_single = y[:, i]\n",
    "            \n",
    "            # Skip if no positive samples\n",
    "            if y_single.sum() == 0:\n",
    "                self.models_.append(None)\n",
    "                continue\n",
    "            \n",
    "            model = xgb.XGBClassifier(**self.xgb_params)\n",
    "            \n",
    "            if X_val is not None and y_val is not None:\n",
    "                y_val_single = y_val[:, i]\n",
    "                model.fit(\n",
    "                    X, y_single,\n",
    "                    eval_set=[(X_val, y_val_single)],\n",
    "                    verbose=False\n",
    "                )\n",
    "            else:\n",
    "                model.fit(X, y_single)\n",
    "            \n",
    "            self.models_.append(model)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predictions = np.zeros((X.shape[0], self.n_classes_))\n",
    "        \n",
    "        for i, model in enumerate(self.models_):\n",
    "            if model is not None:\n",
    "                predictions[:, i] = model.predict(X)\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        probabilities = np.zeros((X.shape[0], self.n_classes_))\n",
    "        \n",
    "        for i, model in enumerate(self.models_):\n",
    "            if model is not None:\n",
    "                proba = model.predict_proba(X)\n",
    "                # Handle case where only one class is present\n",
    "                if proba.shape[1] == 1:\n",
    "                    probabilities[:, i] = 0  # All negative class\n",
    "                else:\n",
    "                    probabilities[:, i] = proba[:, 1]  # Positive class probability\n",
    "        \n",
    "        return probabilities\n",
    "\n",
    "class Train_LGBM(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"LightGBM classifier with validation-based early stopping for multi-label\"\"\"\n",
    "    \n",
    "    def __init__(self, **lgb_params):\n",
    "        self.lgb_params = lgb_params\n",
    "        self.models_ = []\n",
    "        self.n_classes_ = None\n",
    "        \n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        if len(y.shape) == 1:\n",
    "            y = y.reshape(-1, 1)\n",
    "        if X_val is not None and len(y_val.shape) == 1:\n",
    "            y_val = y_val.reshape(-1, 1)\n",
    "            \n",
    "        self.n_classes_ = y.shape[1]\n",
    "        self.models_ = []\n",
    "        \n",
    "        for i in tqdm(range(self.n_classes_), total=self.n_classes_, leave=True, position=0):\n",
    "            \n",
    "            y_single = y[:, i]\n",
    "            \n",
    "            # Skip if no positive samples\n",
    "            if y_single.sum() == 0:\n",
    "                self.models_.append(None)\n",
    "                continue\n",
    "            \n",
    "            model = lgb.LGBMClassifier(**self.lgb_params)\n",
    "            \n",
    "            if X_val is not None and y_val is not None:\n",
    "                y_val_single = y_val[:, i]\n",
    "                model.fit(\n",
    "                    X, y_single,\n",
    "                    eval_set=[(X_val, y_val_single)],\n",
    "                    callbacks=[\n",
    "                        lgb.early_stopping(10, verbose=False),\n",
    "                        lgb.log_evaluation(0)\n",
    "                    ]\n",
    "                )\n",
    "            else:\n",
    "                model.fit(X, y_single)\n",
    "            \n",
    "            self.models_.append(model)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predictions = np.zeros((X.shape[0], self.n_classes_))\n",
    "        \n",
    "        for i, model in enumerate(self.models_):\n",
    "            if model is not None:\n",
    "                predictions[:, i] = model.predict(X)\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        probabilities = np.zeros((X.shape[0], self.n_classes_))\n",
    "        \n",
    "        for i, model in enumerate(self.models_):\n",
    "            if model is not None:\n",
    "                proba = model.predict_proba(X)\n",
    "                # Handle case where only one class is present\n",
    "                if proba.shape[1] == 1:\n",
    "                    probabilities[:, i] = 0  # All negative class\n",
    "                else:\n",
    "                    probabilities[:, i] = proba[:, 1]  # Positive class probability\n",
    "        \n",
    "        return probabilities\n",
    "\n",
    "class Train_logistic(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"Logistic Regression classifier with validation monitoring for multi-label\"\"\"\n",
    "    \n",
    "    def __init__(self, **lr_params):\n",
    "        self.lr_params = lr_params\n",
    "        self.models_ = []\n",
    "        self.n_classes_ = None\n",
    "        self.validation_scores_ = []\n",
    "        \n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        if len(y.shape) == 1:\n",
    "            y = y.reshape(-1, 1)\n",
    "        if X_val is not None and len(y_val.shape) == 1:\n",
    "            y_val = y_val.reshape(-1, 1)\n",
    "            \n",
    "        self.n_classes_ = y.shape[1]\n",
    "        self.models_ = []\n",
    "        self.validation_scores_ = []\n",
    "        \n",
    "        for i in tqdm(range(self.n_classes_), total=self.n_classes_, leave=True, position=0):\n",
    "            \n",
    "            y_single = y[:, i]\n",
    "            \n",
    "            # Skip if no positive samples\n",
    "            if y_single.sum() == 0:\n",
    "                self.models_.append(None)\n",
    "                self.validation_scores_.append(0.0)\n",
    "                continue\n",
    "            \n",
    "            model = LogisticRegression(**self.lr_params)\n",
    "            model.fit(X, y_single)\n",
    "            \n",
    "            # Calculate validation score if validation data provided\n",
    "            if X_val is not None and y_val is not None:\n",
    "                y_val_single = y_val[:, i]\n",
    "                val_score = model.score(X_val, y_val_single)\n",
    "                self.validation_scores_.append(val_score)\n",
    "            else:\n",
    "                self.validation_scores_.append(None)\n",
    "            \n",
    "            self.models_.append(model)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predictions = np.zeros((X.shape[0], self.n_classes_))\n",
    "        \n",
    "        for i, model in enumerate(self.models_):\n",
    "            if model is not None:\n",
    "                predictions[:, i] = model.predict(X)\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        probabilities = np.zeros((X.shape[0], self.n_classes_))\n",
    "        \n",
    "        for i, model in enumerate(self.models_):\n",
    "            if model is not None:\n",
    "                proba = model.predict_proba(X)\n",
    "                # Handle case where only one class is present\n",
    "                if proba.shape[1] == 1:\n",
    "                    probabilities[:, i] = 0  # All negative class\n",
    "                else:\n",
    "                    probabilities[:, i] = proba[:, 1]  # Positive class probability\n",
    "        \n",
    "        return probabilities\n",
    "    \n",
    "    def get_validation_scores(self):\n",
    "        \"\"\"Return validation scores for each label\"\"\"\n",
    "        return self.validation_scores_\n",
    "\n",
    "class Train_RandomForest(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"Random Forest classifier with validation monitoring for multi-label\"\"\"\n",
    "    \n",
    "    def __init__(self, **rf_params):\n",
    "        self.rf_params = rf_params\n",
    "        self.models_ = []\n",
    "        self.n_classes_ = None\n",
    "        self.validation_scores_ = []\n",
    "        self.feature_importances_ = []\n",
    "        \n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        if len(y.shape) == 1:\n",
    "            y = y.reshape(-1, 1)\n",
    "        if X_val is not None and len(y_val.shape) == 1:\n",
    "            y_val = y_val.reshape(-1, 1)\n",
    "            \n",
    "        self.n_classes_ = y.shape[1]\n",
    "        self.models_ = []\n",
    "        self.validation_scores_ = []\n",
    "        self.feature_importances_ = []\n",
    "        \n",
    "        for i in tqdm(range(self.n_classes_), total=self.n_classes_, leave=True, position=0):\n",
    "            \n",
    "            y_single = y[:, i]\n",
    "            \n",
    "            # Skip if no positive samples\n",
    "            if y_single.sum() == 0:\n",
    "                self.models_.append(None)\n",
    "                self.validation_scores_.append(0.0)\n",
    "                self.feature_importances_.append(None)\n",
    "                continue\n",
    "            \n",
    "            model = RandomForestClassifier(**self.rf_params)\n",
    "            model.fit(X, y_single)\n",
    "            \n",
    "            # Store feature importances\n",
    "            self.feature_importances_.append(model.feature_importances_)\n",
    "            \n",
    "            # Calculate validation score if validation data provided\n",
    "            if X_val is not None and y_val is not None:\n",
    "                y_val_single = y_val[:, i]\n",
    "                val_score = model.score(X_val, y_val_single)\n",
    "                self.validation_scores_.append(val_score)\n",
    "            else:\n",
    "                self.validation_scores_.append(None)\n",
    "            \n",
    "            self.models_.append(model)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predictions = np.zeros((X.shape[0], self.n_classes_))\n",
    "        \n",
    "        for i, model in enumerate(self.models_):\n",
    "            if model is not None:\n",
    "                predictions[:, i] = model.predict(X)\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        probabilities = np.zeros((X.shape[0], self.n_classes_))\n",
    "        \n",
    "        for i, model in enumerate(self.models_):\n",
    "            if model is not None:\n",
    "                proba = model.predict_proba(X)\n",
    "                # Handle case where only one class is present\n",
    "                if proba.shape[1] == 1:\n",
    "                    probabilities[:, i] = 0  # All negative class\n",
    "                else:\n",
    "                    probabilities[:, i] = proba[:, 1]  # Positive class probability\n",
    "        \n",
    "        return probabilities\n",
    "    \n",
    "    def get_validation_scores(self):\n",
    "        \"\"\"Return validation scores for each label\"\"\"\n",
    "        return self.validation_scores_\n",
    "    \n",
    "    def get_feature_importances(self):\n",
    "        \"\"\"Return feature importances for each label\"\"\"\n",
    "        return self.feature_importances_\n",
    "\n",
    "def training_function_with_validation(X_train, y_train, X_val, y_val, model_type='lightgbm'):\n",
    "    \"\"\"\n",
    "    Enhanced training function with proper validation control for multi-label classification\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Training {model_type} with validation control...\")\n",
    "    print(f\"X_train shape: {X_train.shape}\")\n",
    "    print(f\"y_train shape: {y_train.shape}\")\n",
    "    print(f\"X_val shape: {X_val.shape}\")\n",
    "    print(f\"y_val shape: {y_val.shape}\")\n",
    "    \n",
    "    if model_type == 'lightgbm':\n",
    "        model = Train_LGBM(\n",
    "            random_state=42,\n",
    "            n_estimators=200,  # More estimators for early stopping\n",
    "            max_depth=6,\n",
    "            learning_rate=0.1,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            verbosity=-1,\n",
    "            early_stopping_rounds=10\n",
    "        )\n",
    "    elif model_type == 'xgboost':\n",
    "        model = Train_XGBoost(\n",
    "            random_state=42,\n",
    "            n_estimators=200,  # More estimators for early stopping\n",
    "            max_depth=6,\n",
    "            learning_rate=0.1,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            eval_metric='logloss',\n",
    "            verbosity=0,\n",
    "            early_stopping_rounds=10\n",
    "        )\n",
    "    elif model_type == 'logistic':\n",
    "        model = Train_logistic(\n",
    "            random_state=42,\n",
    "            max_iter=1000,\n",
    "            C=1.0,\n",
    "            solver='liblinear',\n",
    "            class_weight='balanced'  # Handle class imbalance\n",
    "        )\n",
    "    elif model_type == 'randomforest':\n",
    "        model = Train_RandomForest(\n",
    "            random_state=42,\n",
    "            n_estimators=100,\n",
    "            max_depth=10,\n",
    "            min_samples_split=5,\n",
    "            min_samples_leaf=2,\n",
    "            max_features='sqrt',\n",
    "            class_weight='balanced',  # Handle class imbalance\n",
    "            n_jobs=-1\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"Supported model types: 'lightgbm', 'xgboost', 'logistic', 'randomforest'\")\n",
    "    \n",
    "    # Fit with validation data\n",
    "    model.fit(X_train, y_train, X_val, y_val)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_val = model.predict(X_val)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_acc = accuracy_score(y_train, y_pred_train)\n",
    "    val_acc = accuracy_score(y_val, y_pred_val)\n",
    "    train_f1 = f1_score(y_train, y_pred_train, average='micro')\n",
    "    val_f1 = f1_score(y_val, y_pred_val, average='micro')\n",
    "    \n",
    "    # Calculate hamming loss (lower is better)\n",
    "    train_hamming = hamming_loss(y_train, y_pred_train)\n",
    "    val_hamming = hamming_loss(y_val, y_pred_val)\n",
    "    \n",
    "    # Calculate overfitting gaps for different metrics\n",
    "    accuracy_gap = train_acc - val_acc\n",
    "    f1_gap = train_f1 - val_f1\n",
    "    hamming_gap = val_hamming - train_hamming  # Note: val - train because lower hamming is better\n",
    "    \n",
    "    print(f\"Training completed!\")\n",
    "    print(f\"Train Accuracy: {train_acc:.4f}\")\n",
    "    print(f\"Val Accuracy: {val_acc:.4f}\")\n",
    "    print(f\"Train F1: {train_f1:.4f}\")\n",
    "    print(f\"Val F1: {val_f1:.4f}\")\n",
    "    print(f\"Train Hamming Loss: {train_hamming:.4f}\")\n",
    "    print(f\"Val Hamming Loss: {val_hamming:.4f}\")\n",
    "    print(f\"Overfitting Gap (Accuracy): {accuracy_gap:.4f}\")\n",
    "    print(f\"Overfitting Gap (F1): {f1_gap:.4f}\")\n",
    "    print(f\"Overfitting Gap (Hamming): {hamming_gap:.4f}\")\n",
    "    \n",
    "    return model, {\n",
    "        'train_accuracy': train_acc,\n",
    "        'val_accuracy': val_acc,\n",
    "        'train_f1': train_f1,\n",
    "        'val_f1': val_f1,\n",
    "        'train_hamming_loss': train_hamming,\n",
    "        'val_hamming_loss': val_hamming,\n",
    "        'accuracy_gap': accuracy_gap,\n",
    "        'f1_gap': f1_gap,\n",
    "        'hamming_gap': hamming_gap,\n",
    "        'overfitting_gap': hamming_gap  # Use hamming gap as primary overfitting indicator\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d9918bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting comprehensive model comparison...\n",
      "üöÄ COMPREHENSIVE MODEL COMPARISON WITH VALIDATION CONTROL\n",
      "================================================================================\n",
      "\n",
      "============================================================\n",
      "üîß Training LOGISTIC Model\n",
      "============================================================\n",
      "Training logistic with validation control...\n",
      "X_train shape: (11597, 10000)\n",
      "y_train shape: (11597, 27)\n",
      "X_val shape: (2485, 10000)\n",
      "y_val shape: (2485, 27)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 27/27 [00:23<00:00,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed!\n",
      "Train Accuracy: 0.5919\n",
      "Val Accuracy: 0.4978\n",
      "Train F1: 0.8476\n",
      "Val F1: 0.7936\n",
      "Train Hamming Loss: 0.0199\n",
      "Val Hamming Loss: 0.0269\n",
      "Overfitting Gap (Accuracy): 0.0941\n",
      "Overfitting Gap (F1): 0.0540\n",
      "Overfitting Gap (Hamming): 0.0070\n",
      "‚úÖ LOGISTIC completed successfully!\n",
      "   Test Accuracy: 0.4702\n",
      "   Test F1: 0.7859\n",
      "   Test Hamming Loss: 0.0281\n",
      "   Overfitting Gap (Hamming): 0.0070\n",
      "\n",
      "============================================================\n",
      "üîß Training RANDOMFOREST Model\n",
      "============================================================\n",
      "Training randomforest with validation control...\n",
      "X_train shape: (11597, 10000)\n",
      "y_train shape: (11597, 27)\n",
      "X_val shape: (2485, 10000)\n",
      "y_val shape: (2485, 27)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 27/27 [00:15<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed!\n",
      "Train Accuracy: 0.7369\n",
      "Val Accuracy: 0.5127\n",
      "Train F1: 0.9059\n",
      "Val F1: 0.7894\n",
      "Train Hamming Loss: 0.0114\n",
      "Val Hamming Loss: 0.0245\n",
      "Overfitting Gap (Accuracy): 0.2242\n",
      "Overfitting Gap (F1): 0.1164\n",
      "Overfitting Gap (Hamming): 0.0130\n",
      "‚úÖ RANDOMFOREST completed successfully!\n",
      "   Test Accuracy: 0.5270\n",
      "   Test F1: 0.7901\n",
      "   Test Hamming Loss: 0.0242\n",
      "   Overfitting Gap (Hamming): 0.0130\n",
      "\n",
      "============================================================\n",
      "üîß Training LIGHTGBM Model\n",
      "============================================================\n",
      "Training lightgbm with validation control...\n",
      "X_train shape: (11597, 10000)\n",
      "y_train shape: (11597, 27)\n",
      "X_val shape: (2485, 10000)\n",
      "y_val shape: (2485, 27)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 27/27 [02:37<00:00,  5.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed!\n",
      "Train Accuracy: 0.9586\n",
      "Val Accuracy: 0.6149\n",
      "Train F1: 0.9861\n",
      "Val F1: 0.8340\n",
      "Train Hamming Loss: 0.0016\n",
      "Val Hamming Loss: 0.0181\n",
      "Overfitting Gap (Accuracy): 0.3437\n",
      "Overfitting Gap (F1): 0.1521\n",
      "Overfitting Gap (Hamming): 0.0165\n",
      "‚úÖ LIGHTGBM completed successfully!\n",
      "   Test Accuracy: 0.6070\n",
      "   Test F1: 0.8248\n",
      "   Test Hamming Loss: 0.0190\n",
      "   Overfitting Gap (Hamming): 0.0165\n",
      "\n",
      "============================================================\n",
      "üîß Training XGBOOST Model\n",
      "============================================================\n",
      "Training xgboost with validation control...\n",
      "X_train shape: (11597, 10000)\n",
      "y_train shape: (11597, 27)\n",
      "X_val shape: (2485, 10000)\n",
      "y_val shape: (2485, 27)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 27/27 [08:03<00:00, 17.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed!\n",
      "Train Accuracy: 0.9308\n",
      "Val Accuracy: 0.6205\n",
      "Train F1: 0.9758\n",
      "Val F1: 0.8368\n",
      "Train Hamming Loss: 0.0027\n",
      "Val Hamming Loss: 0.0176\n",
      "Overfitting Gap (Accuracy): 0.3103\n",
      "Overfitting Gap (F1): 0.1390\n",
      "Overfitting Gap (Hamming): 0.0149\n",
      "‚úÖ XGBOOST completed successfully!\n",
      "   Test Accuracy: 0.6219\n",
      "   Test F1: 0.8331\n",
      "   Test Hamming Loss: 0.0179\n",
      "   Overfitting Gap (Hamming): 0.0149\n",
      "\n",
      "====================================================================================================\n",
      "üìä COMPREHENSIVE MODEL ANALYSIS\n",
      "====================================================================================================\n",
      "\n",
      "Model           | Train Acc | Val Acc   | Test Acc  | Train Ham | Val Ham  | Test Ham | Ham Gap  | Status\n",
      "---------------------------------------------------------------------------------------------------------\n",
      "XGBOOST         | 0.9308    | 0.6205    | 0.6219    | 0.0027    | 0.0176   | 0.0179   | 0.0149   | üü¢ Good\n",
      "LIGHTGBM        | 0.9586    | 0.6149    | 0.6070    | 0.0016    | 0.0181   | 0.0190   | 0.0165   | üü¢ Good\n",
      "RANDOMFOREST    | 0.7369    | 0.5127    | 0.5270    | 0.0114    | 0.0245   | 0.0242   | 0.0130   | üü¢ Good\n",
      "LOGISTIC        | 0.5919    | 0.4978    | 0.4702    | 0.0199    | 0.0269   | 0.0281   | 0.0070   | ‚úÖ Excellent\n",
      "\n",
      "üèÜ BEST MODEL (Based on Validation Performance): XGBOOST\n",
      "   üìà Validation Accuracy: 0.6205\n",
      "   üéØ Test Accuracy: 0.6219\n",
      "   üìä Test F1 Score: 0.8331\n",
      "   üîª Test Hamming Loss: 0.0179\n",
      "   ‚öñÔ∏è Overfitting Gap (Hamming): 0.0149\n",
      "   üìè Accuracy Gap: 0.3103\n",
      "   üìà F1 Gap: 0.1390\n",
      "\n",
      "================================================================================\n",
      "üîç MODEL-SPECIFIC INSIGHTS:\n",
      "================================================================================\n",
      "LOGISTIC:\n",
      "   Average per-label validation score: 0.9731\n",
      "   Labels with good performance (>0.8): 27/27\n",
      "RANDOMFOREST:\n",
      "   Average per-label validation score: 0.9755\n",
      "   Labels with good performance (>0.8): 27/27\n",
      "\n",
      "================================================================================\n",
      "üí° RECOMMENDATIONS:\n",
      "================================================================================\n",
      "‚úÖ Your best model shows excellent generalization based on Hamming loss!\n",
      "\n",
      "üéØ Model Selection Priority (Updated with Hamming Loss):\n",
      "   1. Choose model with best VALIDATION performance\n",
      "   2. Prefer models with smaller Hamming loss gap (primary indicator)\n",
      "   3. Consider accuracy and F1 gaps as secondary indicators\n",
      "   4. Evaluate computational efficiency for deployment\n",
      "   5. Lower Hamming loss = better multi-label classification performance\n",
      "\n",
      "üìä Understanding Hamming Loss:\n",
      "   ‚Ä¢ Hamming Loss measures label-wise classification errors\n",
      "   ‚Ä¢ Perfect score = 0.0, higher values = more errors\n",
      "   ‚Ä¢ Particularly important for multi-label problems\n",
      "   ‚Ä¢ Gap = Val_Hamming - Train_Hamming (positive = overfitting)\n"
     ]
    }
   ],
   "source": [
    "# Comprehensive Model Comparison with Validation Control\n",
    "\n",
    "def compare_all_models(X_train, y_train, X_val, y_val, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Train and compare all models with validation control\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"üöÄ COMPREHENSIVE MODEL COMPARISON WITH VALIDATION CONTROL\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    models_to_test = ['logistic', 'randomforest', 'lightgbm', 'xgboost']\n",
    "    results = {}\n",
    "    \n",
    "    for model_type in models_to_test:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"üîß Training {model_type.upper()} Model\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        try:\n",
    "            # Train model with validation\n",
    "            model, metrics = training_function_with_validation(\n",
    "                X_train, y_train, X_val, y_val, model_type=model_type\n",
    "            )\n",
    "            \n",
    "            # Test on unseen data\n",
    "            y_pred_test = model.predict(X_test)\n",
    "            test_acc = accuracy_score(y_test, y_pred_test)\n",
    "            test_f1 = f1_score(y_test, y_pred_test, average='micro')\n",
    "            test_hamming = hamming_loss(y_test, y_pred_test)\n",
    "            \n",
    "            # Store all results\n",
    "            results[model_type] = {\n",
    "                'model': model,\n",
    "                'train_accuracy': metrics['train_accuracy'],\n",
    "                'val_accuracy': metrics['val_accuracy'],\n",
    "                'test_accuracy': test_acc,\n",
    "                'train_f1': metrics['train_f1'],\n",
    "                'val_f1': metrics['val_f1'],\n",
    "                'test_f1': test_f1,\n",
    "                'train_hamming_loss': metrics['train_hamming_loss'],\n",
    "                'val_hamming_loss': metrics['val_hamming_loss'],\n",
    "                'test_hamming_loss': test_hamming,\n",
    "                'accuracy_gap': metrics['accuracy_gap'],\n",
    "                'f1_gap': metrics['f1_gap'],\n",
    "                'hamming_gap': metrics['hamming_gap'],\n",
    "                'overfitting_gap': metrics['overfitting_gap']  # Based on hamming loss\n",
    "            }\n",
    "            \n",
    "            print(f\"‚úÖ {model_type.upper()} completed successfully!\")\n",
    "            print(f\"   Test Accuracy: {test_acc:.4f}\")\n",
    "            print(f\"   Test F1: {test_f1:.4f}\")\n",
    "            print(f\"   Test Hamming Loss: {test_hamming:.4f}\")\n",
    "            print(f\"   Overfitting Gap (Hamming): {metrics['overfitting_gap']:.4f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error training {model_type}: {str(e)}\")\n",
    "            results[model_type] = None\n",
    "    \n",
    "    return results\n",
    "\n",
    "def analyze_model_results(results):\n",
    "    \"\"\"\n",
    "    Analyze and display comprehensive results\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*100}\")\n",
    "    print(\"üìä COMPREHENSIVE MODEL ANALYSIS\")\n",
    "    print(f\"{'='*100}\")\n",
    "    \n",
    "    # Filter successful results\n",
    "    successful_results = {k: v for k, v in results.items() if v is not None}\n",
    "    \n",
    "    if not successful_results:\n",
    "        print(\"‚ùå No models trained successfully!\")\n",
    "        return\n",
    "    \n",
    "    # Display detailed comparison table\n",
    "    print(f\"\\n{'Model':<15} | {'Train Acc':<9} | {'Val Acc':<9} | {'Test Acc':<9} | {'Train Ham':<9} | {'Val Ham':<8} | {'Test Ham':<8} | {'Ham Gap':<8} | {'Status'}\")\n",
    "    print(\"-\" * 105)\n",
    "    \n",
    "    # Sort by validation accuracy (best practice)\n",
    "    sorted_results = sorted(successful_results.items(), \n",
    "                          key=lambda x: x[1]['val_accuracy'], reverse=True)\n",
    "    \n",
    "    for rank, (model_name, result) in enumerate(sorted_results, 1):\n",
    "        hamming_gap = result['hamming_gap']\n",
    "        \n",
    "        # Determine overfitting status based on hamming gap\n",
    "        # For hamming loss, positive gap means validation is worse (overfitting)\n",
    "        if hamming_gap < 0.01:\n",
    "            status = \"‚úÖ Excellent\"\n",
    "        elif hamming_gap < 0.02:\n",
    "            status = \"üü¢ Good\"\n",
    "        elif hamming_gap < 0.04:\n",
    "            status = \"üü° Moderate\"\n",
    "        else:\n",
    "            status = \"üî¥ High\"\n",
    "        \n",
    "        rank_emoji = \"ü•á\" if rank == 1 else \"ü•à\" if rank == 2 else \"ü•â\" if rank == 3 else \"4Ô∏è‚É£\"\n",
    "        \n",
    "        print(f\"{model_name.upper():<15} | {result['train_accuracy']:<9.4f} | {result['val_accuracy']:<9.4f} | \"\n",
    "              f\"{result['test_accuracy']:<9.4f} | {result['train_hamming_loss']:<9.4f} | {result['val_hamming_loss']:<8.4f} | \"\n",
    "              f\"{result['test_hamming_loss']:<8.4f} | {hamming_gap:<8.4f} | {status}\")\n",
    "    \n",
    "    # Identify best models\n",
    "    best_model = sorted_results[0]\n",
    "    print(f\"\\nüèÜ BEST MODEL (Based on Validation Performance): {best_model[0].upper()}\")\n",
    "    print(f\"   üìà Validation Accuracy: {best_model[1]['val_accuracy']:.4f}\")\n",
    "    print(f\"   üéØ Test Accuracy: {best_model[1]['test_accuracy']:.4f}\")\n",
    "    print(f\"   üìä Test F1 Score: {best_model[1]['test_f1']:.4f}\")\n",
    "    print(f\"   üîª Test Hamming Loss: {best_model[1]['test_hamming_loss']:.4f}\")\n",
    "    print(f\"   ‚öñÔ∏è Overfitting Gap (Hamming): {best_model[1]['overfitting_gap']:.4f}\")\n",
    "    print(f\"   üìè Accuracy Gap: {best_model[1]['accuracy_gap']:.4f}\")\n",
    "    print(f\"   üìà F1 Gap: {best_model[1]['f1_gap']:.4f}\")\n",
    "    \n",
    "    # Best test performance (might be different from best validation)\n",
    "    best_test = max(successful_results.items(), key=lambda x: x[1]['test_accuracy'])\n",
    "    if best_test[0] != best_model[0]:\n",
    "        print(f\"\\nüéØ BEST TEST PERFORMANCE: {best_test[0].upper()}\")\n",
    "        print(f\"   Test Accuracy: {best_test[1]['test_accuracy']:.4f}\")\n",
    "        print(f\"   (Note: Choose model based on validation, not test performance)\")\n",
    "    \n",
    "    # Best hamming loss performance\n",
    "    best_hamming = min(successful_results.items(), key=lambda x: x[1]['test_hamming_loss'])\n",
    "    if best_hamming[0] != best_model[0]:\n",
    "        print(f\"\\nüîª BEST HAMMING LOSS PERFORMANCE: {best_hamming[0].upper()}\")\n",
    "        print(f\"   Test Hamming Loss: {best_hamming[1]['test_hamming_loss']:.4f}\")\n",
    "        print(f\"   (Lower hamming loss = better multi-label performance)\")\n",
    "    \n",
    "    # Model-specific insights\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"üîç MODEL-SPECIFIC INSIGHTS:\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    for model_name, result in successful_results.items():\n",
    "        if hasattr(result['model'], 'get_validation_scores'):\n",
    "            val_scores = result['model'].get_validation_scores()\n",
    "            if val_scores and any(score for score in val_scores if score is not None):\n",
    "                valid_scores = [s for s in val_scores if s is not None and s > 0]\n",
    "                if valid_scores:\n",
    "                    avg_label_score = np.mean(valid_scores)\n",
    "                    print(f\"{model_name.upper()}:\")\n",
    "                    print(f\"   Average per-label validation score: {avg_label_score:.4f}\")\n",
    "                    print(f\"   Labels with good performance (>0.8): {sum(1 for s in valid_scores if s > 0.8)}/{len(valid_scores)}\")\n",
    "    \n",
    "    # Recommendations\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"üí° RECOMMENDATIONS:\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    if best_model[1]['overfitting_gap'] < 0.02:\n",
    "        print(\"‚úÖ Your best model shows excellent generalization based on Hamming loss!\")\n",
    "    elif best_model[1]['overfitting_gap'] < 0.04:\n",
    "        print(\"üü¢ Your best model shows good generalization based on Hamming loss!\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Consider additional regularization for your best model:\")\n",
    "        print(\"   - Increase regularization parameters\")\n",
    "        print(\"   - Use more training data\")\n",
    "        print(\"   - Apply feature selection\")\n",
    "        print(\"   - Consider ensemble methods\")\n",
    "    \n",
    "    hamming_gap_threshold = 0.02\n",
    "    models_with_overfitting = [name for name, result in successful_results.items() \n",
    "                              if result['hamming_gap'] > hamming_gap_threshold]\n",
    "    \n",
    "    if models_with_overfitting:\n",
    "        print(f\"\\n‚ö†Ô∏è Models showing overfitting based on Hamming loss (gap > {hamming_gap_threshold}):\")\n",
    "        for model in models_with_overfitting:\n",
    "            result = successful_results[model]\n",
    "            print(f\"   - {model.upper()}:\")\n",
    "            print(f\"     ‚Ä¢ Hamming Gap: {result['hamming_gap']:.4f}\")\n",
    "            print(f\"     ‚Ä¢ Accuracy Gap: {result['accuracy_gap']:.4f}\")\n",
    "            print(f\"     ‚Ä¢ F1 Gap: {result['f1_gap']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nüéØ Model Selection Priority (Updated with Hamming Loss):\")\n",
    "    print(\"   1. Choose model with best VALIDATION performance\")\n",
    "    print(\"   2. Prefer models with smaller Hamming loss gap (primary indicator)\")\n",
    "    print(\"   3. Consider accuracy and F1 gaps as secondary indicators\")\n",
    "    print(\"   4. Evaluate computational efficiency for deployment\")\n",
    "    print(\"   5. Lower Hamming loss = better multi-label classification performance\")\n",
    "    \n",
    "    print(f\"\\nüìä Understanding Hamming Loss:\")\n",
    "    print(\"   ‚Ä¢ Hamming Loss measures label-wise classification errors\")\n",
    "    print(\"   ‚Ä¢ Perfect score = 0.0, higher values = more errors\")\n",
    "    print(\"   ‚Ä¢ Particularly important for multi-label problems\")\n",
    "    print(\"   ‚Ä¢ Gap = Val_Hamming - Train_Hamming (positive = overfitting)\")\n",
    "    \n",
    "    return successful_results\n",
    "\n",
    "# Example usage\n",
    "print(\"Starting comprehensive model comparison...\")\n",
    "all_results = compare_all_models(X_train_tfidf, y_train, X_val_tfidf, y_val, X_test_tfidf, y_test)\n",
    "final_analysis = analyze_model_results(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c057bf7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing enhanced validation-controlled training...\n",
      "============================================================\n",
      "Training lightgbm with validation control...\n",
      "X_train shape: (11597, 10000)\n",
      "y_train shape: (11597, 27)\n",
      "X_val shape: (2485, 10000)\n",
      "y_val shape: (2485, 27)\n",
      "  Training LGBM classifier 1/27\n",
      "  Training LGBM classifier 2/27\n",
      "  Training LGBM classifier 3/27\n",
      "  Training LGBM classifier 4/27\n",
      "  Training LGBM classifier 5/27\n",
      "  Training LGBM classifier 6/27\n",
      "  Training LGBM classifier 7/27\n",
      "  Training LGBM classifier 8/27\n",
      "  Training LGBM classifier 9/27\n",
      "  Training LGBM classifier 10/27\n",
      "  Training LGBM classifier 11/27\n",
      "  Training LGBM classifier 12/27\n",
      "  Training LGBM classifier 13/27\n",
      "  Training LGBM classifier 14/27\n",
      "  Training LGBM classifier 15/27\n",
      "  Training LGBM classifier 16/27\n",
      "  Training LGBM classifier 17/27\n",
      "  Training LGBM classifier 18/27\n",
      "  Training LGBM classifier 19/27\n",
      "  Training LGBM classifier 20/27\n",
      "  Training LGBM classifier 21/27\n",
      "  Training LGBM classifier 22/27\n",
      "  Training LGBM classifier 23/27\n",
      "  Training LGBM classifier 24/27\n",
      "  Training LGBM classifier 25/27\n",
      "  Training LGBM classifier 26/27\n",
      "  Training LGBM classifier 27/27\n",
      "Training completed!\n",
      "Train Accuracy: 0.9586\n",
      "Val Accuracy: 0.6149\n",
      "Train F1: 0.9861\n",
      "Val F1: 0.8340\n",
      "Overfitting Gap (Acc): 0.3437\n",
      "\n",
      "LightGBM Results:\n",
      "  Validation Accuracy: 0.6149\n",
      "  Overfitting Gap: 0.3437\n"
     ]
    }
   ],
   "source": [
    "# # Example usage of the enhanced training function\n",
    "# print(\"Testing enhanced validation-controlled training...\")\n",
    "# print(\"=\"*60)\n",
    "\n",
    "# # Test with LightGBM\n",
    "# lgbm_model, lgbm_metrics = training_function_with_validation(\n",
    "#     X_train_tfidf, y_train, X_val_tfidf, y_val, model_type='lightgbm'\n",
    "# )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea5a3b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------\n",
      "Training xgboost with validation control...\n",
      "X_train shape: (11597, 10000)\n",
      "y_train shape: (11597, 27)\n",
      "X_val shape: (2485, 10000)\n",
      "y_val shape: (2485, 27)\n",
      "  Training XGB classifier 1/27\n",
      "  Training XGB classifier 2/27\n",
      "  Training XGB classifier 3/27\n",
      "  Training XGB classifier 4/27\n",
      "  Training XGB classifier 5/27\n",
      "  Training XGB classifier 6/27\n",
      "  Training XGB classifier 7/27\n",
      "  Training XGB classifier 8/27\n",
      "  Training XGB classifier 9/27\n",
      "  Training XGB classifier 10/27\n",
      "  Training XGB classifier 11/27\n",
      "  Training XGB classifier 12/27\n",
      "  Training XGB classifier 13/27\n",
      "  Training XGB classifier 14/27\n",
      "  Training XGB classifier 15/27\n",
      "  Training XGB classifier 16/27\n",
      "  Training XGB classifier 17/27\n",
      "  Training XGB classifier 18/27\n",
      "  Training XGB classifier 19/27\n",
      "  Training XGB classifier 20/27\n",
      "  Training XGB classifier 21/27\n",
      "  Training XGB classifier 22/27\n",
      "  Training XGB classifier 23/27\n",
      "  Training XGB classifier 24/27\n",
      "  Training XGB classifier 25/27\n",
      "  Training XGB classifier 26/27\n",
      "  Training XGB classifier 27/27\n",
      "Training completed!\n",
      "Train Accuracy: 0.9308\n",
      "Val Accuracy: 0.6205\n",
      "Train F1: 0.9758\n",
      "Val F1: 0.8368\n",
      "Overfitting Gap (Acc): 0.3103\n",
      "\n",
      "XGBoost Results:\n",
      "  Validation Accuracy: 0.6205\n",
      "  Overfitting Gap: 0.3103\n",
      "\n",
      "Final Test Results:\n",
      "  LightGBM Test Accuracy: 0.6070\n",
      "  XGBoost Test Accuracy: 0.6219\n",
      "\n",
      "üèÜ Best validation-controlled model: XGBoost\n",
      "   Test Accuracy: 0.6219\n"
     ]
    }
   ],
   "source": [
    "# # Test with XGBoost\n",
    "# print(f\"\\n{'-'*40}\")\n",
    "# xgb_model, xgb_metrics = training_function_with_validation(\n",
    "#     X_train_tfidf, y_train, X_val_tfidf, y_val, model_type='xgboost'\n",
    "# )\n",
    "\n",
    "# print(f\"\\nXGBoost Results:\")\n",
    "# print(f\"  Validation Accuracy: {xgb_metrics['val_accuracy']:.4f}\")\n",
    "# print(f\"  Overfitting Gap: {xgb_metrics['overfitting_gap']:.4f}\")\n",
    "\n",
    "# # Final test predictions\n",
    "# lgbm_test_pred = lgbm_model.predict(X_test_tfidf)\n",
    "# xgb_test_pred = xgb_model.predict(X_test_tfidf)\n",
    "\n",
    "# lgbm_test_acc = accuracy_score(y_test, lgbm_test_pred)\n",
    "# xgb_test_acc = accuracy_score(y_test, xgb_test_pred)\n",
    "\n",
    "# print(f\"\\nFinal Test Results:\")\n",
    "# print(f\"  LightGBM Test Accuracy: {lgbm_test_acc:.4f}\")\n",
    "# print(f\"  XGBoost Test Accuracy: {xgb_test_acc:.4f}\")\n",
    "\n",
    "# # Determine best model\n",
    "# if lgbm_metrics['val_accuracy'] > xgb_metrics['val_accuracy']:\n",
    "#     best_val_model = 'LightGBM'\n",
    "#     best_model = lgbm_model\n",
    "#     best_test_acc = lgbm_test_acc\n",
    "# else:\n",
    "#     best_val_model = 'XGBoost'\n",
    "#     best_model = xgb_model\n",
    "#     best_test_acc = xgb_test_acc\n",
    "\n",
    "# print(f\"\\nüèÜ Best validation-controlled model: {best_val_model}\")\n",
    "# print(f\"   Test Accuracy: {best_test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217b6c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Additional Validation Techniques for Overfitting Control\n",
    "\n",
    "# from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "# from sklearn.model_selection import validation_curve, learning_curve\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def plot_learning_curve(estimator, X, y, title, cv=5, n_jobs=-1, \n",
    "#                        train_sizes=np.linspace(0.1, 1.0, 10)):\n",
    "#     \"\"\"\n",
    "#     Generate a plot showing the learning curve for a model\n",
    "#     \"\"\"\n",
    "#     train_sizes, train_scores, val_scores = learning_curve(\n",
    "#         estimator, X, y, cv=cv, n_jobs=n_jobs, \n",
    "#         train_sizes=train_sizes, scoring='accuracy'\n",
    "#     )\n",
    "    \n",
    "#     train_scores_mean = np.mean(train_scores, axis=1)\n",
    "#     train_scores_std = np.std(train_scores, axis=1)\n",
    "#     val_scores_mean = np.mean(val_scores, axis=1)\n",
    "#     val_scores_std = np.std(val_scores, axis=1)\n",
    "    \n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     plt.plot(train_sizes, train_scores_mean, 'o-', color='blue', label='Training score')\n",
    "#     plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "#                      train_scores_mean + train_scores_std, alpha=0.1, color='blue')\n",
    "    \n",
    "#     plt.plot(train_sizes, val_scores_mean, 'o-', color='red', label='Cross-validation score')\n",
    "#     plt.fill_between(train_sizes, val_scores_mean - val_scores_std,\n",
    "#                      val_scores_mean + val_scores_std, alpha=0.1, color='red')\n",
    "    \n",
    "#     plt.xlabel('Training Set Size')\n",
    "#     plt.ylabel('Accuracy Score')\n",
    "#     plt.title(f'Learning Curve - {title}')\n",
    "#     plt.legend(loc='best')\n",
    "#     plt.grid(True, alpha=0.3)\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "    \n",
    "#     # Detect overfitting\n",
    "#     final_gap = train_scores_mean[-1] - val_scores_mean[-1]\n",
    "#     if final_gap > 0.1:\n",
    "#         print(f\"‚ö†Ô∏è WARNING: {title} shows signs of overfitting (gap: {final_gap:.4f})\")\n",
    "#     elif final_gap > 0.05:\n",
    "#         print(f\"üî∂ MODERATE: {title} shows moderate overfitting (gap: {final_gap:.4f})\")\n",
    "#     else:\n",
    "#         print(f\"‚úÖ GOOD: {title} shows good generalization (gap: {final_gap:.4f})\")\n",
    "\n",
    "# def cross_validate_with_overfitting_check(model, X, y, cv=5, model_name=\"Model\"):\n",
    "#     \"\"\"\n",
    "#     Perform cross-validation and check for overfitting signs\n",
    "#     \"\"\"\n",
    "#     print(f\"\\nCross-validating {model_name}...\")\n",
    "    \n",
    "#     # Perform cross-validation\n",
    "#     cv_scores = cross_val_score(model, X, y, cv=cv, scoring='accuracy', n_jobs=-1)\n",
    "    \n",
    "#     # Train on full dataset to check training score\n",
    "#     model.fit(X, y)\n",
    "#     train_score = model.score(X, y)\n",
    "    \n",
    "#     cv_mean = cv_scores.mean()\n",
    "#     cv_std = cv_scores.std()\n",
    "    \n",
    "#     print(f\"  Cross-validation scores: {cv_scores}\")\n",
    "#     print(f\"  CV Mean ¬± Std: {cv_mean:.4f} ¬± {cv_std:.4f}\")\n",
    "#     print(f\"  Training score: {train_score:.4f}\")\n",
    "    \n",
    "#     # Check for overfitting\n",
    "#     overfitting_gap = train_score - cv_mean\n",
    "#     print(f\"  Overfitting gap: {overfitting_gap:.4f}\")\n",
    "    \n",
    "#     if overfitting_gap > 0.1:\n",
    "#         status = \"üö® HIGH OVERFITTING\"\n",
    "#     elif overfitting_gap > 0.05:\n",
    "#         status = \"‚ö†Ô∏è MODERATE OVERFITTING\"\n",
    "#     else:\n",
    "#         status = \"‚úÖ GOOD GENERALIZATION\"\n",
    "    \n",
    "#     print(f\"  Status: {status}\")\n",
    "    \n",
    "#     return {\n",
    "#         'cv_scores': cv_scores,\n",
    "#         'cv_mean': cv_mean,\n",
    "#         'cv_std': cv_std,\n",
    "#         'train_score': train_score,\n",
    "#         'overfitting_gap': overfitting_gap,\n",
    "#         'status': status\n",
    "#     }\n",
    "\n",
    "# def plot_validation_curve_param(estimator, X, y, param_name, param_range, title):\n",
    "#     \"\"\"\n",
    "#     Plot validation curve for a specific parameter to find optimal value\n",
    "#     \"\"\"\n",
    "#     train_scores, val_scores = validation_curve(\n",
    "#         estimator, X, y, param_name=param_name, param_range=param_range,\n",
    "#         cv=5, scoring='accuracy', n_jobs=-1\n",
    "#     )\n",
    "    \n",
    "#     train_scores_mean = np.mean(train_scores, axis=1)\n",
    "#     train_scores_std = np.std(train_scores, axis=1)\n",
    "#     val_scores_mean = np.mean(val_scores, axis=1)\n",
    "#     val_scores_std = np.std(val_scores, axis=1)\n",
    "    \n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     plt.semilogx(param_range, train_scores_mean, 'o-', color='blue', label='Training score')\n",
    "#     plt.fill_between(param_range, train_scores_mean - train_scores_std,\n",
    "#                      train_scores_mean + train_scores_std, alpha=0.1, color='blue')\n",
    "    \n",
    "#     plt.semilogx(param_range, val_scores_mean, 'o-', color='red', label='Cross-validation score')\n",
    "#     plt.fill_between(param_range, val_scores_mean - val_scores_std,\n",
    "#                      val_scores_mean + val_scores_std, alpha=0.1, color='red')\n",
    "    \n",
    "#     plt.xlabel(param_name)\n",
    "#     plt.ylabel('Accuracy Score')\n",
    "#     plt.title(f'Validation Curve - {title}')\n",
    "#     plt.legend(loc='best')\n",
    "#     plt.grid(True, alpha=0.3)\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "    \n",
    "#     # Find optimal parameter\n",
    "#     optimal_idx = np.argmax(val_scores_mean)\n",
    "#     optimal_param = param_range[optimal_idx]\n",
    "#     optimal_score = val_scores_mean[optimal_idx]\n",
    "    \n",
    "#     print(f\"Optimal {param_name}: {optimal_param}\")\n",
    "#     print(f\"Optimal CV score: {optimal_score:.4f}\")\n",
    "    \n",
    "#     return optimal_param, optimal_score\n",
    "\n",
    "# # Example: Cross-validation analysis for overfitting detection\n",
    "# print(\"COMPREHENSIVE VALIDATION ANALYSIS\")\n",
    "# print(\"=\"*60)\n",
    "\n",
    "# # Sample a subset for faster computation in demo\n",
    "# sample_size = min(1000, len(X_train_tfidf))\n",
    "# X_sample = X_train_tfidf[:sample_size]\n",
    "# y_sample = y_train[:sample_size]\n",
    "\n",
    "# print(f\"Using sample of {sample_size} examples for validation analysis...\")\n",
    "\n",
    "# # 1. Cross-validation for different models\n",
    "# models_for_cv = {\n",
    "#     'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "#     'Random Forest': RandomForestClassifier(n_estimators=50, random_state=42, max_depth=10),\n",
    "# }\n",
    "\n",
    "# cv_results = {}\n",
    "# for name, model in models_for_cv.items():\n",
    "#     # Use OneVsRestClassifier for multi-label\n",
    "#     multi_label_model = OneVsRestClassifier(model)\n",
    "#     cv_results[name] = cross_validate_with_overfitting_check(\n",
    "#         multi_label_model, X_sample, y_sample, cv=3, model_name=name\n",
    "#     )\n",
    "\n",
    "# # 2. Find models with best generalization\n",
    "# print(f\"\\n{'='*60}\")\n",
    "# print(\"OVERFITTING SUMMARY:\")\n",
    "# print(f\"{'Model':<20} | {'CV Score':<10} | {'Gap':<8} | {'Status'}\")\n",
    "# print(f\"{'-'*65}\")\n",
    "\n",
    "# for name, results in cv_results.items():\n",
    "#     print(f\"{name:<20} | {results['cv_mean']:<10.4f} | {results['overfitting_gap']:<8.4f} | {results['status']}\")\n",
    "\n",
    "# # 3. Recommendations for overfitting control\n",
    "# print(f\"\\n{'='*60}\")\n",
    "# print(\"RECOMMENDATIONS FOR OVERFITTING CONTROL:\")\n",
    "# print()\n",
    "# print(\"1. üìä VALIDATION MONITORING:\")\n",
    "# print(\"   - Always split data into train/validation/test\")\n",
    "# print(\"   - Monitor validation metrics during training\")\n",
    "# print(\"   - Use early stopping when validation stops improving\")\n",
    "# print()\n",
    "# print(\"2. üîß MODEL REGULARIZATION:\")\n",
    "# print(\"   - Logistic Regression: Adjust C parameter (lower = more regularization)\")\n",
    "# print(\"   - Random Forest: Limit max_depth, increase min_samples_split\")\n",
    "# print(\"   - XGBoost/LightGBM: Use early_stopping_rounds, adjust learning_rate\")\n",
    "# print()\n",
    "# print(\"3. üìà TECHNIQUES IMPLEMENTED:\")\n",
    "# print(\"   - Train/Validation/Test split (70/15/15)\")\n",
    "# print(\"   - Cross-validation for robust evaluation\")\n",
    "# print(\"   - Early stopping for tree-based models\")\n",
    "# print(\"   - Validation gap monitoring\")\n",
    "# print(\"   - Learning curve analysis\")\n",
    "# print()\n",
    "# print(\"4. üéØ SELECTION CRITERIA:\")\n",
    "# print(\"   - Choose model with best VALIDATION performance\")\n",
    "# print(\"   - Prefer models with smaller train-validation gap\")\n",
    "# print(\"   - Consider cross-validation consistency\")\n",
    "\n",
    "# # Example of how to use validation curve for parameter tuning\n",
    "# print(f\"\\n{'='*60}\")\n",
    "# print(\"PARAMETER TUNING WITH VALIDATION CURVES:\")\n",
    "# print(\"(Use this approach to find optimal hyperparameters)\")\n",
    "# print()\n",
    "# print(\"Example code for Random Forest max_depth tuning:\")\n",
    "# print(\"\"\"\n",
    "# # Find optimal max_depth for Random Forest\n",
    "# param_range = [3, 5, 7, 10, 15, 20]\n",
    "# optimal_depth, optimal_score = plot_validation_curve_param(\n",
    "#     OneVsRestClassifier(RandomForestClassifier(random_state=42)),\n",
    "#     X_train_tfidf, y_train,\n",
    "#     param_name='estimator__max_depth',\n",
    "#     param_range=param_range,\n",
    "#     title='Random Forest max_depth'\n",
    "# )\n",
    "# \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a7c4d3",
   "metadata": {},
   "source": [
    "## Transformers Encoder Model(MordenBERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48e3714a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import DataCollatorWithPadding\n",
    "import evaluate\n",
    "import warnings\n",
    "# Suppress the tqdm warning temporarily\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='tqdm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d27045a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets_from_arrays(X_train, y_train, X_val=None, y_val=None, X_test=None, y_test=None):\n",
    "    \"\"\"\n",
    "    Convert arrays into HuggingFace datasets format with specified structure\n",
    "    \n",
    "    Returns:\n",
    "        DatasetDict with features:\n",
    "        - dataset[\"train\"][\"text\"]: text data\n",
    "        - dataset[\"train\"][\"labels\"]: multi-label arrays\n",
    "        - dataset[\"val\"][\"text\"]: validation text data (if provided)\n",
    "        - dataset[\"val\"][\"labels\"]: validation labels (if provided)\n",
    "        - dataset[\"test\"][\"text\"]: test text data (if provided)\n",
    "        - dataset[\"test\"][\"labels\"]: test labels (if provided)\n",
    "    \"\"\"\n",
    "    # Create training dataset\n",
    "    train_dict = {\n",
    "        \"text\": X_train.tolist() if hasattr(X_train, 'tolist') else list(X_train),\n",
    "        \"labels\": y_train.tolist() if hasattr(y_train, 'tolist') else list(y_train)\n",
    "    }\n",
    "    \n",
    "    datasets_dict = {\n",
    "        \"train\": Dataset.from_dict(train_dict)\n",
    "    }\n",
    "    \n",
    "    # Add validation dataset if provided\n",
    "    if X_val is not None and y_val is not None:\n",
    "        val_dict = {\n",
    "            \"text\": X_val.tolist() if hasattr(X_val, 'tolist') else list(X_val),\n",
    "            \"labels\": y_val.tolist() if hasattr(y_val, 'tolist') else list(y_val)\n",
    "        }\n",
    "        datasets_dict[\"val\"] = Dataset.from_dict(val_dict)\n",
    "    \n",
    "    # Add test dataset if provided\n",
    "    if X_test is not None and y_test is not None:\n",
    "        test_dict = {\n",
    "            \"text\": X_test.tolist() if hasattr(X_test, 'tolist') else list(X_test),\n",
    "            \"labels\": y_test.tolist() if hasattr(y_test, 'tolist') else list(y_test)\n",
    "        }\n",
    "        datasets_dict[\"test\"] = Dataset.from_dict(test_dict)\n",
    "\n",
    "    # Create DatasetDict\n",
    "    dataset = DatasetDict(datasets_dict)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "669a76ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_data=os.path.join(os.getcwd(), 'processed_data')\n",
    "with open(os.path.join(saved_data,'train_arrays.pkl'), 'rb') as f:\n",
    "    train_data = pickle.load(f)\n",
    "    X_train = train_data['X_train']\n",
    "    y_train = train_data['y_train']\n",
    "\n",
    "with open(os.path.join(saved_data,'val_arrays.pkl'), 'rb') as f:\n",
    "    val_data = pickle.load(f)\n",
    "    X_val = val_data['X_val']\n",
    "    y_val = val_data['y_val']\n",
    "\n",
    "with open(os.path.join(saved_data,'test_arrays.pkl'), 'rb') as f:\n",
    "    test_data = pickle.load(f)\n",
    "    X_test = test_data['X_test']\n",
    "    y_test = test_data['y_test']\n",
    "\n",
    "# Create the datasets\n",
    "dataset = create_datasets_from_arrays(X_train, y_train, X_val, y_val, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6d9195ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'labels'],\n",
       "        num_rows: 11597\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['text', 'labels'],\n",
       "        num_rows: 2485\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'labels'],\n",
       "        num_rows: 2486\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef977ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11597/11597 [01:34<00:00, 123.33 examples/s]\n",
      "Map:  36%|‚ñà‚ñà‚ñà‚ñå      | 891/2485 [00:06<00:10, 146.34 examples/s]"
     ]
    }
   ],
   "source": [
    "model_path = \"answerdotai/ModernBERT-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "def preprocess_function(example):\n",
    "   text = example['text']\n",
    "   example = tokenizer(text, truncation=True)\n",
    "   return example\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f7ca12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a6c5d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b25da0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fdcea26d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/d/TR-Project/.venv/lib/python3.13/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Map:   0%|          | 0/2759 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map:   0%|          | 0/2759 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2759/2759 [00:04<00:00, 570.75 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2759/2759 [00:04<00:00, 570.75 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 381/381 [00:00<00:00, 569.69 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 381/381 [00:00<00:00, 569.69 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "    \n",
    "dataset = load_dataset('knowledgator/events_classification_biotech',trust_remote_code=True) \n",
    "    \n",
    "classes = [class_ for class_ in dataset['train'].features['label 1'].names if class_]\n",
    "class2id = {class_:id for id, class_ in enumerate(classes)}\n",
    "id2class = {id:class_ for class_, id in class2id.items()}\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_path = 'microsoft/deberta-v3-small'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "def preprocess_function(example):\n",
    "   text = f\"{example['title']}.\\n{example['content']}\"\n",
    "   all_labels = example['all_labels']\n",
    "   labels = [0. for i in range(len(classes))]\n",
    "   for label in all_labels:\n",
    "       label_id = class2id[label]\n",
    "       labels[label_id] = 1.\n",
    "  \n",
    "   example = tokenizer(text, truncation=True)\n",
    "   example['labels'] = labels\n",
    "   return example\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ac555c67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_dataset[\"train\"][2]['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a298aad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
