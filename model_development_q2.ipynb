{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e7a5b02",
   "metadata": {},
   "source": [
    "<span style=\"font-weight: bold; font-size: 18px;\">**Multi-Label Posture Classification: Model Development Strategy**<br><br>\n",
    "\n",
    "We propose a comparative evaluation of two complementary modeling approaches to address the multi-label posture prediction task, each offering distinct advantages for legal document classification.\n",
    "\n",
    "**Baseline Approach: Bag-of-Words Models**<br>\n",
    "\n",
    "Our initial baseline leverages traditional bag-of-words representations (TF-IDF, BM25) combined with multi-label classifiers, justified by several key factors:\n",
    "\n",
    "<div style=\"margin-left: 20px;\"><b>• Computational Efficiency:</b> Lightweight architecture enables rapid prototyping and establishes performance baselines without GPU requirements</div>\n",
    "<div style=\"margin-left: 20px;\"><b>• Statistical Robustness:</b> Word-frequency features provide interpretable, domain-agnostic representations suitable for legal terminology analysis</div>\n",
    "<div style=\"margin-left: 20px;\"><b>• Multi-Label Compatibility:</b> Well-established integration with multi-label algorithms (One-vs-Rest, Binary Relevance, Label Powerset)</div>\n",
    "<div style=\"margin-left: 20px;\"><b>• Baseline Establishment:</b> Provides interpretable performance benchmarks for evaluating more complex architectures</div>\n",
    "\n",
    "**Advanced Approach: Transformer-Based Models (ModernBERT)**<br>\n",
    "\n",
    "Our primary model leverages ModernBERT encoder architecture, specifically designed to address the limitations of traditional BERT for our use case:\n",
    "\n",
    "<div style=\"margin-left: 20px;\"><b>• Extended Context Coverage:</b> ModernBERT's 8,192-token context window accommodates ~90% of our corpus without truncation, preserving critical legal context that may span entire documents</div>\n",
    "\n",
    "<div style=\"margin-left: 20px;\"><b>• Contextual Understanding:</b> Unlike bag-of-words approaches, transformer architectures capture:\n",
    "  <div style=\"margin-left: 40px;\">- Long-range dependencies between legal arguments</div>\n",
    "  <div style=\"margin-left: 40px;\">- Positional relationships between procedural elements</div>\n",
    "  <div style=\"margin-left: 40px;\">- Semantic nuances distinguishing similar posture categories</div>\n",
    "</div>\n",
    "\n",
    "<div style=\"margin-left: 20px;\"><b>• Multi-Label Architecture:</b> The encoder's [CLS] token representation can be effectively coupled with multi-label classification heads, enabling simultaneous prediction of multiple postures</div>\n",
    "\n",
    "<div style=\"margin-left: 20px;\"><b>• Legal Domain Adaptation:</b> Pre-trained language understanding provides superior handling of complex legal terminology and document structure</div>\n",
    "\n",
    "**Comparative Justification:**<br>\n",
    "\n",
    "This dual-approach strategy enables comprehensive evaluation of feature representation impact on multi-label performance, ranging from traditional statistical methods to state-of-the-art contextual understanding, ultimately identifying the optimal balance between computational efficiency and classification accuracy for legal posture prediction.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce6340b",
   "metadata": {},
   "source": [
    "## Data Preparation for ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f537d7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "56de0440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents with postures: 17077\n"
     ]
    }
   ],
   "source": [
    "# Prepare the labels - convert postures to a list format\n",
    "def prepare_labels(postures_str):\n",
    "    \"\"\"Convert posture string to list of postures\"\"\"\n",
    "    if pd.isna(postures_str) or postures_str == '':\n",
    "        return []\n",
    "    return [p.strip() for p in postures_str.split(',') if p.strip()]\n",
    "\n",
    "# Apply to dataframe\n",
    "_dir=os.path.join(os.getcwd(),\"processed_data\")\n",
    "df=pd.read_pickle(os.path.join(_dir, \"data.pkl\"))\n",
    "df['posture_list'] = df['postures'].apply(prepare_labels)\n",
    "\n",
    "# Remove documents with no postures\n",
    "df_ml = df[df['posture_list'].apply(len) > 0].copy()\n",
    "print(f\"Documents with postures: {len(df_ml)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4b115e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total unique postures: 230\n",
      "\n",
      "Most common postures:\n",
      "On Appeal                                                         9197\n",
      "Appellate Review                                                  4652\n",
      "Review of Administrative Decision                                 2773\n",
      "Motion to Dismiss                                                 1679\n",
      "Sentencing or Penalty Phase Motion or Objection                   1342\n",
      "Trial or Guilt Phase Motion or Objection                          1097\n",
      "Motion for Attorney's Fees                                         612\n",
      "Post-Trial Hearing Motion                                          512\n",
      "Motion for Preliminary Injunction                                  364\n",
      "Motion to Dismiss for Lack of Subject Matter Jurisdiction          343\n",
      "Motion to Compel Arbitration                                       255\n",
      "Motion for New Trial                                               226\n",
      "Petition to Terminate Parental Rights                              219\n",
      "Motion for Judgment as a Matter of Law (JMOL)/Directed Verdict     212\n",
      "Motion for Reconsideration                                         206\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Analyze posture distribution\n",
    "all_postures_ml = []\n",
    "for postures in df_ml['posture_list']:\n",
    "    all_postures_ml.extend(postures)\n",
    "\n",
    "posture_counts = pd.Series(all_postures_ml).value_counts()\n",
    "print(f\"\\nTotal unique postures: {len(posture_counts)}\")\n",
    "print()\n",
    "print(f\"Most common postures:\")\n",
    "print(posture_counts.head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a11f4a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Postures with >= 100 occurrences: 27\n",
      "['On Appeal', 'Appellate Review', 'Review of Administrative Decision', 'Motion to Dismiss', 'Sentencing or Penalty Phase Motion or Objection', 'Trial or Guilt Phase Motion or Objection', \"Motion for Attorney's Fees\", 'Post-Trial Hearing Motion', 'Motion for Preliminary Injunction', 'Motion to Dismiss for Lack of Subject Matter Jurisdiction', 'Motion to Compel Arbitration', 'Motion for New Trial', 'Petition to Terminate Parental Rights', 'Motion for Judgment as a Matter of Law (JMOL)/Directed Verdict', 'Motion for Reconsideration', 'Motion to Dismiss for Lack of Personal Jurisdiction', 'Motion for Costs', 'Juvenile Delinquency Proceeding', 'Motion for Default Judgment/Order of Default', 'Motion to Dismiss for Lack of Standing', 'Motion to Dismiss for Lack of Jurisdiction', 'Motion to Transfer or Change Venue', 'Petition for Divorce or Dissolution', 'Motion for Contempt', 'Motion for Protective Order', 'Motion for Permanent Injunction', 'Motion to Set Aside or Vacate']\n"
     ]
    }
   ],
   "source": [
    "# Filter to most common postures (those appearing in at least 100 documents)\n",
    "min_frequency = 100\n",
    "common_postures = posture_counts[posture_counts >= min_frequency].index.tolist()\n",
    "print(f\"\\nPostures with >= {min_frequency} occurrences: {len(common_postures)}\")\n",
    "print(common_postures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d8d69416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents after filtering to common postures: 16568\n"
     ]
    }
   ],
   "source": [
    "# Filter documents to only include those with common postures\n",
    "def filter_common_postures(posture_list, common_postures):\n",
    "    \"\"\"Keep only postures that are in the common_postures list\"\"\"\n",
    "    return [p for p in posture_list if p in common_postures]\n",
    "\n",
    "df_ml['filtered_postures'] = df_ml['posture_list'].apply(\n",
    "    lambda x: filter_common_postures(x, common_postures)\n",
    ")\n",
    "\n",
    "# Remove documents that have no common postures after filtering\n",
    "df_ml = df_ml[df_ml['filtered_postures'].apply(len) > 0].copy()\n",
    "print(f\"Documents after filtering to common postures: {len(df_ml)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c65e4e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label matrix shape: (16568, 27)\n",
      "Labels: ['Appellate Review' 'Juvenile Delinquency Proceeding'\n",
      " \"Motion for Attorney's Fees\" 'Motion for Contempt' 'Motion for Costs'\n",
      " 'Motion for Default Judgment/Order of Default'\n",
      " 'Motion for Judgment as a Matter of Law (JMOL)/Directed Verdict'\n",
      " 'Motion for New Trial' 'Motion for Permanent Injunction'\n",
      " 'Motion for Preliminary Injunction' 'Motion for Protective Order'\n",
      " 'Motion for Reconsideration' 'Motion to Compel Arbitration'\n",
      " 'Motion to Dismiss' 'Motion to Dismiss for Lack of Jurisdiction'\n",
      " 'Motion to Dismiss for Lack of Personal Jurisdiction'\n",
      " 'Motion to Dismiss for Lack of Standing'\n",
      " 'Motion to Dismiss for Lack of Subject Matter Jurisdiction'\n",
      " 'Motion to Set Aside or Vacate' 'Motion to Transfer or Change Venue'\n",
      " 'On Appeal' 'Petition for Divorce or Dissolution'\n",
      " 'Petition to Terminate Parental Rights' 'Post-Trial Hearing Motion'\n",
      " 'Review of Administrative Decision'\n",
      " 'Sentencing or Penalty Phase Motion or Objection'\n",
      " 'Trial or Guilt Phase Motion or Objection']\n"
     ]
    }
   ],
   "source": [
    "## Multi-label Classification Setup\n",
    "\n",
    "# Create binary label matrix using MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "y_multilabel = mlb.fit_transform(df_ml['filtered_postures'])\n",
    "\n",
    "print(f\"Label matrix shape: {y_multilabel.shape}\")\n",
    "print(f\"Labels: {mlb.classes_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "26a1650d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_98124 caption {\n",
       "  color: red;\n",
       "  font-size: 15px;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_98124\">\n",
       "  <caption>Distribution of num_postures</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_98124_level0_col0\" class=\"col_heading level0 col0\" >count</th>\n",
       "      <th id=\"T_98124_level0_col1\" class=\"col_heading level0 col1\" >percentage</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >num_postures</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_98124_level0_row0\" class=\"row_heading level0 row0\" >1</th>\n",
       "      <td id=\"T_98124_row0_col0\" class=\"data row0 col0\" >7,649</td>\n",
       "      <td id=\"T_98124_row0_col1\" class=\"data row0 col1\" >46.17%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_98124_level0_row1\" class=\"row_heading level0 row1\" >2</th>\n",
       "      <td id=\"T_98124_row1_col0\" class=\"data row1 col0\" >7,567</td>\n",
       "      <td id=\"T_98124_row1_col1\" class=\"data row1 col1\" >45.67%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_98124_level0_row2\" class=\"row_heading level0 row2\" >3</th>\n",
       "      <td id=\"T_98124_row2_col0\" class=\"data row2 col0\" >1,127</td>\n",
       "      <td id=\"T_98124_row2_col1\" class=\"data row2 col1\" >6.80%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_98124_level0_row3\" class=\"row_heading level0 row3\" >4</th>\n",
       "      <td id=\"T_98124_row3_col0\" class=\"data row3 col0\" >189</td>\n",
       "      <td id=\"T_98124_row3_col1\" class=\"data row3 col1\" >1.14%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_98124_level0_row4\" class=\"row_heading level0 row4\" >5</th>\n",
       "      <td id=\"T_98124_row4_col0\" class=\"data row4 col0\" >32</td>\n",
       "      <td id=\"T_98124_row4_col1\" class=\"data row4 col1\" >0.19%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_98124_level0_row5\" class=\"row_heading level0 row5\" >6</th>\n",
       "      <td id=\"T_98124_row5_col0\" class=\"data row5 col0\" >2</td>\n",
       "      <td id=\"T_98124_row5_col1\" class=\"data row5 col1\" >0.01%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_98124_level0_row6\" class=\"row_heading level0 row6\" >7</th>\n",
       "      <td id=\"T_98124_row6_col0\" class=\"data row6 col0\" >2</td>\n",
       "      <td id=\"T_98124_row6_col1\" class=\"data row6 col1\" >0.01%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f91afd6be00>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_counts = df_ml['num_postures'].value_counts(dropna=False)\n",
    "_pct = df_ml['num_postures'].value_counts(dropna=False,normalize=True) \n",
    "\n",
    "pd.DataFrame({\n",
    "    'count': _counts,\n",
    "    'percentage': _pct\n",
    "}).sort_index().style.format({'count':'{:,}','percentage':'{:.2%}'}).set_caption(\"Distribution of num_postures\")\\\n",
    "    .set_table_styles([{'selector': 'caption','props': [('color', 'red'),('font-size', '15px')]}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3bf28e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 16568\n",
      "Training set: 11597 (70.00%)\n",
      "Validation set: 2485 (15.00%)\n",
      "Test set: 2486 (15.00%)\n"
     ]
    }
   ],
   "source": [
    "# Prepare text data\n",
    "X_text = df_ml['full_text'].values\n",
    "\n",
    "# Split the data\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X_text, y_multilabel, \n",
    "    test_size=0.3, # 30% for temp (which will be split into val and test)\n",
    "    random_state=42, \n",
    "    stratify=None\n",
    ")\n",
    "\n",
    " # Split temp into validation and test (50-50 split of the 30%)\n",
    "# # This gives us 15% each\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp,\n",
    "    test_size=0.5,\n",
    "    random_state=42, \n",
    "    stratify=None\n",
    ")\n",
    "\n",
    "print(f\"Total samples: {len(df_ml)}\")\n",
    "print(f\"Training set: {len(X_train)} ({len(X_train)/len(df_ml):.2%})\")\n",
    "print(f\"Validation set: {len(X_val)} ({len(X_val)/len(df_ml):.2%})\")\n",
    "print(f\"Test set: {len(X_test)} ({len(X_test)/len(df_ml):.2%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a3e4c020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label distribution in training set:\n",
      "Appellate Review: 3310 (28.5%)\n",
      "Juvenile Delinquency Proceeding: 103 (0.9%)\n",
      "Motion for Attorney's Fees: 412 (3.6%)\n",
      "Motion for Contempt: 88 (0.8%)\n",
      "Motion for Costs: 121 (1.0%)\n",
      "Motion for Default Judgment/Order of Default: 101 (0.9%)\n",
      "Motion for Judgment as a Matter of Law (JMOL)/Directed Verdict: 147 (1.3%)\n",
      "Motion for New Trial: 156 (1.3%)\n",
      "Motion for Permanent Injunction: 73 (0.6%)\n",
      "Motion for Preliminary Injunction: 254 (2.2%)\n",
      "Motion for Protective Order: 73 (0.6%)\n",
      "Motion for Reconsideration: 145 (1.3%)\n",
      "Motion to Compel Arbitration: 179 (1.5%)\n",
      "Motion to Dismiss: 1155 (10.0%)\n",
      "Motion to Dismiss for Lack of Jurisdiction: 82 (0.7%)\n",
      "Motion to Dismiss for Lack of Personal Jurisdiction: 138 (1.2%)\n",
      "Motion to Dismiss for Lack of Standing: 87 (0.8%)\n",
      "Motion to Dismiss for Lack of Subject Matter Jurisdiction: 231 (2.0%)\n",
      "Motion to Set Aside or Vacate: 73 (0.6%)\n",
      "Motion to Transfer or Change Venue: 88 (0.8%)\n",
      "On Appeal: 6404 (55.2%)\n",
      "Petition for Divorce or Dissolution: 82 (0.7%)\n",
      "Petition to Terminate Parental Rights: 167 (1.4%)\n",
      "Post-Trial Hearing Motion: 372 (3.2%)\n",
      "Review of Administrative Decision: 1940 (16.7%)\n",
      "Sentencing or Penalty Phase Motion or Objection: 953 (8.2%)\n",
      "Trial or Guilt Phase Motion or Objection: 776 (6.7%)\n"
     ]
    }
   ],
   "source": [
    "# Check label distribution\n",
    "train_label_sums = y_train.sum(axis=0)\n",
    "val_label_sums = y_val.sum(axis=0)\n",
    "test_label_sums = y_test.sum(axis=0)\n",
    "\n",
    "print(\"\\nLabel distribution in training set:\")\n",
    "for i, label in enumerate(mlb.classes_):\n",
    "    print(f\"{label}: {train_label_sums[i]} ({train_label_sums[i]/len(y_train)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5177e708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All arrays saved with pickle!\n"
     ]
    }
   ],
   "source": [
    "## save preprocess data\n",
    "saved_data=os.path.join(os.getcwd(), 'processed_data')\n",
    "os.makedirs(saved_data, exist_ok=True)\n",
    "# Save using pickle\n",
    "with open(os.path.join(saved_data,'train_arrays.pkl'), 'wb') as f:\n",
    "    pickle.dump({'X_train': X_train, 'y_train': y_train, 'label_train': label_train}, f)\n",
    "\n",
    "with open(os.path.join(saved_data,'val_arrays.pkl'), 'wb') as f:\n",
    "    pickle.dump({'X_val': X_val, 'y_val': y_val, 'label_val': label_val}, f)\n",
    "\n",
    "with open(os.path.join(saved_data,'test_arrays.pkl'), 'wb') as f:\n",
    "    pickle.dump({'X_test': X_test, 'y_test': y_test, 'label_test': label_test}, f)\n",
    "\n",
    "with open(os.path.join(saved_data,'class_name.pkl'), 'wb') as f:\n",
    "    pickle.dump({'class_name': mlb.classes_}, f)\n",
    "\n",
    "print(\"All arrays saved with pickle!\")\n",
    "\n",
    "# To load later:\n",
    "# with open(os.path.join(saved_data,'train_arrays.pkl'), 'rb') as f:\n",
    "#     train_data = pickle.load(f)\n",
    "#     X_train = train_data['X_train']\n",
    "#     y_train = train_data['y_train']\n",
    "#     label_train = train_data['label_train']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4273a31e",
   "metadata": {},
   "source": [
    "## Bag-of-word (TFIDF): Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8244ef62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from lightgbm import early_stopping, log_evaluation\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, hamming_loss\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, f1_score, \n",
    "    roc_auc_score, average_precision_score,\n",
    "    hamming_loss, jaccard_score\n",
    ")\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aef25990",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TfidfVectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Create TF-IDF vectorizer\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Using parameters optimized for legal text\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m tfidf = \u001b[43mTfidfVectorizer\u001b[49m(\n\u001b[32m      4\u001b[39m     max_features=\u001b[32m10000\u001b[39m,  \u001b[38;5;66;03m# Limit features for computational efficiency\u001b[39;00m\n\u001b[32m      5\u001b[39m     stop_words=\u001b[33m'\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      6\u001b[39m     ngram_range=(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m),  \u001b[38;5;66;03m# Include unigrams and bigrams\u001b[39;00m\n\u001b[32m      7\u001b[39m     min_df=\u001b[32m5\u001b[39m,           \u001b[38;5;66;03m# Ignore terms that appear in fewer than 5 documents\u001b[39;00m\n\u001b[32m      8\u001b[39m     max_df=\u001b[32m0.95\u001b[39m,        \u001b[38;5;66;03m# Ignore terms that appear in more than 95% of documents\u001b[39;00m\n\u001b[32m      9\u001b[39m     sublinear_tf=\u001b[38;5;28;01mTrue\u001b[39;00m   \u001b[38;5;66;03m# Apply sublinear scaling\u001b[39;00m\n\u001b[32m     10\u001b[39m )\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mFitting TF-IDF vectorizer...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     13\u001b[39m X_train_tfidf = tfidf.fit_transform(X_train)\n",
      "\u001b[31mNameError\u001b[39m: name 'TfidfVectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "# Create TF-IDF vectorizer\n",
    "# Using parameters optimized for legal text\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=10000,  # Limit features for computational efficiency\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 2),  # Include unigrams and bigrams\n",
    "    min_df=5,           # Ignore terms that appear in fewer than 5 documents\n",
    "    max_df=0.95,        # Ignore terms that appear in more than 95% of documents\n",
    "    sublinear_tf=True   # Apply sublinear scaling\n",
    ")\n",
    "\n",
    "print(\"Fitting TF-IDF vectorizer...\")\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_val_tfidf = tfidf.transform(X_val)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "print(f\"TF-IDF matrix shape (train): {X_train_tfidf.shape}\")\n",
    "print(f\"TF-IDF matrix shape (val): {X_val_tfidf.shape}\")\n",
    "print(f\"TF-IDF matrix shape (test): {X_test_tfidf.shape}\")\n",
    "print(f\"Vocabulary size: {len(tfidf.vocabulary_)}\")\n",
    "\n",
    "# Show some sample features\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "print(f\"\\nSample features: {feature_names[:20]}\")\n",
    "print(f\"Last features: {feature_names[-20:]}\")\n",
    "\n",
    "def comprehensive_evaluation(y_true, y_pred_binary, y_pred_proba, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation function for multi-label classification.\n",
    "    \n",
    "    Args:\n",
    "        y_true: Ground truth binary labels (n_samples, n_labels)\n",
    "        y_pred_binary: Predicted binary labels (n_samples, n_labels) \n",
    "        y_pred_proba: Predicted probabilities (n_samples, n_labels)\n",
    "        threshold: Threshold for converting probabilities to binary (default: 0.5)\n",
    "    \n",
    "    Returns:\n",
    "        dict: Comprehensive metrics including all averaging methods\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    from sklearn.metrics import (\n",
    "        precision_score, recall_score, f1_score, accuracy_score,\n",
    "        hamming_loss, jaccard_score, roc_auc_score, average_precision_score\n",
    "    )\n",
    "    \n",
    "    # Ensure inputs are numpy arrays\n",
    "    y_true = np.array(y_true, dtype=int)\n",
    "    y_pred_binary = np.array(y_pred_binary, dtype=int)\n",
    "    y_pred_proba = np.array(y_pred_proba, dtype=float)\n",
    "    \n",
    "    metrics = {}\n",
    "    \n",
    "    try:\n",
    "        # SAMPLES AVERAGE (per-sample then average across samples)\n",
    "        metrics['precision_samples'] = precision_score(y_true, y_pred_binary, average='samples', zero_division=0)\n",
    "        metrics['recall_samples'] = recall_score(y_true, y_pred_binary, average='samples', zero_division=0)\n",
    "        metrics['f1_samples'] = f1_score(y_true, y_pred_binary, average='samples', zero_division=0)\n",
    "        \n",
    "        # MICRO AVERAGE (global average)\n",
    "        metrics['precision_micro'] = precision_score(y_true, y_pred_binary, average='micro', zero_division=0)\n",
    "        metrics['recall_micro'] = recall_score(y_true, y_pred_binary, average='micro', zero_division=0)\n",
    "        metrics['f1_micro'] = f1_score(y_true, y_pred_binary, average='micro', zero_division=0)\n",
    "        \n",
    "        # MACRO AVERAGE (unweighted average across labels)\n",
    "        metrics['precision_macro'] = precision_score(y_true, y_pred_binary, average='macro', zero_division=0)\n",
    "        metrics['recall_macro'] = recall_score(y_true, y_pred_binary, average='macro', zero_division=0)\n",
    "        metrics['f1_macro'] = f1_score(y_true, y_pred_binary, average='macro', zero_division=0)\n",
    "        \n",
    "        # WEIGHTED AVERAGE (weighted by support)\n",
    "        metrics['precision_weighted'] = precision_score(y_true, y_pred_binary, average='weighted', zero_division=0)\n",
    "        metrics['recall_weighted'] = recall_score(y_true, y_pred_binary, average='weighted', zero_division=0)\n",
    "        metrics['f1_weighted'] = f1_score(y_true, y_pred_binary, average='weighted', zero_division=0)\n",
    "        \n",
    "        # ACCURACY METRICS\n",
    "        metrics['accuracy'] = accuracy_score(y_true, y_pred_binary)\n",
    "        metrics['hamming_loss'] = hamming_loss(y_true, y_pred_binary)\n",
    "        \n",
    "        # JACCARD (IoU) METRICS \n",
    "        metrics['jaccard_samples'] = jaccard_score(y_true, y_pred_binary, average='samples', zero_division=0)\n",
    "        metrics['jaccard_macro'] = jaccard_score(y_true, y_pred_binary, average='macro', zero_division=0)\n",
    "        metrics['jaccard_weighted'] = jaccard_score(y_true, y_pred_binary, average='weighted', zero_division=0)\n",
    "        \n",
    "        # ROC-AUC METRICS (using probabilities)\n",
    "        try:\n",
    "            metrics['roc_auc_micro'] = roc_auc_score(y_true, y_pred_proba, average='micro')\n",
    "            metrics['roc_auc_macro'] = roc_auc_score(y_true, y_pred_proba, average='macro')\n",
    "            metrics['roc_auc_weighted'] = roc_auc_score(y_true, y_pred_proba, average='weighted')\n",
    "            metrics['roc_auc_samples'] = roc_auc_score(y_true, y_pred_proba, average='samples')\n",
    "        except ValueError as e:\n",
    "            print(f\"Warning: ROC-AUC calculation failed: {e}\")\n",
    "            metrics['roc_auc_micro'] = 0.0\n",
    "            metrics['roc_auc_macro'] = 0.0\n",
    "            metrics['roc_auc_weighted'] = 0.0\n",
    "            metrics['roc_auc_samples'] = 0.0\n",
    "        \n",
    "        # PR-AUC METRICS (using probabilities)\n",
    "        try:\n",
    "            metrics['pr_auc_micro'] = average_precision_score(y_true, y_pred_proba, average='micro')\n",
    "            metrics['pr_auc_macro'] = average_precision_score(y_true, y_pred_proba, average='macro')\n",
    "            metrics['pr_auc_weighted'] = average_precision_score(y_true, y_pred_proba, average='weighted')\n",
    "            metrics['pr_auc_samples'] = average_precision_score(y_true, y_pred_proba, average='samples')\n",
    "        except ValueError as e:\n",
    "            print(f\"Warning: PR-AUC calculation failed: {e}\")\n",
    "            metrics['pr_auc_micro'] = 0.0\n",
    "            metrics['pr_auc_macro'] = 0.0\n",
    "            metrics['pr_auc_weighted'] = 0.0\n",
    "            metrics['pr_auc_samples'] = 0.0\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in comprehensive_evaluation: {e}\")\n",
    "        # Return minimal metrics if calculation fails\n",
    "        metrics = {\n",
    "            'precision_micro': 0.0, 'recall_micro': 0.0, 'f1_micro': 0.0,\n",
    "            'precision_macro': 0.0, 'recall_macro': 0.0, 'f1_macro': 0.0,\n",
    "            'accuracy': 0.0, 'hamming_loss': 1.0\n",
    "        }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "print(\"✅ Comprehensive evaluation function updated and ready to use!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92f09a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_evaluation(y_true, y_pred_proba, y_pred_binary=None, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation for multi-label classification with all averaging methods\n",
    "    \"\"\"\n",
    "    if y_pred_binary is None:\n",
    "        y_pred_binary = (y_pred_proba >= threshold).astype(int)\n",
    "    \n",
    "    metrics = {}\n",
    "    \n",
    "    # SAMPLES AVERAGE (per-sample then average across samples)\n",
    "    metrics['precision_samples'] = precision_score(y_true, y_pred_binary, average='samples', zero_division=0)\n",
    "    metrics['recall_samples'] = recall_score(y_true, y_pred_binary, average='samples', zero_division=0)\n",
    "    metrics['f1_samples'] = f1_score(y_true, y_pred_binary, average='samples', zero_division=0)\n",
    "    \n",
    "    # MICRO AVERAGE (global aggregation)\n",
    "    metrics['precision_micro'] = precision_score(y_true, y_pred_binary, average='micro', zero_division=0)\n",
    "    metrics['recall_micro'] = recall_score(y_true, y_pred_binary, average='micro', zero_division=0)\n",
    "    metrics['f1_micro'] = f1_score(y_true, y_pred_binary, average='micro', zero_division=0)\n",
    "    \n",
    "    # MACRO AVERAGE (unweighted average across labels)\n",
    "    metrics['precision_macro'] = precision_score(y_true, y_pred_binary, average='macro', zero_division=0)\n",
    "    metrics['recall_macro'] = recall_score(y_true, y_pred_binary, average='macro', zero_division=0)\n",
    "    metrics['f1_macro'] = f1_score(y_true, y_pred_binary, average='macro', zero_division=0)\n",
    "    \n",
    "    # WEIGHTED AVERAGE (weighted by support/frequency)\n",
    "    metrics['precision_weighted'] = precision_score(y_true, y_pred_binary, average='weighted', zero_division=0)\n",
    "    metrics['recall_weighted'] = recall_score(y_true, y_pred_binary, average='weighted', zero_division=0)\n",
    "    metrics['f1_weighted'] = f1_score(y_true, y_pred_binary, average='weighted', zero_division=0)\n",
    "    \n",
    "    # ROC-AUC (multiple averaging methods)\n",
    "    try:\n",
    "        metrics['roc_auc_macro'] = roc_auc_score(y_true, y_pred_proba, average='macro')\n",
    "        metrics['roc_auc_weighted'] = roc_auc_score(y_true, y_pred_proba, average='weighted')\n",
    "        metrics['roc_auc_samples'] = roc_auc_score(y_true, y_pred_proba, average='samples')\n",
    "    except ValueError as e:\n",
    "        print(f\"ROC-AUC calculation failed: {e}\")\n",
    "        metrics['roc_auc_macro'] = 0.0\n",
    "        metrics['roc_auc_weighted'] = 0.0\n",
    "        metrics['roc_auc_samples'] = 0.0\n",
    "    \n",
    "    # Precision-Recall AUC (multiple averaging methods)\n",
    "    try:\n",
    "        metrics['pr_auc_macro'] = average_precision_score(y_true, y_pred_proba, average='macro')\n",
    "        metrics['pr_auc_weighted'] = average_precision_score(y_true, y_pred_proba, average='weighted')\n",
    "        metrics['pr_auc_samples'] = average_precision_score(y_true, y_pred_proba, average='samples')\n",
    "    except ValueError as e:\n",
    "        print(f\"PR-AUC calculation failed: {e}\")\n",
    "        metrics['pr_auc_macro'] = 0.0\n",
    "        metrics['pr_auc_weighted'] = 0.0\n",
    "        metrics['pr_auc_samples'] = 0.0\n",
    "    \n",
    "    # Hamming Loss (inherently micro-averaged)\n",
    "    metrics['hamming_loss'] = hamming_loss(y_true, y_pred_binary)\n",
    "    \n",
    "    # Jaccard Score (multiple averaging methods)\n",
    "    metrics['jaccard_samples'] = jaccard_score(y_true, y_pred_binary, average='samples', zero_division=0)\n",
    "    metrics['jaccard_macro'] = jaccard_score(y_true, y_pred_binary, average='macro', zero_division=0)\n",
    "    metrics['jaccard_weighted'] = jaccard_score(y_true, y_pred_binary, average='weighted', zero_division=0)\n",
    "    \n",
    "    # Note: micro average for Jaccard in multi-label is not directly supported in sklearn\n",
    "    # but can be calculated manually if needed\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c020c01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define models to test with optimized hyperparameters and validation-aware training\n",
    "# models = {\n",
    "#     'Logistic Regression': OneVsRestClassifier(\n",
    "#         LogisticRegression(\n",
    "#             random_state=42, \n",
    "#             max_iter=1000,\n",
    "#             C=1.0,\n",
    "#             solver='liblinear'\n",
    "#         )\n",
    "#     ),\n",
    "#     'Random Forest': OneVsRestClassifier(\n",
    "#         RandomForestClassifier(\n",
    "#             n_estimators=100, \n",
    "#             random_state=42, \n",
    "#             n_jobs=-1,\n",
    "#             max_depth=10,\n",
    "#             min_samples_split=5,\n",
    "#             min_samples_leaf=2,\n",
    "#             # Additional overfitting control\n",
    "#             min_impurity_decrease=0.0001,\n",
    "#             max_features='sqrt'\n",
    "#         )\n",
    "#     ),\n",
    "#     'XGBoost': OneVsRestClassifier(\n",
    "#         xgb.XGBClassifier(\n",
    "#             random_state=42,\n",
    "#             n_estimators=100,\n",
    "#             max_depth=6,\n",
    "#             learning_rate=0.1,\n",
    "#             subsample=0.8,\n",
    "#             colsample_bytree=0.8,\n",
    "#             eval_metric='logloss',\n",
    "#             verbosity=0,\n",
    "#             # Early stopping will be handled in training loop\n",
    "#             early_stopping_rounds=10\n",
    "#         )\n",
    "#     ),\n",
    "#     'LightGBM': OneVsRestClassifier(\n",
    "#         lgb.LGBMClassifier(\n",
    "#             random_state=42,\n",
    "#             n_estimators=100,\n",
    "#             max_depth=6,\n",
    "#             learning_rate=0.1,\n",
    "#             subsample=0.8,\n",
    "#             colsample_bytree=0.8,\n",
    "#             verbosity=-1,\n",
    "#             # Early stopping will be handled in training loop\n",
    "#             early_stopping_rounds=10\n",
    "#         )\n",
    "#     )\n",
    "# }\n",
    "\n",
    "# # Enhanced training function with validation monitoring\n",
    "# def train_with_validation_control(model, X_train, y_train, X_val, y_val, model_name):\n",
    "#     \"\"\"\n",
    "#     Train model with validation monitoring to control overfitting\n",
    "#     \"\"\"\n",
    "#     print(f\"\\nTraining {model_name} with validation control...\")\n",
    "    \n",
    "#     if model_name in ['XGBoost', 'LightGBM']:\n",
    "#         # For tree-based models, we can use early stopping\n",
    "#         if model_name == 'XGBoost':\n",
    "#             # XGBoost with early stopping\n",
    "#             for i, estimator in enumerate(model.estimators_):\n",
    "#                 print(f\"  Training label {i+1}/{len(model.estimators_)}\")\n",
    "                \n",
    "#                 # Get single label\n",
    "#                 y_train_single = y_train[:, i]\n",
    "#                 y_val_single = y_val[:, i]\n",
    "                \n",
    "#                 # Only train if there are positive samples\n",
    "#                 if y_train_single.sum() > 0:\n",
    "#                     estimator.fit(\n",
    "#                         X_train, y_train_single,\n",
    "#                         eval_set=[(X_val, y_val_single)],\n",
    "#                         verbose=False\n",
    "#                     )\n",
    "#                 else:\n",
    "#                     # For labels with no positive samples, create a dummy classifier\n",
    "#                     estimator.fit(X_train[:10], y_train_single[:10])\n",
    "        \n",
    "#         elif model_name == 'LightGBM':\n",
    "#             # LightGBM with early stopping\n",
    "#             for i, estimator in enumerate(model.estimators_):\n",
    "#                 print(f\"  Training label {i+1}/{len(model.estimators_)}\")\n",
    "                \n",
    "#                 # Get single label\n",
    "#                 y_train_single = y_train[:, i]\n",
    "#                 y_val_single = y_val[:, i]\n",
    "                \n",
    "#                 # Only train if there are positive samples\n",
    "#                 if y_train_single.sum() > 0:\n",
    "#                     estimator.fit(\n",
    "#                         X_train, y_train_single,\n",
    "#                         eval_set=[(X_val, y_val_single)],\n",
    "#                         callbacks=[\n",
    "#                             early_stopping(10, verbose=False),\n",
    "#                             log_evaluation(0)  # No logging\n",
    "#                         ]\n",
    "#                     )\n",
    "#                 else:\n",
    "#                     # For labels with no positive samples, create a dummy classifier\n",
    "#                     estimator.fit(X_train[:10], y_train_single[:10])\n",
    "#     else:\n",
    "#         # For other models, use regular training\n",
    "#         model.fit(X_train, y_train)\n",
    "    \n",
    "#     return model\n",
    "\n",
    "# # Store results with validation tracking\n",
    "# results = {}\n",
    "# validation_scores = {}\n",
    "\n",
    "# print(\"Training and evaluating models with validation control...\")\n",
    "# print(\"=\"*60)\n",
    "# print(\"Models to evaluate:\")\n",
    "# for name in models.keys():\n",
    "#     print(f\"  • {name}\")\n",
    "# print()\n",
    "\n",
    "# for name, model in models.items():\n",
    "#     # Train with validation control\n",
    "#     if name in ['XGBoost', 'LightGBM']:\n",
    "#         # For tree-based models, we need to handle OneVsRestClassifier manually\n",
    "#         # to implement early stopping properly\n",
    "#         trained_model = OneVsRestClassifier(\n",
    "#             model.estimator,\n",
    "#             n_jobs=1  # Sequential to handle early stopping\n",
    "#         )\n",
    "#         trained_model.fit(X_train_tfidf, y_train)\n",
    "#     else:\n",
    "#         trained_model = model\n",
    "#         trained_model.fit(X_train_tfidf, y_train)\n",
    "    \n",
    "#     # Make predictions on all sets\n",
    "#     y_pred_train = trained_model.predict(X_train_tfidf)\n",
    "#     y_pred_val = trained_model.predict(X_val_tfidf)\n",
    "#     y_pred_test = trained_model.predict(X_test_tfidf)\n",
    "    \n",
    "#     # Calculate metrics for all sets\n",
    "#     train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "#     val_accuracy = accuracy_score(y_val, y_pred_val)\n",
    "#     test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "    \n",
    "#     train_hamming = hamming_loss(y_train, y_pred_train)\n",
    "#     val_hamming = hamming_loss(y_val, y_pred_val)\n",
    "#     test_hamming = hamming_loss(y_test, y_pred_test)\n",
    "    \n",
    "#     # Calculate F1 scores\n",
    "#     train_f1_micro = f1_score(y_train, y_pred_train, average='micro')\n",
    "#     val_f1_micro = f1_score(y_val, y_pred_val, average='micro')\n",
    "#     test_f1_micro = f1_score(y_test, y_pred_test, average='micro')\n",
    "    \n",
    "#     # Store results\n",
    "#     results[name] = {\n",
    "#         'model': trained_model,\n",
    "#         'train_accuracy': train_accuracy,\n",
    "#         'val_accuracy': val_accuracy,\n",
    "#         'test_accuracy': test_accuracy,\n",
    "#         'train_hamming_loss': train_hamming,\n",
    "#         'val_hamming_loss': val_hamming,\n",
    "#         'test_hamming_loss': test_hamming,\n",
    "#         'train_f1_micro': train_f1_micro,\n",
    "#         'val_f1_micro': val_f1_micro,\n",
    "#         'test_f1_micro': test_f1_micro,\n",
    "#         'y_pred_test': y_pred_test,\n",
    "#         'y_pred_val': y_pred_val\n",
    "#     }\n",
    "    \n",
    "#     # Check for overfitting\n",
    "#     accuracy_gap = train_accuracy - val_accuracy\n",
    "#     f1_gap = train_f1_micro - val_f1_micro\n",
    "    \n",
    "#     overfitting_status = \"✅ Good\" if accuracy_gap < 0.05 else \"⚠️ Moderate\" if accuracy_gap < 0.1 else \"🚨 High\"\n",
    "    \n",
    "#     print(f\"\\n{name} Results:\")\n",
    "#     print(f\"  Train Accuracy: {train_accuracy:.4f}\")\n",
    "#     print(f\"  Val Accuracy:   {val_accuracy:.4f}\")\n",
    "#     print(f\"  Test Accuracy:  {test_accuracy:.4f}\")\n",
    "#     print(f\"  Train-Val Gap:  {accuracy_gap:.4f} ({overfitting_status})\")\n",
    "#     print(f\"  Train F1:       {train_f1_micro:.4f}\")\n",
    "#     print(f\"  Val F1:         {val_f1_micro:.4f}\")\n",
    "#     print(f\"  Test F1:        {test_f1_micro:.4f}\")\n",
    "#     print(f\"  F1 Gap:         {f1_gap:.4f}\")\n",
    "\n",
    "# print(\"\\n\" + \"=\"*80)\n",
    "# print(\"Model Comparison with Overfitting Analysis:\")\n",
    "# print(f\"{'Model':<15} | {'Test Acc':<8} | {'Val Acc':<8} | {'Gap':<6} | {'Status':<12} | {'Performance':<12}\")\n",
    "# print(\"-\" * 85)\n",
    "\n",
    "# # Sort results by validation accuracy (better indicator than test accuracy)\n",
    "# sorted_results = sorted(results.items(), key=lambda x: x[1]['val_accuracy'], reverse=True)\n",
    "\n",
    "# for name, result in sorted_results:\n",
    "#     gap = result['train_accuracy'] - result['val_accuracy']\n",
    "#     status = \"Good\" if gap < 0.05 else \"Moderate\" if gap < 0.1 else \"High\"\n",
    "#     performance = \"🥇 Best\" if name == sorted_results[0][0] else \"🥈 Good\" if result['val_accuracy'] > 0.55 else \"⚠️ Poor\"\n",
    "#     print(f\"{name:<15} | {result['test_accuracy']:<8.4f} | {result['val_accuracy']:<8.4f} | {gap:<6.4f} | {status:<12} | {performance}\")\n",
    "\n",
    "# # Identify best model based on validation performance\n",
    "# best_model_name = sorted_results[0][0]\n",
    "# best_model = sorted_results[0][1]['model']\n",
    "# print(f\"\\n🏆 Best performing model (based on validation): {best_model_name}\")\n",
    "# print(f\"   Validation Accuracy: {sorted_results[0][1]['val_accuracy']:.4f}\")\n",
    "# print(f\"   Test Accuracy: {sorted_results[0][1]['test_accuracy']:.4f}\")\n",
    "# print(f\"   Overfitting Gap: {sorted_results[0][1]['train_accuracy'] - sorted_results[0][1]['val_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3da6407",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, f1_score, \n",
    "    roc_auc_score, average_precision_score,\n",
    "    hamming_loss, jaccard_score, accuracy_score\n",
    ")\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from tqdm import tqdm\n",
    "\n",
    "class Train_XGBoost(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"XGBoost classifier with validation-based early stopping for multi-label\"\"\"\n",
    "    \n",
    "    def __init__(self, **xgb_params):\n",
    "        self.xgb_params = xgb_params\n",
    "        self.models_ = []\n",
    "        self.n_classes_ = None\n",
    "        \n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        if len(y.shape) == 1:\n",
    "            y = y.reshape(-1, 1)\n",
    "        if X_val is not None and len(y_val.shape) == 1:\n",
    "            y_val = y_val.reshape(-1, 1)\n",
    "            \n",
    "        self.n_classes_ = y.shape[1]\n",
    "        self.models_ = []\n",
    "        \n",
    "        for i in tqdm(range(self.n_classes_), total=self.n_classes_, leave=True, position=0):\n",
    "            \n",
    "            y_single = y[:, i]\n",
    "            \n",
    "            # Skip if no positive samples\n",
    "            if y_single.sum() == 0:\n",
    "                self.models_.append(None)\n",
    "                continue\n",
    "            \n",
    "            model = xgb.XGBClassifier(**self.xgb_params)\n",
    "            \n",
    "            if X_val is not None and y_val is not None:\n",
    "                y_val_single = y_val[:, i]\n",
    "                model.fit(\n",
    "                    X, y_single,\n",
    "                    eval_set=[(X_val, y_val_single)],\n",
    "                    verbose=False\n",
    "                )\n",
    "            else:\n",
    "                model.fit(X, y_single)\n",
    "            \n",
    "            self.models_.append(model)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predictions = np.zeros((X.shape[0], self.n_classes_))\n",
    "        \n",
    "        for i, model in enumerate(self.models_):\n",
    "            if model is not None:\n",
    "                predictions[:, i] = model.predict(X)\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        probabilities = np.zeros((X.shape[0], self.n_classes_))\n",
    "        \n",
    "        for i, model in enumerate(self.models_):\n",
    "            if model is not None:\n",
    "                proba = model.predict_proba(X)\n",
    "                # Handle case where only one class is present\n",
    "                if proba.shape[1] == 1:\n",
    "                    probabilities[:, i] = 0  # All negative class\n",
    "                else:\n",
    "                    probabilities[:, i] = proba[:, 1]  # Positive class probability\n",
    "        \n",
    "        return probabilities\n",
    "\n",
    "class Train_LGBM(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"LightGBM classifier with validation-based early stopping for multi-label\"\"\"\n",
    "    \n",
    "    def __init__(self, **lgb_params):\n",
    "        self.lgb_params = lgb_params\n",
    "        self.models_ = []\n",
    "        self.n_classes_ = None\n",
    "        \n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        if len(y.shape) == 1:\n",
    "            y = y.reshape(-1, 1)\n",
    "        if X_val is not None and len(y_val.shape) == 1:\n",
    "            y_val = y_val.reshape(-1, 1)\n",
    "            \n",
    "        self.n_classes_ = y.shape[1]\n",
    "        self.models_ = []\n",
    "        \n",
    "        for i in tqdm(range(self.n_classes_), total=self.n_classes_, leave=True, position=0):\n",
    "            \n",
    "            y_single = y[:, i]\n",
    "            \n",
    "            # Skip if no positive samples\n",
    "            if y_single.sum() == 0:\n",
    "                self.models_.append(None)\n",
    "                continue\n",
    "            \n",
    "            model = lgb.LGBMClassifier(**self.lgb_params)\n",
    "            \n",
    "            if X_val is not None and y_val is not None:\n",
    "                y_val_single = y_val[:, i]\n",
    "                model.fit(\n",
    "                    X, y_single,\n",
    "                    eval_set=[(X_val, y_val_single)],\n",
    "                    callbacks=[\n",
    "                        lgb.early_stopping(10, verbose=False),\n",
    "                        lgb.log_evaluation(0)\n",
    "                    ]\n",
    "                )\n",
    "            else:\n",
    "                model.fit(X, y_single)\n",
    "            \n",
    "            self.models_.append(model)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predictions = np.zeros((X.shape[0], self.n_classes_))\n",
    "        \n",
    "        for i, model in enumerate(self.models_):\n",
    "            if model is not None:\n",
    "                predictions[:, i] = model.predict(X)\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        probabilities = np.zeros((X.shape[0], self.n_classes_))\n",
    "        \n",
    "        for i, model in enumerate(self.models_):\n",
    "            if model is not None:\n",
    "                proba = model.predict_proba(X)\n",
    "                # Handle case where only one class is present\n",
    "                if proba.shape[1] == 1:\n",
    "                    probabilities[:, i] = 0  # All negative class\n",
    "                else:\n",
    "                    probabilities[:, i] = proba[:, 1]  # Positive class probability\n",
    "        \n",
    "        return probabilities\n",
    "\n",
    "class Train_logistic(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"Logistic Regression classifier with validation monitoring for multi-label\"\"\"\n",
    "    \n",
    "    def __init__(self, **lr_params):\n",
    "        self.lr_params = lr_params\n",
    "        self.models_ = []\n",
    "        self.n_classes_ = None\n",
    "        self.validation_scores_ = []\n",
    "        \n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        if len(y.shape) == 1:\n",
    "            y = y.reshape(-1, 1)\n",
    "        if X_val is not None and len(y_val.shape) == 1:\n",
    "            y_val = y_val.reshape(-1, 1)\n",
    "            \n",
    "        self.n_classes_ = y.shape[1]\n",
    "        self.models_ = []\n",
    "        self.validation_scores_ = []\n",
    "        \n",
    "        for i in tqdm(range(self.n_classes_), total=self.n_classes_, leave=True, position=0):\n",
    "            \n",
    "            y_single = y[:, i]\n",
    "            \n",
    "            # Skip if no positive samples\n",
    "            if y_single.sum() == 0:\n",
    "                self.models_.append(None)\n",
    "                self.validation_scores_.append(0.0)\n",
    "                continue\n",
    "            \n",
    "            model = LogisticRegression(**self.lr_params)\n",
    "            model.fit(X, y_single)\n",
    "            \n",
    "            # Calculate validation score if validation data provided\n",
    "            if X_val is not None and y_val is not None:\n",
    "                y_val_single = y_val[:, i]\n",
    "                val_score = model.score(X_val, y_val_single)\n",
    "                self.validation_scores_.append(val_score)\n",
    "            else:\n",
    "                self.validation_scores_.append(None)\n",
    "            \n",
    "            self.models_.append(model)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predictions = np.zeros((X.shape[0], self.n_classes_))\n",
    "        \n",
    "        for i, model in enumerate(self.models_):\n",
    "            if model is not None:\n",
    "                predictions[:, i] = model.predict(X)\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        probabilities = np.zeros((X.shape[0], self.n_classes_))\n",
    "        \n",
    "        for i, model in enumerate(self.models_):\n",
    "            if model is not None:\n",
    "                proba = model.predict_proba(X)\n",
    "                # Handle case where only one class is present\n",
    "                if proba.shape[1] == 1:\n",
    "                    probabilities[:, i] = 0  # All negative class\n",
    "                else:\n",
    "                    probabilities[:, i] = proba[:, 1]  # Positive class probability\n",
    "        \n",
    "        return probabilities\n",
    "    \n",
    "    def get_validation_scores(self):\n",
    "        \"\"\"Return validation scores for each label\"\"\"\n",
    "        return self.validation_scores_\n",
    "\n",
    "class Train_RandomForest(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"Random Forest classifier with validation monitoring for multi-label\"\"\"\n",
    "    \n",
    "    def __init__(self, **rf_params):\n",
    "        self.rf_params = rf_params\n",
    "        self.models_ = []\n",
    "        self.n_classes_ = None\n",
    "        self.validation_scores_ = []\n",
    "        self.feature_importances_ = []\n",
    "        \n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        if len(y.shape) == 1:\n",
    "            y = y.reshape(-1, 1)\n",
    "        if X_val is not None and len(y_val.shape) == 1:\n",
    "            y_val = y_val.reshape(-1, 1)\n",
    "            \n",
    "        self.n_classes_ = y.shape[1]\n",
    "        self.models_ = []\n",
    "        self.validation_scores_ = []\n",
    "        self.feature_importances_ = []\n",
    "        \n",
    "        for i in tqdm(range(self.n_classes_), total=self.n_classes_, leave=True, position=0):\n",
    "            \n",
    "            y_single = y[:, i]\n",
    "            \n",
    "            # Skip if no positive samples\n",
    "            if y_single.sum() == 0:\n",
    "                self.models_.append(None)\n",
    "                self.validation_scores_.append(0.0)\n",
    "                self.feature_importances_.append(None)\n",
    "                continue\n",
    "            \n",
    "            model = RandomForestClassifier(**self.rf_params)\n",
    "            model.fit(X, y_single)\n",
    "            \n",
    "            # Store feature importances\n",
    "            self.feature_importances_.append(model.feature_importances_)\n",
    "            \n",
    "            # Calculate validation score if validation data provided\n",
    "            if X_val is not None and y_val is not None:\n",
    "                y_val_single = y_val[:, i]\n",
    "                val_score = model.score(X_val, y_val_single)\n",
    "                self.validation_scores_.append(val_score)\n",
    "            else:\n",
    "                self.validation_scores_.append(None)\n",
    "            \n",
    "            self.models_.append(model)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predictions = np.zeros((X.shape[0], self.n_classes_))\n",
    "        \n",
    "        for i, model in enumerate(self.models_):\n",
    "            if model is not None:\n",
    "                predictions[:, i] = model.predict(X)\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        probabilities = np.zeros((X.shape[0], self.n_classes_))\n",
    "        \n",
    "        for i, model in enumerate(self.models_):\n",
    "            if model is not None:\n",
    "                proba = model.predict_proba(X)\n",
    "                # Handle case where only one class is present\n",
    "                if proba.shape[1] == 1:\n",
    "                    probabilities[:, i] = 0  # All negative class\n",
    "                else:\n",
    "                    probabilities[:, i] = proba[:, 1]  # Positive class probability\n",
    "        \n",
    "        return probabilities\n",
    "    \n",
    "    def get_validation_scores(self):\n",
    "        \"\"\"Return validation scores for each label\"\"\"\n",
    "        return self.validation_scores_\n",
    "    \n",
    "    def get_feature_importances(self):\n",
    "        \"\"\"Return feature importances for each label\"\"\"\n",
    "        return self.feature_importances_\n",
    "\n",
    "def training_function_with_validation(X_train, y_train, X_val, y_val, model_type='lightgbm'):\n",
    "    \"\"\"\n",
    "    Enhanced training function with proper validation control for multi-label classification\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Training {model_type} with validation control...\")\n",
    "    print(f\"X_train shape: {X_train.shape}\")\n",
    "    print(f\"y_train shape: {y_train.shape}\")\n",
    "    print(f\"X_val shape: {X_val.shape}\")\n",
    "    print(f\"y_val shape: {y_val.shape}\")\n",
    "    \n",
    "    if model_type == 'lightgbm':\n",
    "        model = Train_LGBM(\n",
    "            random_state=42,\n",
    "            n_estimators=200,  # More estimators for early stopping\n",
    "            max_depth=6,\n",
    "            learning_rate=0.1,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            verbosity=-1,\n",
    "            early_stopping_rounds=10\n",
    "        )\n",
    "    elif model_type == 'xgboost':\n",
    "        model = Train_XGBoost(\n",
    "            random_state=42,\n",
    "            n_estimators=200,  # More estimators for early stopping\n",
    "            max_depth=6,\n",
    "            learning_rate=0.1,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            eval_metric='logloss',\n",
    "            verbosity=0,\n",
    "            early_stopping_rounds=10\n",
    "        )\n",
    "    elif model_type == 'logistic':\n",
    "        model = Train_logistic(\n",
    "            random_state=42,\n",
    "            max_iter=1000,\n",
    "            C=1.0,\n",
    "            solver='liblinear',\n",
    "            class_weight='balanced'  # Handle class imbalance\n",
    "        )\n",
    "    elif model_type == 'randomforest':\n",
    "        model = Train_RandomForest(\n",
    "            random_state=42,\n",
    "            n_estimators=100,\n",
    "            max_depth=10,\n",
    "            min_samples_split=5,\n",
    "            min_samples_leaf=2,\n",
    "            max_features='sqrt',\n",
    "            class_weight='balanced',  # Handle class imbalance\n",
    "            n_jobs=-1\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"Supported model types: 'lightgbm', 'xgboost', 'logistic', 'randomforest'\")\n",
    "    \n",
    "    # Fit with validation data\n",
    "    model.fit(X_train, y_train, X_val, y_val)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_val = model.predict(X_val)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_acc = accuracy_score(y_train, y_pred_train)\n",
    "    val_acc = accuracy_score(y_val, y_pred_val)\n",
    "    train_f1 = f1_score(y_train, y_pred_train, average='micro')\n",
    "    val_f1 = f1_score(y_val, y_pred_val, average='micro')\n",
    "    \n",
    "    # Calculate hamming loss (lower is better)\n",
    "    train_hamming = hamming_loss(y_train, y_pred_train)\n",
    "    val_hamming = hamming_loss(y_val, y_pred_val)\n",
    "    \n",
    "    # Calculate overfitting gaps for different metrics\n",
    "    accuracy_gap = train_acc - val_acc\n",
    "    f1_gap = train_f1 - val_f1\n",
    "    hamming_gap = val_hamming - train_hamming  # Note: val - train because lower hamming is better\n",
    "    \n",
    "    print(f\"Training completed!\")\n",
    "    print(f\"Train Accuracy: {train_acc:.4f}\")\n",
    "    print(f\"Val Accuracy: {val_acc:.4f}\")\n",
    "    print(f\"Train F1: {train_f1:.4f}\")\n",
    "    print(f\"Val F1: {val_f1:.4f}\")\n",
    "    print(f\"Train Hamming Loss: {train_hamming:.4f}\")\n",
    "    print(f\"Val Hamming Loss: {val_hamming:.4f}\")\n",
    "    print(f\"Overfitting Gap (Accuracy): {accuracy_gap:.4f}\")\n",
    "    print(f\"Overfitting Gap (F1): {f1_gap:.4f}\")\n",
    "    print(f\"Overfitting Gap (Hamming): {hamming_gap:.4f}\")\n",
    "    \n",
    "    return model, {\n",
    "        'train_accuracy': train_acc,\n",
    "        'val_accuracy': val_acc,\n",
    "        'train_f1': train_f1,\n",
    "        'val_f1': val_f1,\n",
    "        'train_hamming_loss': train_hamming,\n",
    "        'val_hamming_loss': val_hamming,\n",
    "        'accuracy_gap': accuracy_gap,\n",
    "        'f1_gap': f1_gap,\n",
    "        'hamming_gap': hamming_gap,\n",
    "        'overfitting_gap': hamming_gap  # Use hamming gap as primary overfitting indicator\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d9918bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting comprehensive model comparison...\n",
      "🚀 COMPREHENSIVE MODEL COMPARISON WITH VALIDATION CONTROL\n",
      "================================================================================\n",
      "\n",
      "============================================================\n",
      "🔧 Training LOGISTIC Model\n",
      "============================================================\n",
      "Training logistic with validation control...\n",
      "X_train shape: (11597, 10000)\n",
      "y_train shape: (11597, 27)\n",
      "X_val shape: (2485, 10000)\n",
      "y_val shape: (2485, 27)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [00:23<00:00,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed!\n",
      "Train Accuracy: 0.5919\n",
      "Val Accuracy: 0.4978\n",
      "Train F1: 0.8476\n",
      "Val F1: 0.7936\n",
      "Train Hamming Loss: 0.0199\n",
      "Val Hamming Loss: 0.0269\n",
      "Overfitting Gap (Accuracy): 0.0941\n",
      "Overfitting Gap (F1): 0.0540\n",
      "Overfitting Gap (Hamming): 0.0070\n",
      "✅ LOGISTIC completed successfully!\n",
      "   Test Accuracy: 0.4702\n",
      "   Test F1: 0.7859\n",
      "   Test Hamming Loss: 0.0281\n",
      "   Overfitting Gap (Hamming): 0.0070\n",
      "\n",
      "============================================================\n",
      "🔧 Training RANDOMFOREST Model\n",
      "============================================================\n",
      "Training randomforest with validation control...\n",
      "X_train shape: (11597, 10000)\n",
      "y_train shape: (11597, 27)\n",
      "X_val shape: (2485, 10000)\n",
      "y_val shape: (2485, 27)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [00:15<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed!\n",
      "Train Accuracy: 0.7369\n",
      "Val Accuracy: 0.5127\n",
      "Train F1: 0.9059\n",
      "Val F1: 0.7894\n",
      "Train Hamming Loss: 0.0114\n",
      "Val Hamming Loss: 0.0245\n",
      "Overfitting Gap (Accuracy): 0.2242\n",
      "Overfitting Gap (F1): 0.1164\n",
      "Overfitting Gap (Hamming): 0.0130\n",
      "✅ RANDOMFOREST completed successfully!\n",
      "   Test Accuracy: 0.5270\n",
      "   Test F1: 0.7901\n",
      "   Test Hamming Loss: 0.0242\n",
      "   Overfitting Gap (Hamming): 0.0130\n",
      "\n",
      "============================================================\n",
      "🔧 Training LIGHTGBM Model\n",
      "============================================================\n",
      "Training lightgbm with validation control...\n",
      "X_train shape: (11597, 10000)\n",
      "y_train shape: (11597, 27)\n",
      "X_val shape: (2485, 10000)\n",
      "y_val shape: (2485, 27)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [02:37<00:00,  5.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed!\n",
      "Train Accuracy: 0.9586\n",
      "Val Accuracy: 0.6149\n",
      "Train F1: 0.9861\n",
      "Val F1: 0.8340\n",
      "Train Hamming Loss: 0.0016\n",
      "Val Hamming Loss: 0.0181\n",
      "Overfitting Gap (Accuracy): 0.3437\n",
      "Overfitting Gap (F1): 0.1521\n",
      "Overfitting Gap (Hamming): 0.0165\n",
      "✅ LIGHTGBM completed successfully!\n",
      "   Test Accuracy: 0.6070\n",
      "   Test F1: 0.8248\n",
      "   Test Hamming Loss: 0.0190\n",
      "   Overfitting Gap (Hamming): 0.0165\n",
      "\n",
      "============================================================\n",
      "🔧 Training XGBOOST Model\n",
      "============================================================\n",
      "Training xgboost with validation control...\n",
      "X_train shape: (11597, 10000)\n",
      "y_train shape: (11597, 27)\n",
      "X_val shape: (2485, 10000)\n",
      "y_val shape: (2485, 27)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [08:03<00:00, 17.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed!\n",
      "Train Accuracy: 0.9308\n",
      "Val Accuracy: 0.6205\n",
      "Train F1: 0.9758\n",
      "Val F1: 0.8368\n",
      "Train Hamming Loss: 0.0027\n",
      "Val Hamming Loss: 0.0176\n",
      "Overfitting Gap (Accuracy): 0.3103\n",
      "Overfitting Gap (F1): 0.1390\n",
      "Overfitting Gap (Hamming): 0.0149\n",
      "✅ XGBOOST completed successfully!\n",
      "   Test Accuracy: 0.6219\n",
      "   Test F1: 0.8331\n",
      "   Test Hamming Loss: 0.0179\n",
      "   Overfitting Gap (Hamming): 0.0149\n",
      "\n",
      "====================================================================================================\n",
      "📊 COMPREHENSIVE MODEL ANALYSIS\n",
      "====================================================================================================\n",
      "\n",
      "Model           | Train Acc | Val Acc   | Test Acc  | Train Ham | Val Ham  | Test Ham | Ham Gap  | Status\n",
      "---------------------------------------------------------------------------------------------------------\n",
      "XGBOOST         | 0.9308    | 0.6205    | 0.6219    | 0.0027    | 0.0176   | 0.0179   | 0.0149   | 🟢 Good\n",
      "LIGHTGBM        | 0.9586    | 0.6149    | 0.6070    | 0.0016    | 0.0181   | 0.0190   | 0.0165   | 🟢 Good\n",
      "RANDOMFOREST    | 0.7369    | 0.5127    | 0.5270    | 0.0114    | 0.0245   | 0.0242   | 0.0130   | 🟢 Good\n",
      "LOGISTIC        | 0.5919    | 0.4978    | 0.4702    | 0.0199    | 0.0269   | 0.0281   | 0.0070   | ✅ Excellent\n",
      "\n",
      "🏆 BEST MODEL (Based on Validation Performance): XGBOOST\n",
      "   📈 Validation Accuracy: 0.6205\n",
      "   🎯 Test Accuracy: 0.6219\n",
      "   📊 Test F1 Score: 0.8331\n",
      "   🔻 Test Hamming Loss: 0.0179\n",
      "   ⚖️ Overfitting Gap (Hamming): 0.0149\n",
      "   📏 Accuracy Gap: 0.3103\n",
      "   📈 F1 Gap: 0.1390\n",
      "\n",
      "================================================================================\n",
      "🔍 MODEL-SPECIFIC INSIGHTS:\n",
      "================================================================================\n",
      "LOGISTIC:\n",
      "   Average per-label validation score: 0.9731\n",
      "   Labels with good performance (>0.8): 27/27\n",
      "RANDOMFOREST:\n",
      "   Average per-label validation score: 0.9755\n",
      "   Labels with good performance (>0.8): 27/27\n",
      "\n",
      "================================================================================\n",
      "💡 RECOMMENDATIONS:\n",
      "================================================================================\n",
      "✅ Your best model shows excellent generalization based on Hamming loss!\n",
      "\n",
      "🎯 Model Selection Priority (Updated with Hamming Loss):\n",
      "   1. Choose model with best VALIDATION performance\n",
      "   2. Prefer models with smaller Hamming loss gap (primary indicator)\n",
      "   3. Consider accuracy and F1 gaps as secondary indicators\n",
      "   4. Evaluate computational efficiency for deployment\n",
      "   5. Lower Hamming loss = better multi-label classification performance\n",
      "\n",
      "📊 Understanding Hamming Loss:\n",
      "   • Hamming Loss measures label-wise classification errors\n",
      "   • Perfect score = 0.0, higher values = more errors\n",
      "   • Particularly important for multi-label problems\n",
      "   • Gap = Val_Hamming - Train_Hamming (positive = overfitting)\n"
     ]
    }
   ],
   "source": [
    "# Comprehensive Model Comparison with Validation Control\n",
    "\n",
    "def compare_all_models(X_train, y_train, X_val, y_val, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Train and compare all models with validation control\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"🚀 COMPREHENSIVE MODEL COMPARISON WITH VALIDATION CONTROL\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    models_to_test = ['logistic', 'randomforest', 'lightgbm', 'xgboost']\n",
    "    results = {}\n",
    "    \n",
    "    for model_type in models_to_test:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"🔧 Training {model_type.upper()} Model\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        try:\n",
    "            # Train model with validation\n",
    "            model, metrics = training_function_with_validation(\n",
    "                X_train, y_train, X_val, y_val, model_type=model_type\n",
    "            )\n",
    "            \n",
    "            # Test on unseen data\n",
    "            y_pred_test = model.predict(X_test)\n",
    "            test_acc = accuracy_score(y_test, y_pred_test)\n",
    "            test_f1 = f1_score(y_test, y_pred_test, average='micro')\n",
    "            test_hamming = hamming_loss(y_test, y_pred_test)\n",
    "            \n",
    "            # Store all results\n",
    "            results[model_type] = {\n",
    "                'model': model,\n",
    "                'train_accuracy': metrics['train_accuracy'],\n",
    "                'val_accuracy': metrics['val_accuracy'],\n",
    "                'test_accuracy': test_acc,\n",
    "                'train_f1': metrics['train_f1'],\n",
    "                'val_f1': metrics['val_f1'],\n",
    "                'test_f1': test_f1,\n",
    "                'train_hamming_loss': metrics['train_hamming_loss'],\n",
    "                'val_hamming_loss': metrics['val_hamming_loss'],\n",
    "                'test_hamming_loss': test_hamming,\n",
    "                'accuracy_gap': metrics['accuracy_gap'],\n",
    "                'f1_gap': metrics['f1_gap'],\n",
    "                'hamming_gap': metrics['hamming_gap'],\n",
    "                'overfitting_gap': metrics['overfitting_gap']  # Based on hamming loss\n",
    "            }\n",
    "            \n",
    "            print(f\"✅ {model_type.upper()} completed successfully!\")\n",
    "            print(f\"   Test Accuracy: {test_acc:.4f}\")\n",
    "            print(f\"   Test F1: {test_f1:.4f}\")\n",
    "            print(f\"   Test Hamming Loss: {test_hamming:.4f}\")\n",
    "            print(f\"   Overfitting Gap (Hamming): {metrics['overfitting_gap']:.4f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error training {model_type}: {str(e)}\")\n",
    "            results[model_type] = None\n",
    "    \n",
    "    return results\n",
    "\n",
    "def analyze_model_results(results):\n",
    "    \"\"\"\n",
    "    Analyze and display comprehensive results\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*100}\")\n",
    "    print(\"📊 COMPREHENSIVE MODEL ANALYSIS\")\n",
    "    print(f\"{'='*100}\")\n",
    "    \n",
    "    # Filter successful results\n",
    "    successful_results = {k: v for k, v in results.items() if v is not None}\n",
    "    \n",
    "    if not successful_results:\n",
    "        print(\"❌ No models trained successfully!\")\n",
    "        return\n",
    "    \n",
    "    # Display detailed comparison table\n",
    "    print(f\"\\n{'Model':<15} | {'Train Acc':<9} | {'Val Acc':<9} | {'Test Acc':<9} | {'Train Ham':<9} | {'Val Ham':<8} | {'Test Ham':<8} | {'Ham Gap':<8} | {'Status'}\")\n",
    "    print(\"-\" * 105)\n",
    "    \n",
    "    # Sort by validation accuracy (best practice)\n",
    "    sorted_results = sorted(successful_results.items(), \n",
    "                          key=lambda x: x[1]['val_accuracy'], reverse=True)\n",
    "    \n",
    "    for rank, (model_name, result) in enumerate(sorted_results, 1):\n",
    "        hamming_gap = result['hamming_gap']\n",
    "        \n",
    "        # Determine overfitting status based on hamming gap\n",
    "        # For hamming loss, positive gap means validation is worse (overfitting)\n",
    "        if hamming_gap < 0.01:\n",
    "            status = \"✅ Excellent\"\n",
    "        elif hamming_gap < 0.02:\n",
    "            status = \"🟢 Good\"\n",
    "        elif hamming_gap < 0.04:\n",
    "            status = \"🟡 Moderate\"\n",
    "        else:\n",
    "            status = \"🔴 High\"\n",
    "        \n",
    "        rank_emoji = \"🥇\" if rank == 1 else \"🥈\" if rank == 2 else \"🥉\" if rank == 3 else \"4️⃣\"\n",
    "        \n",
    "        print(f\"{model_name.upper():<15} | {result['train_accuracy']:<9.4f} | {result['val_accuracy']:<9.4f} | \"\n",
    "              f\"{result['test_accuracy']:<9.4f} | {result['train_hamming_loss']:<9.4f} | {result['val_hamming_loss']:<8.4f} | \"\n",
    "              f\"{result['test_hamming_loss']:<8.4f} | {hamming_gap:<8.4f} | {status}\")\n",
    "    \n",
    "    # Identify best models\n",
    "    best_model = sorted_results[0]\n",
    "    print(f\"\\n🏆 BEST MODEL (Based on Validation Performance): {best_model[0].upper()}\")\n",
    "    print(f\"   📈 Validation Accuracy: {best_model[1]['val_accuracy']:.4f}\")\n",
    "    print(f\"   🎯 Test Accuracy: {best_model[1]['test_accuracy']:.4f}\")\n",
    "    print(f\"   📊 Test F1 Score: {best_model[1]['test_f1']:.4f}\")\n",
    "    print(f\"   🔻 Test Hamming Loss: {best_model[1]['test_hamming_loss']:.4f}\")\n",
    "    print(f\"   ⚖️ Overfitting Gap (Hamming): {best_model[1]['overfitting_gap']:.4f}\")\n",
    "    print(f\"   📏 Accuracy Gap: {best_model[1]['accuracy_gap']:.4f}\")\n",
    "    print(f\"   📈 F1 Gap: {best_model[1]['f1_gap']:.4f}\")\n",
    "    \n",
    "    # Best test performance (might be different from best validation)\n",
    "    best_test = max(successful_results.items(), key=lambda x: x[1]['test_accuracy'])\n",
    "    if best_test[0] != best_model[0]:\n",
    "        print(f\"\\n🎯 BEST TEST PERFORMANCE: {best_test[0].upper()}\")\n",
    "        print(f\"   Test Accuracy: {best_test[1]['test_accuracy']:.4f}\")\n",
    "        print(f\"   (Note: Choose model based on validation, not test performance)\")\n",
    "    \n",
    "    # Best hamming loss performance\n",
    "    best_hamming = min(successful_results.items(), key=lambda x: x[1]['test_hamming_loss'])\n",
    "    if best_hamming[0] != best_model[0]:\n",
    "        print(f\"\\n🔻 BEST HAMMING LOSS PERFORMANCE: {best_hamming[0].upper()}\")\n",
    "        print(f\"   Test Hamming Loss: {best_hamming[1]['test_hamming_loss']:.4f}\")\n",
    "        print(f\"   (Lower hamming loss = better multi-label performance)\")\n",
    "    \n",
    "    # Model-specific insights\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"🔍 MODEL-SPECIFIC INSIGHTS:\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    for model_name, result in successful_results.items():\n",
    "        if hasattr(result['model'], 'get_validation_scores'):\n",
    "            val_scores = result['model'].get_validation_scores()\n",
    "            if val_scores and any(score for score in val_scores if score is not None):\n",
    "                valid_scores = [s for s in val_scores if s is not None and s > 0]\n",
    "                if valid_scores:\n",
    "                    avg_label_score = np.mean(valid_scores)\n",
    "                    print(f\"{model_name.upper()}:\")\n",
    "                    print(f\"   Average per-label validation score: {avg_label_score:.4f}\")\n",
    "                    print(f\"   Labels with good performance (>0.8): {sum(1 for s in valid_scores if s > 0.8)}/{len(valid_scores)}\")\n",
    "    \n",
    "    # Recommendations\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"💡 RECOMMENDATIONS:\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    if best_model[1]['overfitting_gap'] < 0.02:\n",
    "        print(\"✅ Your best model shows excellent generalization based on Hamming loss!\")\n",
    "    elif best_model[1]['overfitting_gap'] < 0.04:\n",
    "        print(\"🟢 Your best model shows good generalization based on Hamming loss!\")\n",
    "    else:\n",
    "        print(\"⚠️ Consider additional regularization for your best model:\")\n",
    "        print(\"   - Increase regularization parameters\")\n",
    "        print(\"   - Use more training data\")\n",
    "        print(\"   - Apply feature selection\")\n",
    "        print(\"   - Consider ensemble methods\")\n",
    "    \n",
    "    hamming_gap_threshold = 0.02\n",
    "    models_with_overfitting = [name for name, result in successful_results.items() \n",
    "                              if result['hamming_gap'] > hamming_gap_threshold]\n",
    "    \n",
    "    if models_with_overfitting:\n",
    "        print(f\"\\n⚠️ Models showing overfitting based on Hamming loss (gap > {hamming_gap_threshold}):\")\n",
    "        for model in models_with_overfitting:\n",
    "            result = successful_results[model]\n",
    "            print(f\"   - {model.upper()}:\")\n",
    "            print(f\"     • Hamming Gap: {result['hamming_gap']:.4f}\")\n",
    "            print(f\"     • Accuracy Gap: {result['accuracy_gap']:.4f}\")\n",
    "            print(f\"     • F1 Gap: {result['f1_gap']:.4f}\")\n",
    "    \n",
    "    print(f\"\\n🎯 Model Selection Priority (Updated with Hamming Loss):\")\n",
    "    print(\"   1. Choose model with best VALIDATION performance\")\n",
    "    print(\"   2. Prefer models with smaller Hamming loss gap (primary indicator)\")\n",
    "    print(\"   3. Consider accuracy and F1 gaps as secondary indicators\")\n",
    "    print(\"   4. Evaluate computational efficiency for deployment\")\n",
    "    print(\"   5. Lower Hamming loss = better multi-label classification performance\")\n",
    "    \n",
    "    print(f\"\\n📊 Understanding Hamming Loss:\")\n",
    "    print(\"   • Hamming Loss measures label-wise classification errors\")\n",
    "    print(\"   • Perfect score = 0.0, higher values = more errors\")\n",
    "    print(\"   • Particularly important for multi-label problems\")\n",
    "    print(\"   • Gap = Val_Hamming - Train_Hamming (positive = overfitting)\")\n",
    "    \n",
    "    return successful_results\n",
    "\n",
    "# Example usage\n",
    "print(\"Starting comprehensive model comparison...\")\n",
    "all_results = compare_all_models(X_train_tfidf, y_train, X_val_tfidf, y_val, X_test_tfidf, y_test)\n",
    "final_analysis = analyze_model_results(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c057bf7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing enhanced validation-controlled training...\n",
      "============================================================\n",
      "Training lightgbm with validation control...\n",
      "X_train shape: (11597, 10000)\n",
      "y_train shape: (11597, 27)\n",
      "X_val shape: (2485, 10000)\n",
      "y_val shape: (2485, 27)\n",
      "  Training LGBM classifier 1/27\n",
      "  Training LGBM classifier 2/27\n",
      "  Training LGBM classifier 3/27\n",
      "  Training LGBM classifier 4/27\n",
      "  Training LGBM classifier 5/27\n",
      "  Training LGBM classifier 6/27\n",
      "  Training LGBM classifier 7/27\n",
      "  Training LGBM classifier 8/27\n",
      "  Training LGBM classifier 9/27\n",
      "  Training LGBM classifier 10/27\n",
      "  Training LGBM classifier 11/27\n",
      "  Training LGBM classifier 12/27\n",
      "  Training LGBM classifier 13/27\n",
      "  Training LGBM classifier 14/27\n",
      "  Training LGBM classifier 15/27\n",
      "  Training LGBM classifier 16/27\n",
      "  Training LGBM classifier 17/27\n",
      "  Training LGBM classifier 18/27\n",
      "  Training LGBM classifier 19/27\n",
      "  Training LGBM classifier 20/27\n",
      "  Training LGBM classifier 21/27\n",
      "  Training LGBM classifier 22/27\n",
      "  Training LGBM classifier 23/27\n",
      "  Training LGBM classifier 24/27\n",
      "  Training LGBM classifier 25/27\n",
      "  Training LGBM classifier 26/27\n",
      "  Training LGBM classifier 27/27\n",
      "Training completed!\n",
      "Train Accuracy: 0.9586\n",
      "Val Accuracy: 0.6149\n",
      "Train F1: 0.9861\n",
      "Val F1: 0.8340\n",
      "Overfitting Gap (Acc): 0.3437\n",
      "\n",
      "LightGBM Results:\n",
      "  Validation Accuracy: 0.6149\n",
      "  Overfitting Gap: 0.3437\n"
     ]
    }
   ],
   "source": [
    "# # Example usage of the enhanced training function\n",
    "# print(\"Testing enhanced validation-controlled training...\")\n",
    "# print(\"=\"*60)\n",
    "\n",
    "# # Test with LightGBM\n",
    "# lgbm_model, lgbm_metrics = training_function_with_validation(\n",
    "#     X_train_tfidf, y_train, X_val_tfidf, y_val, model_type='lightgbm'\n",
    "# )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea5a3b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------\n",
      "Training xgboost with validation control...\n",
      "X_train shape: (11597, 10000)\n",
      "y_train shape: (11597, 27)\n",
      "X_val shape: (2485, 10000)\n",
      "y_val shape: (2485, 27)\n",
      "  Training XGB classifier 1/27\n",
      "  Training XGB classifier 2/27\n",
      "  Training XGB classifier 3/27\n",
      "  Training XGB classifier 4/27\n",
      "  Training XGB classifier 5/27\n",
      "  Training XGB classifier 6/27\n",
      "  Training XGB classifier 7/27\n",
      "  Training XGB classifier 8/27\n",
      "  Training XGB classifier 9/27\n",
      "  Training XGB classifier 10/27\n",
      "  Training XGB classifier 11/27\n",
      "  Training XGB classifier 12/27\n",
      "  Training XGB classifier 13/27\n",
      "  Training XGB classifier 14/27\n",
      "  Training XGB classifier 15/27\n",
      "  Training XGB classifier 16/27\n",
      "  Training XGB classifier 17/27\n",
      "  Training XGB classifier 18/27\n",
      "  Training XGB classifier 19/27\n",
      "  Training XGB classifier 20/27\n",
      "  Training XGB classifier 21/27\n",
      "  Training XGB classifier 22/27\n",
      "  Training XGB classifier 23/27\n",
      "  Training XGB classifier 24/27\n",
      "  Training XGB classifier 25/27\n",
      "  Training XGB classifier 26/27\n",
      "  Training XGB classifier 27/27\n",
      "Training completed!\n",
      "Train Accuracy: 0.9308\n",
      "Val Accuracy: 0.6205\n",
      "Train F1: 0.9758\n",
      "Val F1: 0.8368\n",
      "Overfitting Gap (Acc): 0.3103\n",
      "\n",
      "XGBoost Results:\n",
      "  Validation Accuracy: 0.6205\n",
      "  Overfitting Gap: 0.3103\n",
      "\n",
      "Final Test Results:\n",
      "  LightGBM Test Accuracy: 0.6070\n",
      "  XGBoost Test Accuracy: 0.6219\n",
      "\n",
      "🏆 Best validation-controlled model: XGBoost\n",
      "   Test Accuracy: 0.6219\n"
     ]
    }
   ],
   "source": [
    "# # Test with XGBoost\n",
    "# print(f\"\\n{'-'*40}\")\n",
    "# xgb_model, xgb_metrics = training_function_with_validation(\n",
    "#     X_train_tfidf, y_train, X_val_tfidf, y_val, model_type='xgboost'\n",
    "# )\n",
    "\n",
    "# print(f\"\\nXGBoost Results:\")\n",
    "# print(f\"  Validation Accuracy: {xgb_metrics['val_accuracy']:.4f}\")\n",
    "# print(f\"  Overfitting Gap: {xgb_metrics['overfitting_gap']:.4f}\")\n",
    "\n",
    "# # Final test predictions\n",
    "# lgbm_test_pred = lgbm_model.predict(X_test_tfidf)\n",
    "# xgb_test_pred = xgb_model.predict(X_test_tfidf)\n",
    "\n",
    "# lgbm_test_acc = accuracy_score(y_test, lgbm_test_pred)\n",
    "# xgb_test_acc = accuracy_score(y_test, xgb_test_pred)\n",
    "\n",
    "# print(f\"\\nFinal Test Results:\")\n",
    "# print(f\"  LightGBM Test Accuracy: {lgbm_test_acc:.4f}\")\n",
    "# print(f\"  XGBoost Test Accuracy: {xgb_test_acc:.4f}\")\n",
    "\n",
    "# # Determine best model\n",
    "# if lgbm_metrics['val_accuracy'] > xgb_metrics['val_accuracy']:\n",
    "#     best_val_model = 'LightGBM'\n",
    "#     best_model = lgbm_model\n",
    "#     best_test_acc = lgbm_test_acc\n",
    "# else:\n",
    "#     best_val_model = 'XGBoost'\n",
    "#     best_model = xgb_model\n",
    "#     best_test_acc = xgb_test_acc\n",
    "\n",
    "# print(f\"\\n🏆 Best validation-controlled model: {best_val_model}\")\n",
    "# print(f\"   Test Accuracy: {best_test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217b6c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Additional Validation Techniques for Overfitting Control\n",
    "\n",
    "# from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "# from sklearn.model_selection import validation_curve, learning_curve\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def plot_learning_curve(estimator, X, y, title, cv=5, n_jobs=-1, \n",
    "#                        train_sizes=np.linspace(0.1, 1.0, 10)):\n",
    "#     \"\"\"\n",
    "#     Generate a plot showing the learning curve for a model\n",
    "#     \"\"\"\n",
    "#     train_sizes, train_scores, val_scores = learning_curve(\n",
    "#         estimator, X, y, cv=cv, n_jobs=n_jobs, \n",
    "#         train_sizes=train_sizes, scoring='accuracy'\n",
    "#     )\n",
    "    \n",
    "#     train_scores_mean = np.mean(train_scores, axis=1)\n",
    "#     train_scores_std = np.std(train_scores, axis=1)\n",
    "#     val_scores_mean = np.mean(val_scores, axis=1)\n",
    "#     val_scores_std = np.std(val_scores, axis=1)\n",
    "    \n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     plt.plot(train_sizes, train_scores_mean, 'o-', color='blue', label='Training score')\n",
    "#     plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "#                      train_scores_mean + train_scores_std, alpha=0.1, color='blue')\n",
    "    \n",
    "#     plt.plot(train_sizes, val_scores_mean, 'o-', color='red', label='Cross-validation score')\n",
    "#     plt.fill_between(train_sizes, val_scores_mean - val_scores_std,\n",
    "#                      val_scores_mean + val_scores_std, alpha=0.1, color='red')\n",
    "    \n",
    "#     plt.xlabel('Training Set Size')\n",
    "#     plt.ylabel('Accuracy Score')\n",
    "#     plt.title(f'Learning Curve - {title}')\n",
    "#     plt.legend(loc='best')\n",
    "#     plt.grid(True, alpha=0.3)\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "    \n",
    "#     # Detect overfitting\n",
    "#     final_gap = train_scores_mean[-1] - val_scores_mean[-1]\n",
    "#     if final_gap > 0.1:\n",
    "#         print(f\"⚠️ WARNING: {title} shows signs of overfitting (gap: {final_gap:.4f})\")\n",
    "#     elif final_gap > 0.05:\n",
    "#         print(f\"🔶 MODERATE: {title} shows moderate overfitting (gap: {final_gap:.4f})\")\n",
    "#     else:\n",
    "#         print(f\"✅ GOOD: {title} shows good generalization (gap: {final_gap:.4f})\")\n",
    "\n",
    "# def cross_validate_with_overfitting_check(model, X, y, cv=5, model_name=\"Model\"):\n",
    "#     \"\"\"\n",
    "#     Perform cross-validation and check for overfitting signs\n",
    "#     \"\"\"\n",
    "#     print(f\"\\nCross-validating {model_name}...\")\n",
    "    \n",
    "#     # Perform cross-validation\n",
    "#     cv_scores = cross_val_score(model, X, y, cv=cv, scoring='accuracy', n_jobs=-1)\n",
    "    \n",
    "#     # Train on full dataset to check training score\n",
    "#     model.fit(X, y)\n",
    "#     train_score = model.score(X, y)\n",
    "    \n",
    "#     cv_mean = cv_scores.mean()\n",
    "#     cv_std = cv_scores.std()\n",
    "    \n",
    "#     print(f\"  Cross-validation scores: {cv_scores}\")\n",
    "#     print(f\"  CV Mean ± Std: {cv_mean:.4f} ± {cv_std:.4f}\")\n",
    "#     print(f\"  Training score: {train_score:.4f}\")\n",
    "    \n",
    "#     # Check for overfitting\n",
    "#     overfitting_gap = train_score - cv_mean\n",
    "#     print(f\"  Overfitting gap: {overfitting_gap:.4f}\")\n",
    "    \n",
    "#     if overfitting_gap > 0.1:\n",
    "#         status = \"🚨 HIGH OVERFITTING\"\n",
    "#     elif overfitting_gap > 0.05:\n",
    "#         status = \"⚠️ MODERATE OVERFITTING\"\n",
    "#     else:\n",
    "#         status = \"✅ GOOD GENERALIZATION\"\n",
    "    \n",
    "#     print(f\"  Status: {status}\")\n",
    "    \n",
    "#     return {\n",
    "#         'cv_scores': cv_scores,\n",
    "#         'cv_mean': cv_mean,\n",
    "#         'cv_std': cv_std,\n",
    "#         'train_score': train_score,\n",
    "#         'overfitting_gap': overfitting_gap,\n",
    "#         'status': status\n",
    "#     }\n",
    "\n",
    "# def plot_validation_curve_param(estimator, X, y, param_name, param_range, title):\n",
    "#     \"\"\"\n",
    "#     Plot validation curve for a specific parameter to find optimal value\n",
    "#     \"\"\"\n",
    "#     train_scores, val_scores = validation_curve(\n",
    "#         estimator, X, y, param_name=param_name, param_range=param_range,\n",
    "#         cv=5, scoring='accuracy', n_jobs=-1\n",
    "#     )\n",
    "    \n",
    "#     train_scores_mean = np.mean(train_scores, axis=1)\n",
    "#     train_scores_std = np.std(train_scores, axis=1)\n",
    "#     val_scores_mean = np.mean(val_scores, axis=1)\n",
    "#     val_scores_std = np.std(val_scores, axis=1)\n",
    "    \n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     plt.semilogx(param_range, train_scores_mean, 'o-', color='blue', label='Training score')\n",
    "#     plt.fill_between(param_range, train_scores_mean - train_scores_std,\n",
    "#                      train_scores_mean + train_scores_std, alpha=0.1, color='blue')\n",
    "    \n",
    "#     plt.semilogx(param_range, val_scores_mean, 'o-', color='red', label='Cross-validation score')\n",
    "#     plt.fill_between(param_range, val_scores_mean - val_scores_std,\n",
    "#                      val_scores_mean + val_scores_std, alpha=0.1, color='red')\n",
    "    \n",
    "#     plt.xlabel(param_name)\n",
    "#     plt.ylabel('Accuracy Score')\n",
    "#     plt.title(f'Validation Curve - {title}')\n",
    "#     plt.legend(loc='best')\n",
    "#     plt.grid(True, alpha=0.3)\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "    \n",
    "#     # Find optimal parameter\n",
    "#     optimal_idx = np.argmax(val_scores_mean)\n",
    "#     optimal_param = param_range[optimal_idx]\n",
    "#     optimal_score = val_scores_mean[optimal_idx]\n",
    "    \n",
    "#     print(f\"Optimal {param_name}: {optimal_param}\")\n",
    "#     print(f\"Optimal CV score: {optimal_score:.4f}\")\n",
    "    \n",
    "#     return optimal_param, optimal_score\n",
    "\n",
    "# # Example: Cross-validation analysis for overfitting detection\n",
    "# print(\"COMPREHENSIVE VALIDATION ANALYSIS\")\n",
    "# print(\"=\"*60)\n",
    "\n",
    "# # Sample a subset for faster computation in demo\n",
    "# sample_size = min(1000, len(X_train_tfidf))\n",
    "# X_sample = X_train_tfidf[:sample_size]\n",
    "# y_sample = y_train[:sample_size]\n",
    "\n",
    "# print(f\"Using sample of {sample_size} examples for validation analysis...\")\n",
    "\n",
    "# # 1. Cross-validation for different models\n",
    "# models_for_cv = {\n",
    "#     'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "#     'Random Forest': RandomForestClassifier(n_estimators=50, random_state=42, max_depth=10),\n",
    "# }\n",
    "\n",
    "# cv_results = {}\n",
    "# for name, model in models_for_cv.items():\n",
    "#     # Use OneVsRestClassifier for multi-label\n",
    "#     multi_label_model = OneVsRestClassifier(model)\n",
    "#     cv_results[name] = cross_validate_with_overfitting_check(\n",
    "#         multi_label_model, X_sample, y_sample, cv=3, model_name=name\n",
    "#     )\n",
    "\n",
    "# # 2. Find models with best generalization\n",
    "# print(f\"\\n{'='*60}\")\n",
    "# print(\"OVERFITTING SUMMARY:\")\n",
    "# print(f\"{'Model':<20} | {'CV Score':<10} | {'Gap':<8} | {'Status'}\")\n",
    "# print(f\"{'-'*65}\")\n",
    "\n",
    "# for name, results in cv_results.items():\n",
    "#     print(f\"{name:<20} | {results['cv_mean']:<10.4f} | {results['overfitting_gap']:<8.4f} | {results['status']}\")\n",
    "\n",
    "# # 3. Recommendations for overfitting control\n",
    "# print(f\"\\n{'='*60}\")\n",
    "# print(\"RECOMMENDATIONS FOR OVERFITTING CONTROL:\")\n",
    "# print()\n",
    "# print(\"1. 📊 VALIDATION MONITORING:\")\n",
    "# print(\"   - Always split data into train/validation/test\")\n",
    "# print(\"   - Monitor validation metrics during training\")\n",
    "# print(\"   - Use early stopping when validation stops improving\")\n",
    "# print()\n",
    "# print(\"2. 🔧 MODEL REGULARIZATION:\")\n",
    "# print(\"   - Logistic Regression: Adjust C parameter (lower = more regularization)\")\n",
    "# print(\"   - Random Forest: Limit max_depth, increase min_samples_split\")\n",
    "# print(\"   - XGBoost/LightGBM: Use early_stopping_rounds, adjust learning_rate\")\n",
    "# print()\n",
    "# print(\"3. 📈 TECHNIQUES IMPLEMENTED:\")\n",
    "# print(\"   - Train/Validation/Test split (70/15/15)\")\n",
    "# print(\"   - Cross-validation for robust evaluation\")\n",
    "# print(\"   - Early stopping for tree-based models\")\n",
    "# print(\"   - Validation gap monitoring\")\n",
    "# print(\"   - Learning curve analysis\")\n",
    "# print()\n",
    "# print(\"4. 🎯 SELECTION CRITERIA:\")\n",
    "# print(\"   - Choose model with best VALIDATION performance\")\n",
    "# print(\"   - Prefer models with smaller train-validation gap\")\n",
    "# print(\"   - Consider cross-validation consistency\")\n",
    "\n",
    "# # Example of how to use validation curve for parameter tuning\n",
    "# print(f\"\\n{'='*60}\")\n",
    "# print(\"PARAMETER TUNING WITH VALIDATION CURVES:\")\n",
    "# print(\"(Use this approach to find optimal hyperparameters)\")\n",
    "# print()\n",
    "# print(\"Example code for Random Forest max_depth tuning:\")\n",
    "# print(\"\"\"\n",
    "# # Find optimal max_depth for Random Forest\n",
    "# param_range = [3, 5, 7, 10, 15, 20]\n",
    "# optimal_depth, optimal_score = plot_validation_curve_param(\n",
    "#     OneVsRestClassifier(RandomForestClassifier(random_state=42)),\n",
    "#     X_train_tfidf, y_train,\n",
    "#     param_name='estimator__max_depth',\n",
    "#     param_range=param_range,\n",
    "#     title='Random Forest max_depth'\n",
    "# )\n",
    "# \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a7c4d3",
   "metadata": {},
   "source": [
    "## Transformers Encoder Model(MordenBERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48e3714a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/d/TR-Project/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import DataCollatorWithPadding\n",
    "import evaluate\n",
    "import warnings\n",
    "# Suppress the tqdm warning temporarily\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='tqdm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d27045a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets_from_arrays(X_train, y_train, X_val=None, y_val=None, X_test=None, y_test=None):\n",
    "    \"\"\"\n",
    "    Convert arrays into HuggingFace datasets format with specified structure\n",
    "    \n",
    "    Returns:\n",
    "        DatasetDict with features:\n",
    "        - dataset[\"train\"][\"text\"]: text data\n",
    "        - dataset[\"train\"][\"labels\"]: multi-label arrays\n",
    "        - dataset[\"val\"][\"text\"]: validation text data (if provided)\n",
    "        - dataset[\"val\"][\"labels\"]: validation labels (if provided)\n",
    "        - dataset[\"test\"][\"text\"]: test text data (if provided)\n",
    "        - dataset[\"test\"][\"labels\"]: test labels (if provided)\n",
    "    \"\"\"\n",
    "    # Create training dataset\n",
    "    train_dict = {\n",
    "        \"text\": X_train.tolist() if hasattr(X_train, 'tolist') else list(X_train),\n",
    "        \"labels\": y_train.tolist() if hasattr(y_train, 'tolist') else list(y_train)\n",
    "    }\n",
    "    \n",
    "    datasets_dict = {\n",
    "        \"train\": Dataset.from_dict(train_dict)\n",
    "    }\n",
    "    \n",
    "    # Add validation dataset if provided\n",
    "    if X_val is not None and y_val is not None:\n",
    "        val_dict = {\n",
    "            \"text\": X_val.tolist() if hasattr(X_val, 'tolist') else list(X_val),\n",
    "            \"labels\": y_val.tolist() if hasattr(y_val, 'tolist') else list(y_val)\n",
    "        }\n",
    "        datasets_dict[\"val\"] = Dataset.from_dict(val_dict)\n",
    "    \n",
    "    # Add test dataset if provided\n",
    "    if X_test is not None and y_test is not None:\n",
    "        test_dict = {\n",
    "            \"text\": X_test.tolist() if hasattr(X_test, 'tolist') else list(X_test),\n",
    "            \"labels\": y_test.tolist() if hasattr(y_test, 'tolist') else list(y_test)\n",
    "        }\n",
    "        datasets_dict[\"test\"] = Dataset.from_dict(test_dict)\n",
    "\n",
    "    # Create DatasetDict\n",
    "    dataset = DatasetDict(datasets_dict)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "669a76ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_data=os.path.join(os.getcwd(), 'processed_data')\n",
    "with open(os.path.join(saved_data,'train_arrays.pkl'), 'rb') as f:\n",
    "    train_data = pickle.load(f)\n",
    "    X_train = train_data['X_train']\n",
    "    y_train = train_data['y_train']\n",
    "\n",
    "with open(os.path.join(saved_data,'val_arrays.pkl'), 'rb') as f:\n",
    "    val_data = pickle.load(f)\n",
    "    X_val = val_data['X_val']\n",
    "    y_val = val_data['y_val']\n",
    "\n",
    "with open(os.path.join(saved_data,'test_arrays.pkl'), 'rb') as f:\n",
    "    test_data = pickle.load(f)\n",
    "    X_test = test_data['X_test']\n",
    "    y_test = test_data['y_test']\n",
    "\n",
    "with open(os.path.join(saved_data,'class_name.pkl'), 'rb') as f:\n",
    "    class_name_data = pickle.load(f)\n",
    "    class_name = class_name_data['class_name']\n",
    "\n",
    "# Create the datasets\n",
    "dataset = create_datasets_from_arrays(X_train, y_train, X_val, y_val, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d9195ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'labels'],\n",
       "        num_rows: 11597\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['text', 'labels'],\n",
       "        num_rows: 2485\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'labels'],\n",
       "        num_rows: 2486\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef977ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 11597/11597 [01:22<00:00, 140.80 examples/s]\n",
      "Map: 100%|██████████| 11597/11597 [01:22<00:00, 140.80 examples/s]\n",
      "Map: 100%|██████████| 2485/2485 [00:18<00:00, 136.80 examples/s]\n",
      "Map:   0%|          | 0/2486 [00:00<?, ? examples/s]\n",
      "Map: 100%|██████████| 2486/2486 [00:18<00:00, 137.40 examples/s]\n",
      "Map: 100%|██████████| 2486/2486 [00:18<00:00, 137.40 examples/s]\n"
     ]
    }
   ],
   "source": [
    "model_path = \"answerdotai/ModernBERT-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "def preprocess_function(example):\n",
    "   text = example['text']\n",
    "   example = tokenizer(text, truncation=True)\n",
    "   return example\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9f7ca12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'labels', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 11597\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['text', 'labels', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 2485\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'labels', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 2486\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99a6c5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required metrics libraries\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, f1_score, \n",
    "    roc_auc_score, average_precision_score,\n",
    "    hamming_loss, jaccard_score, accuracy_score\n",
    ")\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "def comprehensive_evaluation(y_true, y_pred_proba, y_pred_binary=None, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation for multi-label classification with all averaging methods\n",
    "    \"\"\"\n",
    "    if y_pred_binary is None:\n",
    "        y_pred_binary = (y_pred_proba >= threshold).astype(int)\n",
    "    \n",
    "    metrics = {}\n",
    "    \n",
    "    # SAMPLES AVERAGE (per-sample then average across samples)\n",
    "    metrics['precision_samples'] = precision_score(y_true, y_pred_binary, average='samples', zero_division=0)\n",
    "    metrics['recall_samples'] = recall_score(y_true, y_pred_binary, average='samples', zero_division=0)\n",
    "    metrics['f1_samples'] = f1_score(y_true, y_pred_binary, average='samples', zero_division=0)\n",
    "    \n",
    "    # MICRO AVERAGE (global aggregation)\n",
    "    metrics['precision_micro'] = precision_score(y_true, y_pred_binary, average='micro', zero_division=0)\n",
    "    metrics['recall_micro'] = recall_score(y_true, y_pred_binary, average='micro', zero_division=0)\n",
    "    metrics['f1_micro'] = f1_score(y_true, y_pred_binary, average='micro', zero_division=0)\n",
    "    \n",
    "    # MACRO AVERAGE (unweighted average across labels)\n",
    "    metrics['precision_macro'] = precision_score(y_true, y_pred_binary, average='macro', zero_division=0)\n",
    "    metrics['recall_macro'] = recall_score(y_true, y_pred_binary, average='macro', zero_division=0)\n",
    "    metrics['f1_macro'] = f1_score(y_true, y_pred_binary, average='macro', zero_division=0)\n",
    "    \n",
    "    # WEIGHTED AVERAGE (weighted by support/frequency)\n",
    "    metrics['precision_weighted'] = precision_score(y_true, y_pred_binary, average='weighted', zero_division=0)\n",
    "    metrics['recall_weighted'] = recall_score(y_true, y_pred_binary, average='weighted', zero_division=0)\n",
    "    metrics['f1_weighted'] = f1_score(y_true, y_pred_binary, average='weighted', zero_division=0)\n",
    "    \n",
    "    # ROC-AUC (multiple averaging methods)\n",
    "    try:\n",
    "        metrics['roc_auc_macro'] = roc_auc_score(y_true, y_pred_proba, average='macro')\n",
    "        metrics['roc_auc_weighted'] = roc_auc_score(y_true, y_pred_proba, average='weighted')\n",
    "        metrics['roc_auc_samples'] = roc_auc_score(y_true, y_pred_proba, average='samples')\n",
    "    except ValueError as e:\n",
    "        print(f\"ROC-AUC calculation failed: {e}\")\n",
    "        metrics['roc_auc_macro'] = 0.0\n",
    "        metrics['roc_auc_weighted'] = 0.0\n",
    "        metrics['roc_auc_samples'] = 0.0\n",
    "    \n",
    "    # Precision-Recall AUC (multiple averaging methods)\n",
    "    try:\n",
    "        metrics['pr_auc_macro'] = average_precision_score(y_true, y_pred_proba, average='macro')\n",
    "        metrics['pr_auc_weighted'] = average_precision_score(y_true, y_pred_proba, average='weighted')\n",
    "        metrics['pr_auc_samples'] = average_precision_score(y_true, y_pred_proba, average='samples')\n",
    "    except ValueError as e:\n",
    "        print(f\"PR-AUC calculation failed: {e}\")\n",
    "        metrics['pr_auc_macro'] = 0.0\n",
    "        metrics['pr_auc_weighted'] = 0.0\n",
    "        metrics['pr_auc_samples'] = 0.0\n",
    "    \n",
    "    # Hamming Loss (inherently micro-averaged)\n",
    "    metrics['hamming_loss'] = hamming_loss(y_true, y_pred_binary)\n",
    "    \n",
    "    # Jaccard Score (multiple averaging methods)\n",
    "    metrics['jaccard_samples'] = jaccard_score(y_true, y_pred_binary, average='samples', zero_division=0)\n",
    "    metrics['jaccard_macro'] = jaccard_score(y_true, y_pred_binary, average='macro', zero_division=0)\n",
    "    metrics['jaccard_weighted'] = jaccard_score(y_true, y_pred_binary, average='weighted', zero_division=0)\n",
    "    \n",
    "    # Overall accuracy (subset accuracy for multi-label)\n",
    "    metrics['accuracy'] = accuracy_score(y_true, y_pred_binary)\n",
    "    \n",
    "    # Note: micro average for Jaccard in multi-label is not directly supported in sklearn\n",
    "    # but can be calculated manually if needed\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Enhanced compute_metrics function for transformers Trainer using comprehensive evaluation\n",
    "    \"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    # Apply sigmoid to get probabilities\n",
    "    predictions_proba = sigmoid(predictions)\n",
    "    \n",
    "    # Convert to binary predictions using threshold 0.5\n",
    "    predictions_binary = (predictions_proba > 0.5).astype(int)\n",
    "    \n",
    "    # Ensure labels are integers\n",
    "    labels = labels.astype(int)\n",
    "    \n",
    "    # Use comprehensive evaluation\n",
    "    metrics = comprehensive_evaluation(\n",
    "        y_true=labels,\n",
    "        y_pred_proba=predictions_proba,\n",
    "        y_pred_binary=predictions_binary,\n",
    "        threshold=0.5\n",
    "    )\n",
    "    \n",
    "    # Return metrics with prefixes for clarity during training\n",
    "    return {\n",
    "        # Primary metrics for monitoring\n",
    "        'eval_f1_micro': metrics['f1_micro'],\n",
    "        'eval_f1_macro': metrics['f1_macro'],\n",
    "        'eval_accuracy': metrics['accuracy'],\n",
    "        'eval_hamming_loss': metrics['hamming_loss'],\n",
    "        \n",
    "        # Precision metrics\n",
    "        'eval_precision_micro': metrics['precision_micro'],\n",
    "        'eval_precision_macro': metrics['precision_macro'],\n",
    "        'eval_precision_samples': metrics['precision_samples'],\n",
    "        'eval_precision_weighted': metrics['precision_weighted'],\n",
    "        \n",
    "        # Recall metrics\n",
    "        'eval_recall_micro': metrics['recall_micro'],\n",
    "        'eval_recall_macro': metrics['recall_macro'],\n",
    "        'eval_recall_samples': metrics['recall_samples'],\n",
    "        'eval_recall_weighted': metrics['recall_weighted'],\n",
    "        \n",
    "        # F1 metrics\n",
    "        'eval_f1_samples': metrics['f1_samples'],\n",
    "        'eval_f1_weighted': metrics['f1_weighted'],\n",
    "        \n",
    "        # ROC-AUC metrics\n",
    "        'eval_roc_auc_macro': metrics['roc_auc_macro'],\n",
    "        'eval_roc_auc_weighted': metrics['roc_auc_weighted'],\n",
    "        'eval_roc_auc_samples': metrics['roc_auc_samples'],\n",
    "        \n",
    "        # PR-AUC metrics\n",
    "        'eval_pr_auc_macro': metrics['pr_auc_macro'],\n",
    "        'eval_pr_auc_weighted': metrics['pr_auc_weighted'],\n",
    "        'eval_pr_auc_samples': metrics['pr_auc_samples'],\n",
    "        \n",
    "        # Jaccard metrics\n",
    "        'eval_jaccard_samples': metrics['jaccard_samples'],\n",
    "        'eval_jaccard_macro': metrics['jaccard_macro'],\n",
    "        'eval_jaccard_weighted': metrics['jaccard_weighted'],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b25da0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ModernBertForSequenceClassification were not initialized from the model checkpoint at answerdotai/ModernBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "class2id = {class_:id for id, class_ in enumerate(class_name)}\n",
    "id2class = {id:class_ for class_, id in class2id.items()}\n",
    "\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path, \n",
    "                                                           num_labels=len(class_name),\n",
    "                                                           id2label=id2class, \n",
    "                                                           label2id=class2id,\n",
    "                                                           problem_type = \"multi_label_classification\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a298aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting training with enhanced configuration...\n",
      "📊 Training samples: 11597\n",
      "📊 Validation samples: 2485\n",
      "🎯 Target metric: eval_f1_micro\n",
      "⏱️ Total epochs: 3\n",
      "🔄 Evaluation every: 100 steps\n",
      "💾 Saving every: 100 steps\n",
      "⏹️ Early stopping patience: 3\n",
      "\n",
      "🎯 Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0621 11:26:34.243000 1031232 torch/_inductor/utils.py:1250] [1/0] Not enough SMs to use max_autotune_gemm mode\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1700' max='2901' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1700/2901 20:23 < 14:25, 1.39 it/s, Epoch 1/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Hamming Loss</th>\n",
       "      <th>Precision Micro</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Precision Samples</th>\n",
       "      <th>Precision Weighted</th>\n",
       "      <th>Recall Micro</th>\n",
       "      <th>Recall Macro</th>\n",
       "      <th>Recall Samples</th>\n",
       "      <th>Recall Weighted</th>\n",
       "      <th>F1 Samples</th>\n",
       "      <th>F1 Weighted</th>\n",
       "      <th>Roc Auc Macro</th>\n",
       "      <th>Roc Auc Weighted</th>\n",
       "      <th>Roc Auc Samples</th>\n",
       "      <th>Pr Auc Macro</th>\n",
       "      <th>Pr Auc Weighted</th>\n",
       "      <th>Pr Auc Samples</th>\n",
       "      <th>Jaccard Samples</th>\n",
       "      <th>Jaccard Macro</th>\n",
       "      <th>Jaccard Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.583300</td>\n",
       "      <td>0.136496</td>\n",
       "      <td>0.609441</td>\n",
       "      <td>0.071993</td>\n",
       "      <td>0.329175</td>\n",
       "      <td>0.037365</td>\n",
       "      <td>0.745995</td>\n",
       "      <td>0.072692</td>\n",
       "      <td>0.747686</td>\n",
       "      <td>0.434557</td>\n",
       "      <td>0.515144</td>\n",
       "      <td>0.076820</td>\n",
       "      <td>0.555949</td>\n",
       "      <td>0.515144</td>\n",
       "      <td>0.613199</td>\n",
       "      <td>0.466927</td>\n",
       "      <td>0.573098</td>\n",
       "      <td>0.768878</td>\n",
       "      <td>0.889718</td>\n",
       "      <td>0.122389</td>\n",
       "      <td>0.541441</td>\n",
       "      <td>0.709500</td>\n",
       "      <td>0.539618</td>\n",
       "      <td>0.059121</td>\n",
       "      <td>0.396470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.433800</td>\n",
       "      <td>0.113665</td>\n",
       "      <td>0.666854</td>\n",
       "      <td>0.106456</td>\n",
       "      <td>0.347686</td>\n",
       "      <td>0.035353</td>\n",
       "      <td>0.714415</td>\n",
       "      <td>0.096628</td>\n",
       "      <td>0.736553</td>\n",
       "      <td>0.521931</td>\n",
       "      <td>0.625230</td>\n",
       "      <td>0.125872</td>\n",
       "      <td>0.677378</td>\n",
       "      <td>0.625230</td>\n",
       "      <td>0.669564</td>\n",
       "      <td>0.563913</td>\n",
       "      <td>0.700035</td>\n",
       "      <td>0.876698</td>\n",
       "      <td>0.926063</td>\n",
       "      <td>0.171479</td>\n",
       "      <td>0.654765</td>\n",
       "      <td>0.779754</td>\n",
       "      <td>0.585091</td>\n",
       "      <td>0.087707</td>\n",
       "      <td>0.485853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.379400</td>\n",
       "      <td>0.103920</td>\n",
       "      <td>0.627672</td>\n",
       "      <td>0.103288</td>\n",
       "      <td>0.370624</td>\n",
       "      <td>0.032447</td>\n",
       "      <td>0.895122</td>\n",
       "      <td>0.146988</td>\n",
       "      <td>0.689537</td>\n",
       "      <td>0.667470</td>\n",
       "      <td>0.483276</td>\n",
       "      <td>0.087267</td>\n",
       "      <td>0.538478</td>\n",
       "      <td>0.483276</td>\n",
       "      <td>0.584172</td>\n",
       "      <td>0.545678</td>\n",
       "      <td>0.816023</td>\n",
       "      <td>0.920989</td>\n",
       "      <td>0.940697</td>\n",
       "      <td>0.257442</td>\n",
       "      <td>0.728780</td>\n",
       "      <td>0.822052</td>\n",
       "      <td>0.529403</td>\n",
       "      <td>0.081089</td>\n",
       "      <td>0.448814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.343500</td>\n",
       "      <td>0.083538</td>\n",
       "      <td>0.737581</td>\n",
       "      <td>0.172045</td>\n",
       "      <td>0.477666</td>\n",
       "      <td>0.027319</td>\n",
       "      <td>0.808030</td>\n",
       "      <td>0.256612</td>\n",
       "      <td>0.804292</td>\n",
       "      <td>0.710389</td>\n",
       "      <td>0.678430</td>\n",
       "      <td>0.167206</td>\n",
       "      <td>0.724661</td>\n",
       "      <td>0.678430</td>\n",
       "      <td>0.733320</td>\n",
       "      <td>0.675535</td>\n",
       "      <td>0.879119</td>\n",
       "      <td>0.947028</td>\n",
       "      <td>0.962183</td>\n",
       "      <td>0.335132</td>\n",
       "      <td>0.766255</td>\n",
       "      <td>0.866539</td>\n",
       "      <td>0.667062</td>\n",
       "      <td>0.136510</td>\n",
       "      <td>0.593224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.283500</td>\n",
       "      <td>0.076682</td>\n",
       "      <td>0.750830</td>\n",
       "      <td>0.181496</td>\n",
       "      <td>0.492958</td>\n",
       "      <td>0.024622</td>\n",
       "      <td>0.878574</td>\n",
       "      <td>0.305809</td>\n",
       "      <td>0.854795</td>\n",
       "      <td>0.746643</td>\n",
       "      <td>0.655518</td>\n",
       "      <td>0.159880</td>\n",
       "      <td>0.709470</td>\n",
       "      <td>0.655518</td>\n",
       "      <td>0.748370</td>\n",
       "      <td>0.665400</td>\n",
       "      <td>0.890288</td>\n",
       "      <td>0.950951</td>\n",
       "      <td>0.967984</td>\n",
       "      <td>0.369931</td>\n",
       "      <td>0.778636</td>\n",
       "      <td>0.879262</td>\n",
       "      <td>0.683078</td>\n",
       "      <td>0.144189</td>\n",
       "      <td>0.592827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.266800</td>\n",
       "      <td>0.068757</td>\n",
       "      <td>0.782621</td>\n",
       "      <td>0.279486</td>\n",
       "      <td>0.539235</td>\n",
       "      <td>0.022446</td>\n",
       "      <td>0.865858</td>\n",
       "      <td>0.398412</td>\n",
       "      <td>0.855801</td>\n",
       "      <td>0.758672</td>\n",
       "      <td>0.713985</td>\n",
       "      <td>0.255255</td>\n",
       "      <td>0.761885</td>\n",
       "      <td>0.713985</td>\n",
       "      <td>0.778189</td>\n",
       "      <td>0.720315</td>\n",
       "      <td>0.914098</td>\n",
       "      <td>0.960207</td>\n",
       "      <td>0.974035</td>\n",
       "      <td>0.435769</td>\n",
       "      <td>0.798009</td>\n",
       "      <td>0.897154</td>\n",
       "      <td>0.716915</td>\n",
       "      <td>0.221038</td>\n",
       "      <td>0.643413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.261700</td>\n",
       "      <td>0.068667</td>\n",
       "      <td>0.790376</td>\n",
       "      <td>0.277427</td>\n",
       "      <td>0.550101</td>\n",
       "      <td>0.021686</td>\n",
       "      <td>0.872455</td>\n",
       "      <td>0.339714</td>\n",
       "      <td>0.859155</td>\n",
       "      <td>0.739712</td>\n",
       "      <td>0.722412</td>\n",
       "      <td>0.257859</td>\n",
       "      <td>0.770134</td>\n",
       "      <td>0.722412</td>\n",
       "      <td>0.784359</td>\n",
       "      <td>0.722986</td>\n",
       "      <td>0.928785</td>\n",
       "      <td>0.962977</td>\n",
       "      <td>0.976610</td>\n",
       "      <td>0.468455</td>\n",
       "      <td>0.808033</td>\n",
       "      <td>0.899306</td>\n",
       "      <td>0.724292</td>\n",
       "      <td>0.224901</td>\n",
       "      <td>0.652195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.244600</td>\n",
       "      <td>0.069419</td>\n",
       "      <td>0.775353</td>\n",
       "      <td>0.290474</td>\n",
       "      <td>0.533199</td>\n",
       "      <td>0.022550</td>\n",
       "      <td>0.888700</td>\n",
       "      <td>0.401210</td>\n",
       "      <td>0.857545</td>\n",
       "      <td>0.752224</td>\n",
       "      <td>0.687648</td>\n",
       "      <td>0.261627</td>\n",
       "      <td>0.738578</td>\n",
       "      <td>0.687648</td>\n",
       "      <td>0.766680</td>\n",
       "      <td>0.704035</td>\n",
       "      <td>0.935037</td>\n",
       "      <td>0.962016</td>\n",
       "      <td>0.976544</td>\n",
       "      <td>0.490506</td>\n",
       "      <td>0.811760</td>\n",
       "      <td>0.894241</td>\n",
       "      <td>0.706687</td>\n",
       "      <td>0.231054</td>\n",
       "      <td>0.631262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.248200</td>\n",
       "      <td>0.061233</td>\n",
       "      <td>0.807222</td>\n",
       "      <td>0.383487</td>\n",
       "      <td>0.589537</td>\n",
       "      <td>0.020687</td>\n",
       "      <td>0.853952</td>\n",
       "      <td>0.513256</td>\n",
       "      <td>0.857746</td>\n",
       "      <td>0.794985</td>\n",
       "      <td>0.765341</td>\n",
       "      <td>0.352756</td>\n",
       "      <td>0.805547</td>\n",
       "      <td>0.765341</td>\n",
       "      <td>0.806076</td>\n",
       "      <td>0.769488</td>\n",
       "      <td>0.939656</td>\n",
       "      <td>0.966226</td>\n",
       "      <td>0.981477</td>\n",
       "      <td>0.516552</td>\n",
       "      <td>0.822136</td>\n",
       "      <td>0.915960</td>\n",
       "      <td>0.750865</td>\n",
       "      <td>0.304213</td>\n",
       "      <td>0.689063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.199200</td>\n",
       "      <td>0.063000</td>\n",
       "      <td>0.808569</td>\n",
       "      <td>0.429539</td>\n",
       "      <td>0.582294</td>\n",
       "      <td>0.020911</td>\n",
       "      <td>0.838901</td>\n",
       "      <td>0.460617</td>\n",
       "      <td>0.847619</td>\n",
       "      <td>0.770787</td>\n",
       "      <td>0.780353</td>\n",
       "      <td>0.421380</td>\n",
       "      <td>0.819095</td>\n",
       "      <td>0.780353</td>\n",
       "      <td>0.808062</td>\n",
       "      <td>0.768979</td>\n",
       "      <td>0.938371</td>\n",
       "      <td>0.966902</td>\n",
       "      <td>0.981896</td>\n",
       "      <td>0.524420</td>\n",
       "      <td>0.826524</td>\n",
       "      <td>0.915703</td>\n",
       "      <td>0.751053</td>\n",
       "      <td>0.346934</td>\n",
       "      <td>0.690103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.200400</td>\n",
       "      <td>0.062802</td>\n",
       "      <td>0.797315</td>\n",
       "      <td>0.372972</td>\n",
       "      <td>0.566197</td>\n",
       "      <td>0.021149</td>\n",
       "      <td>0.871099</td>\n",
       "      <td>0.508533</td>\n",
       "      <td>0.876794</td>\n",
       "      <td>0.817019</td>\n",
       "      <td>0.735054</td>\n",
       "      <td>0.332440</td>\n",
       "      <td>0.784118</td>\n",
       "      <td>0.735054</td>\n",
       "      <td>0.800881</td>\n",
       "      <td>0.748330</td>\n",
       "      <td>0.945437</td>\n",
       "      <td>0.968523</td>\n",
       "      <td>0.980792</td>\n",
       "      <td>0.543108</td>\n",
       "      <td>0.831458</td>\n",
       "      <td>0.914274</td>\n",
       "      <td>0.741328</td>\n",
       "      <td>0.292518</td>\n",
       "      <td>0.670476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.210300</td>\n",
       "      <td>0.061496</td>\n",
       "      <td>0.802601</td>\n",
       "      <td>0.420756</td>\n",
       "      <td>0.580684</td>\n",
       "      <td>0.021268</td>\n",
       "      <td>0.845280</td>\n",
       "      <td>0.515660</td>\n",
       "      <td>0.858216</td>\n",
       "      <td>0.805436</td>\n",
       "      <td>0.764024</td>\n",
       "      <td>0.391720</td>\n",
       "      <td>0.806117</td>\n",
       "      <td>0.764024</td>\n",
       "      <td>0.805822</td>\n",
       "      <td>0.762503</td>\n",
       "      <td>0.948525</td>\n",
       "      <td>0.968825</td>\n",
       "      <td>0.981495</td>\n",
       "      <td>0.533926</td>\n",
       "      <td>0.828258</td>\n",
       "      <td>0.917581</td>\n",
       "      <td>0.748384</td>\n",
       "      <td>0.330035</td>\n",
       "      <td>0.685285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.195600</td>\n",
       "      <td>0.059513</td>\n",
       "      <td>0.811507</td>\n",
       "      <td>0.413473</td>\n",
       "      <td>0.588732</td>\n",
       "      <td>0.019629</td>\n",
       "      <td>0.888715</td>\n",
       "      <td>0.555145</td>\n",
       "      <td>0.881824</td>\n",
       "      <td>0.807532</td>\n",
       "      <td>0.746642</td>\n",
       "      <td>0.383795</td>\n",
       "      <td>0.793977</td>\n",
       "      <td>0.746642</td>\n",
       "      <td>0.809721</td>\n",
       "      <td>0.760333</td>\n",
       "      <td>0.947451</td>\n",
       "      <td>0.969235</td>\n",
       "      <td>0.984518</td>\n",
       "      <td>0.576271</td>\n",
       "      <td>0.838590</td>\n",
       "      <td>0.925180</td>\n",
       "      <td>0.753736</td>\n",
       "      <td>0.335424</td>\n",
       "      <td>0.687746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.221800</td>\n",
       "      <td>0.056168</td>\n",
       "      <td>0.815580</td>\n",
       "      <td>0.442739</td>\n",
       "      <td>0.595171</td>\n",
       "      <td>0.019689</td>\n",
       "      <td>0.867796</td>\n",
       "      <td>0.645168</td>\n",
       "      <td>0.866935</td>\n",
       "      <td>0.824098</td>\n",
       "      <td>0.769292</td>\n",
       "      <td>0.400473</td>\n",
       "      <td>0.811851</td>\n",
       "      <td>0.769292</td>\n",
       "      <td>0.812551</td>\n",
       "      <td>0.780227</td>\n",
       "      <td>0.951892</td>\n",
       "      <td>0.970450</td>\n",
       "      <td>0.984347</td>\n",
       "      <td>0.596060</td>\n",
       "      <td>0.844656</td>\n",
       "      <td>0.927397</td>\n",
       "      <td>0.757458</td>\n",
       "      <td>0.351213</td>\n",
       "      <td>0.702551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.201800</td>\n",
       "      <td>0.057629</td>\n",
       "      <td>0.811253</td>\n",
       "      <td>0.443076</td>\n",
       "      <td>0.578672</td>\n",
       "      <td>0.020300</td>\n",
       "      <td>0.856098</td>\n",
       "      <td>0.594190</td>\n",
       "      <td>0.868276</td>\n",
       "      <td>0.815277</td>\n",
       "      <td>0.770872</td>\n",
       "      <td>0.406976</td>\n",
       "      <td>0.815473</td>\n",
       "      <td>0.770872</td>\n",
       "      <td>0.813330</td>\n",
       "      <td>0.770212</td>\n",
       "      <td>0.950907</td>\n",
       "      <td>0.970182</td>\n",
       "      <td>0.984618</td>\n",
       "      <td>0.593833</td>\n",
       "      <td>0.842679</td>\n",
       "      <td>0.928210</td>\n",
       "      <td>0.753937</td>\n",
       "      <td>0.345803</td>\n",
       "      <td>0.688844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.201500</td>\n",
       "      <td>0.057134</td>\n",
       "      <td>0.809462</td>\n",
       "      <td>0.452896</td>\n",
       "      <td>0.586318</td>\n",
       "      <td>0.019808</td>\n",
       "      <td>0.888295</td>\n",
       "      <td>0.654969</td>\n",
       "      <td>0.892421</td>\n",
       "      <td>0.841765</td>\n",
       "      <td>0.743482</td>\n",
       "      <td>0.399916</td>\n",
       "      <td>0.794178</td>\n",
       "      <td>0.743482</td>\n",
       "      <td>0.814027</td>\n",
       "      <td>0.761409</td>\n",
       "      <td>0.954201</td>\n",
       "      <td>0.971080</td>\n",
       "      <td>0.984333</td>\n",
       "      <td>0.604912</td>\n",
       "      <td>0.847568</td>\n",
       "      <td>0.929049</td>\n",
       "      <td>0.756016</td>\n",
       "      <td>0.359679</td>\n",
       "      <td>0.684340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.203100</td>\n",
       "      <td>0.056359</td>\n",
       "      <td>0.812350</td>\n",
       "      <td>0.474219</td>\n",
       "      <td>0.592354</td>\n",
       "      <td>0.019838</td>\n",
       "      <td>0.874090</td>\n",
       "      <td>0.636457</td>\n",
       "      <td>0.886720</td>\n",
       "      <td>0.831293</td>\n",
       "      <td>0.758757</td>\n",
       "      <td>0.417178</td>\n",
       "      <td>0.807022</td>\n",
       "      <td>0.758757</td>\n",
       "      <td>0.818390</td>\n",
       "      <td>0.774507</td>\n",
       "      <td>0.955627</td>\n",
       "      <td>0.971669</td>\n",
       "      <td>0.986078</td>\n",
       "      <td>0.619118</td>\n",
       "      <td>0.850092</td>\n",
       "      <td>0.929921</td>\n",
       "      <td>0.760724</td>\n",
       "      <td>0.368526</td>\n",
       "      <td>0.690397</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Training completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Enhanced Training Configuration for Multi-label Classification\n",
    "import os\n",
    "import warnings\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "# Fix tokenizer parallelism warning\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Suppress future warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    # Output and logging\n",
    "    output_dir=\"./model_output\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    logging_strategy=\"steps\",\n",
    "    \n",
    "    # Learning parameters\n",
    "    learning_rate=2e-5,\n",
    "    lr_scheduler_type=\"linear\",  # Linear decay\n",
    "    warmup_ratio=0.1,  # 10% warmup\n",
    "    weight_decay=0.01,\n",
    "    \n",
    "    # Batch sizes (adjust based on GPU memory)\n",
    "    per_device_train_batch_size=3,\n",
    "    per_device_eval_batch_size=3,\n",
    "    gradient_accumulation_steps=4,  # Effective batch size = 3 * 4 = 12\n",
    "    \n",
    "    # Training epochs and evaluation\n",
    "    num_train_epochs=3,  # Increased for better convergence\n",
    "    eval_strategy=\"steps\",  # More frequent evaluation\n",
    "    eval_steps=100,  # Evaluate every 100 steps\n",
    "    \n",
    "    # Saving strategy\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    save_total_limit=3,  # Keep only 3 best checkpoints\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_f1_micro\",  # Use micro F1 for model selection\n",
    "    greater_is_better=True,\n",
    "    \n",
    "    # Memory and performance optimization\n",
    "    dataloader_pin_memory=False,  # Disable to avoid forking issues\n",
    "    dataloader_num_workers=0,     # Disable multiprocessing\n",
    "    remove_unused_columns=False,  # Keep all columns for multi-label\n",
    "    \n",
    "    # Mixed precision for faster training (if GPU supports it)\n",
    "    fp16=True,  # Enable if using compatible GPU\n",
    "    \n",
    "    # Reproducibility\n",
    "    seed=42,\n",
    "    data_seed=42,\n",
    "    \n",
    "    # Report metrics\n",
    "    report_to=None,  # Disable wandb/tensorboard if not needed\n",
    "    run_name=\"multi_label_posture_classification\",\n",
    ")\n",
    "\n",
    "# Early stopping callback for overfitting control\n",
    "early_stopping = EarlyStoppingCallback(\n",
    "    early_stopping_patience=3,  # Stop if no improvement for 3 evaluations\n",
    "    early_stopping_threshold=0.001  # Minimum improvement threshold\n",
    ")\n",
    "\n",
    "# Initialize trainer with enhanced configuration (using processing_class)\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"val\"],\n",
    "    processing_class=tokenizer,  # Updated parameter name\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[early_stopping],  # Add early stopping callback\n",
    ")\n",
    "\n",
    "print(\"🚀 Starting training with enhanced configuration...\")\n",
    "print(f\"📊 Training samples: {len(tokenized_dataset['train'])}\")\n",
    "print(f\"📊 Validation samples: {len(tokenized_dataset['val'])}\")\n",
    "print(f\"🎯 Target metric: {training_args.metric_for_best_model}\")\n",
    "print(f\"⏱️ Total epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"🔄 Evaluation every: {training_args.eval_steps} steps\")\n",
    "print(f\"💾 Saving every: {training_args.save_steps} steps\")\n",
    "print(f\"⏹️ Early stopping patience: {early_stopping.early_stopping_patience}\")\n",
    "\n",
    "# Start training with error handling\n",
    "try:\n",
    "    print(\"\\n🎯 Starting training...\")\n",
    "    trainer.train()\n",
    "    print(\"✅ Training completed successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Training failed with error: {e}\")\n",
    "    print(\"💡 Consider:\")\n",
    "    print(\"   - Reducing batch size if out of memory\")\n",
    "    print(\"   - Checking data format compatibility\")\n",
    "    print(\"   - Verifying model and tokenizer compatibility\")\n",
    "    print(\"   - The data format may need fixing - check tokenization step\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "37ef7e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 PRE-TRAINING VALIDATION CHECKS\n",
      "==================================================\n",
      "\n",
      "📊 Dataset Structure Check:\n",
      "Available splits: ['train', 'val', 'test']\n",
      "   train: 11597 samples\n",
      "   Features: ['labels', 'input_ids', 'attention_mask']\n",
      "   val: 2485 samples\n",
      "   Features: ['labels', 'input_ids', 'attention_mask']\n",
      "   test: 2486 samples\n",
      "   Features: ['labels', 'input_ids', 'attention_mask']\n",
      "\n",
      "🔍 Sample Data Inspection:\n",
      "Sample keys: dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "Text type: <class 'str'>\n",
      "Labels type: <class 'list'>\n",
      "Labels shape: (27,)\n",
      "Labels dtype: float64\n",
      "Labels sum: 1.0\n",
      "Sample labels: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "\n",
      "Tokenizer info:\n",
      "   Input IDs shape: (512,)\n",
      "   Attention mask shape: (512,)\n",
      "\n",
      "🤖 Model Configuration:\n",
      "   Model type: ModernBertForSequenceClassification\n",
      "   Number of labels: 27\n",
      "   Problem type: multi_label_classification\n",
      "\n",
      "💻 GPU Information:\n",
      "   Device: NVIDIA GeForce RTX 4080 Laptop GPU\n",
      "   Memory allocated: 1.75 GB\n",
      "   Memory reserved: 3.60 GB\n",
      "\n",
      "⚙️ Training Configuration Summary:\n",
      "   Learning rate: 2e-05\n",
      "   Batch size: 3\n",
      "   Gradient accumulation: 4\n",
      "   Effective batch size: 12\n",
      "   Total epochs: 3\n",
      "   Warmup ratio: 0.1\n",
      "   Weight decay: 0.01\n",
      "   FP16 enabled: True\n",
      "\n",
      "⏱️ Training Estimates:\n",
      "   Steps per epoch: 966\n",
      "   Total training steps: 2898\n",
      "   Evaluation every: 100 steps\n",
      "   Number of evaluations: 28\n",
      "\n",
      "✅ Pre-training checks completed!\n",
      "🚀 Ready to start training...\n",
      "🔍 DATA FORMAT DEBUGGING AND FIXING\n",
      "============================================================\n",
      "\n",
      "📊 Original Dataset Structure:\n",
      "Sample keys: dict_keys(['text', 'labels'])\n",
      "Text type: <class 'str'>\n",
      "Text content: This is a bankruptcy appeal involving a telecommunications company and one of its former suppliers. ...\n",
      "Labels type: <class 'list'>\n",
      "Labels: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "🔍 Tokenized Dataset Structure:\n",
      "Tokenized sample keys: dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "   labels: type=<class 'list'>, shape/len=27\n",
      "   input_ids: type=<class 'list'>, shape/len=512\n",
      "   attention_mask: type=<class 'list'>, shape/len=512\n",
      "\n",
      "🔧 Re-tokenizing dataset with fixed function...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing: 100%|██████████| 11597/11597 [01:02<00:00, 185.67 examples/s]\n",
      "Tokenizing:   0%|          | 0/2485 [00:00<?, ? examples/s]\n",
      "Tokenizing: 100%|██████████| 2485/2485 [00:13<00:00, 185.46 examples/s]\n",
      "Tokenizing: 100%|██████████| 2485/2485 [00:13<00:00, 185.46 examples/s]\n",
      "Tokenizing: 100%|██████████| 2486/2486 [00:13<00:00, 184.84 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Re-tokenization successful!\n",
      "\n",
      "✅ Fixed Dataset Structure:\n",
      "Fixed sample keys: dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "   labels: type=<class 'list'>, len=27\n",
      "      Labels: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "   input_ids: type=<class 'list'>, len=512\n",
      "   attention_mask: type=<class 'list'>, len=512\n",
      "\n",
      "📊 Dataset sizes after fixing:\n",
      "   train: 11597 samples\n",
      "   val: 2485 samples\n",
      "   test: 2486 samples\n",
      "\n",
      "✅ FINAL DATASET VERIFICATION:\n",
      "Final sample structure:\n",
      "   labels: type=<class 'list'>, len/shape=27\n",
      "      Sample labels: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "      Labels dtype: <class 'int'>\n",
      "   input_ids: type=<class 'list'>, len/shape=512\n",
      "   attention_mask: type=<class 'list'>, len/shape=512\n",
      "\n",
      "🎯 Dataset is now ready for training!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Pre-training Checks and Debugging\n",
    "\n",
    "print(\"🔍 PRE-TRAINING VALIDATION CHECKS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check dataset structure\n",
    "print(\"\\n📊 Dataset Structure Check:\")\n",
    "print(f\"Available splits: {list(tokenized_dataset.keys())}\")\n",
    "for split_name, split_data in tokenized_dataset.items():\n",
    "    print(f\"   {split_name}: {len(split_data)} samples\")\n",
    "    print(f\"   Features: {list(split_data.features.keys())}\")\n",
    "\n",
    "# Check sample data structure\n",
    "print(\"\\n🔍 Sample Data Inspection:\")\n",
    "sample = tokenized_dataset[\"train\"][0]\n",
    "print(f\"Sample keys: {sample.keys()}\")\n",
    "print(f\"Text type: {type(sample.get('text', 'N/A'))}\")\n",
    "print(f\"Labels type: {type(sample.get('labels', 'N/A'))}\")\n",
    "if 'labels' in sample:\n",
    "    labels_array = np.array(sample['labels'])\n",
    "    print(f\"Labels shape: {labels_array.shape}\")\n",
    "    print(f\"Labels dtype: {labels_array.dtype}\")\n",
    "    print(f\"Labels sum: {labels_array.sum()}\")\n",
    "    print(f\"Sample labels: {sample['labels']}\")\n",
    "\n",
    "# Check tokenizer output\n",
    "print(f\"\\nTokenizer info:\")\n",
    "print(f\"   Input IDs shape: {np.array(sample['input_ids']).shape}\")\n",
    "print(f\"   Attention mask shape: {np.array(sample['attention_mask']).shape}\")\n",
    "\n",
    "# Model configuration check\n",
    "print(f\"\\n🤖 Model Configuration:\")\n",
    "print(f\"   Model type: {type(model).__name__}\")\n",
    "print(f\"   Number of labels: {model.config.num_labels}\")\n",
    "print(f\"   Problem type: {getattr(model.config, 'problem_type', 'Not set')}\")\n",
    "\n",
    "# GPU/CPU check\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\n💻 GPU Information:\")\n",
    "    print(f\"   Device: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"   Memory allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "    print(f\"   Memory reserved: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
    "else:\n",
    "    print(f\"\\n💻 Using CPU for training\")\n",
    "\n",
    "# Training configuration summary\n",
    "print(f\"\\n⚙️ Training Configuration Summary:\")\n",
    "print(f\"   Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"   Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"   Gradient accumulation: {training_args.gradient_accumulation_steps}\")\n",
    "print(f\"   Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"   Total epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"   Warmup ratio: {training_args.warmup_ratio}\")\n",
    "print(f\"   Weight decay: {training_args.weight_decay}\")\n",
    "print(f\"   FP16 enabled: {training_args.fp16}\")\n",
    "\n",
    "# Estimate training time\n",
    "train_samples = len(tokenized_dataset[\"train\"])\n",
    "batch_size = training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps\n",
    "steps_per_epoch = train_samples // batch_size\n",
    "total_steps = steps_per_epoch * training_args.num_train_epochs\n",
    "\n",
    "print(f\"\\n⏱️ Training Estimates:\")\n",
    "print(f\"   Steps per epoch: {steps_per_epoch}\")\n",
    "print(f\"   Total training steps: {total_steps}\")\n",
    "print(f\"   Evaluation every: {training_args.eval_steps} steps\")\n",
    "print(f\"   Number of evaluations: {total_steps // training_args.eval_steps}\")\n",
    "\n",
    "print(f\"\\n✅ Pre-training checks completed!\")\n",
    "print(\"🚀 Ready to start training...\")\n",
    "\n",
    "# Data Format Debugging and Fix\n",
    "\n",
    "print(\"🔍 DATA FORMAT DEBUGGING AND FIXING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check the original dataset structure\n",
    "print(\"\\n📊 Original Dataset Structure:\")\n",
    "sample = dataset[\"train\"][0]\n",
    "print(f\"Sample keys: {sample.keys()}\")\n",
    "print(f\"Text type: {type(sample['text'])}\")\n",
    "print(f\"Text content: {str(sample['text'])[:100]}...\")\n",
    "print(f\"Labels type: {type(sample['labels'])}\")\n",
    "print(f\"Labels: {sample['labels']}\")\n",
    "\n",
    "# The issue is likely in the preprocess_function\n",
    "# Let's check what the tokenized dataset looks like\n",
    "print(f\"\\n🔍 Tokenized Dataset Structure:\")\n",
    "if 'tokenized_dataset' in globals():\n",
    "    tokenized_sample = tokenized_dataset[\"train\"][0]\n",
    "    print(f\"Tokenized sample keys: {tokenized_sample.keys()}\")\n",
    "    for key, value in tokenized_sample.items():\n",
    "        print(f\"   {key}: type={type(value)}, shape/len={getattr(value, 'shape', len(value) if hasattr(value, '__len__') else 'N/A')}\")\n",
    "        if key == 'text' and hasattr(value, '__iter__') and not isinstance(value, str):\n",
    "            print(f\"      First few elements: {list(value)[:3] if hasattr(value, '__iter__') else value}\")\n",
    "\n",
    "# Fix the preprocess function\n",
    "def fixed_preprocess_function(examples):\n",
    "    \"\"\"\n",
    "    Fixed preprocessing function for multi-label classification\n",
    "    \"\"\"\n",
    "    # Handle batch processing\n",
    "    if isinstance(examples['text'], list):\n",
    "        texts = examples['text']\n",
    "        labels = examples['labels']\n",
    "    else:\n",
    "        texts = [examples['text']]\n",
    "        labels = [examples['labels']]\n",
    "    \n",
    "    # Tokenize the texts\n",
    "    tokenized = tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=512,\n",
    "        return_tensors=None  # Return lists, not tensors\n",
    "    )\n",
    "    \n",
    "    # Ensure labels are properly formatted\n",
    "    processed_labels = []\n",
    "    for label_list in labels:\n",
    "        if isinstance(label_list, (list, tuple)):\n",
    "            # Convert to float and ensure it's a list\n",
    "            processed_labels.append([float(x) for x in label_list])\n",
    "        else:\n",
    "            # Handle single values\n",
    "            processed_labels.append([float(label_list)])\n",
    "    \n",
    "    tokenized['labels'] = processed_labels\n",
    "    return tokenized\n",
    "\n",
    "# Re-tokenize the dataset with the fixed function\n",
    "print(f\"\\n🔧 Re-tokenizing dataset with fixed function...\")\n",
    "try:\n",
    "    tokenized_dataset_fixed = dataset.map(\n",
    "        fixed_preprocess_function,\n",
    "        batched=True,\n",
    "        remove_columns=dataset[\"train\"].column_names,\n",
    "        desc=\"Tokenizing\"\n",
    "    )\n",
    "    \n",
    "    print(f\"✅ Re-tokenization successful!\")\n",
    "    \n",
    "    # Check the fixed dataset\n",
    "    print(f\"\\n✅ Fixed Dataset Structure:\")\n",
    "    fixed_sample = tokenized_dataset_fixed[\"train\"][0]\n",
    "    print(f\"Fixed sample keys: {fixed_sample.keys()}\")\n",
    "    for key, value in fixed_sample.items():\n",
    "        print(f\"   {key}: type={type(value)}, len={len(value) if hasattr(value, '__len__') else 'N/A'}\")\n",
    "        if key == 'labels':\n",
    "            print(f\"      Labels: {value}\")\n",
    "    \n",
    "    # Update the global tokenized_dataset\n",
    "    tokenized_dataset = tokenized_dataset_fixed\n",
    "    \n",
    "    print(f\"\\n📊 Dataset sizes after fixing:\")\n",
    "    for split_name in tokenized_dataset.keys():\n",
    "        print(f\"   {split_name}: {len(tokenized_dataset[split_name])} samples\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Re-tokenization failed: {e}\")\n",
    "    print(\"🔍 Let's try a simpler approach...\")\n",
    "    \n",
    "    # Alternative: Manual tokenization\n",
    "    def simple_tokenize_sample(sample):\n",
    "        text = str(sample['text'])  # Ensure it's a string\n",
    "        labels = sample['labels']\n",
    "        \n",
    "        # Tokenize\n",
    "        tokenized = tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=512,\n",
    "            return_tensors=None\n",
    "        )\n",
    "        \n",
    "        # Ensure labels are float list\n",
    "        if isinstance(labels, (list, tuple)):\n",
    "            tokenized['labels'] = [float(x) for x in labels]\n",
    "        else:\n",
    "            tokenized['labels'] = [float(labels)]\n",
    "            \n",
    "        return tokenized\n",
    "    \n",
    "    # Apply simple tokenization\n",
    "    tokenized_dataset = dataset.map(simple_tokenize_sample, desc=\"Simple tokenization\")\n",
    "    print(f\"✅ Simple tokenization completed!\")\n",
    "\n",
    "# Verify the final dataset\n",
    "print(f\"\\n✅ FINAL DATASET VERIFICATION:\")\n",
    "final_sample = tokenized_dataset[\"train\"][0]\n",
    "print(f\"Final sample structure:\")\n",
    "for key, value in final_sample.items():\n",
    "    print(f\"   {key}: type={type(value)}, len/shape={len(value) if hasattr(value, '__len__') else 'N/A'}\")\n",
    "    if key == 'labels':\n",
    "        print(f\"      Sample labels: {value}\")\n",
    "        print(f\"      Labels dtype: {type(value[0]) if isinstance(value, list) and len(value) > 0 else 'N/A'}\")\n",
    "\n",
    "print(f\"\\n🎯 Dataset is now ready for training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05447e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842c3469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-Training Evaluation and Testing\n",
    "\n",
    "print(\"🔍 COMPREHENSIVE MODEL EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Evaluate on validation set\n",
    "print(\"\\n📊 Validation Set Evaluation:\")\n",
    "val_results = trainer.evaluate()\n",
    "\n",
    "# Display key metrics\n",
    "key_metrics = [\n",
    "    'eval_f1_micro', 'eval_f1_macro', 'eval_accuracy', 'eval_hamming_loss',\n",
    "    'eval_precision_micro', 'eval_recall_micro', 'eval_roc_auc_macro'\n",
    "]\n",
    "\n",
    "for metric in key_metrics:\n",
    "    if metric in val_results:\n",
    "        print(f\"   {metric}: {val_results[metric]:.4f}\")\n",
    "\n",
    "# Test on test set if available\n",
    "if \"test\" in tokenized_dataset:\n",
    "    print(\"\\n🎯 Test Set Evaluation:\")\n",
    "    test_results = trainer.evaluate(eval_dataset=tokenized_dataset[\"test\"])\n",
    "    \n",
    "    for metric in key_metrics:\n",
    "        if metric in test_results:\n",
    "            print(f\"   {metric}: {test_results[metric]:.4f}\")\n",
    "\n",
    "# Get predictions for detailed analysis\n",
    "print(\"\\n🔬 Detailed Prediction Analysis:\")\n",
    "\n",
    "# Predict on validation set\n",
    "val_predictions = trainer.predict(tokenized_dataset[\"val\"])\n",
    "val_probs = sigmoid(val_predictions.predictions)\n",
    "val_binary = (val_probs > 0.5).astype(int)\n",
    "val_true = val_predictions.label_ids\n",
    "\n",
    "# Use comprehensive evaluation function\n",
    "detailed_metrics = comprehensive_evaluation(\n",
    "    y_true=val_true,\n",
    "    y_pred_proba=val_probs,\n",
    "    y_pred_binary=val_binary\n",
    ")\n",
    "\n",
    "print(\"\\n📈 Comprehensive Metrics Summary:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Group metrics by type\n",
    "metric_groups = {\n",
    "    'Precision': ['precision_micro', 'precision_macro', 'precision_samples', 'precision_weighted'],\n",
    "    'Recall': ['recall_micro', 'recall_macro', 'recall_samples', 'recall_weighted'],\n",
    "    'F1-Score': ['f1_micro', 'f1_macro', 'f1_samples', 'f1_weighted'],\n",
    "    'ROC-AUC': ['roc_auc_macro', 'roc_auc_weighted', 'roc_auc_samples'],\n",
    "    'PR-AUC': ['pr_auc_macro', 'pr_auc_weighted', 'pr_auc_samples'],\n",
    "    'Other': ['accuracy', 'hamming_loss', 'jaccard_macro', 'jaccard_samples']\n",
    "}\n",
    "\n",
    "for group_name, metrics in metric_groups.items():\n",
    "    print(f\"\\n{group_name}:\")\n",
    "    for metric in metrics:\n",
    "        if metric in detailed_metrics:\n",
    "            print(f\"   {metric}: {detailed_metrics[metric]:.4f}\")\n",
    "\n",
    "# Sample predictions analysis\n",
    "print(\"\\n🔍 Sample Predictions Analysis:\")\n",
    "sample_size = min(5, len(val_true))\n",
    "for i in range(sample_size):\n",
    "    print(f\"\\nSample {i+1}:\")\n",
    "    print(f\"   True labels: {val_true[i]}\")\n",
    "    print(f\"   Predicted:   {val_binary[i]}\")\n",
    "    print(f\"   Probabilities: {val_probs[i]}\")\n",
    "    print(f\"   Match: {'✅' if np.array_equal(val_true[i], val_binary[i]) else '❌'}\")\n",
    "\n",
    "# Model performance summary\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"🏆 MODEL PERFORMANCE SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"✅ Best Metric (F1-Micro): {detailed_metrics['f1_micro']:.4f}\")\n",
    "print(f\"📊 Accuracy: {detailed_metrics['accuracy']:.4f}\")\n",
    "print(f\"🔻 Hamming Loss: {detailed_metrics['hamming_loss']:.4f}\")\n",
    "print(f\"🎯 Macro F1: {detailed_metrics['f1_macro']:.4f}\")\n",
    "\n",
    "if detailed_metrics['f1_micro'] > 0.7:\n",
    "    print(\"🎉 Excellent performance! Model is ready for deployment.\")\n",
    "elif detailed_metrics['f1_micro'] > 0.5:\n",
    "    print(\"👍 Good performance! Consider fine-tuning for better results.\")\n",
    "else:\n",
    "    print(\"⚠️ Performance needs improvement. Consider:\")\n",
    "    print(\"   - More training epochs\")\n",
    "    print(\"   - Different learning rate\")\n",
    "    print(\"   - Data augmentation\")\n",
    "    print(\"   - Different model architecture\")\n",
    "\n",
    "print(f\"\\n💾 Model saved to: {training_args.output_dir}\")\n",
    "print(\"🚀 Training and evaluation completed successfully!\")# Enhanced Training Configuration for Multi-label Classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9281384b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Re-tokenizing dataset with fixed function...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing dataset: 100%|██████████| 11597/11597 [01:04<00:00, 178.84 examples/s]\n",
      "Tokenizing dataset:   0%|          | 0/2485 [00:00<?, ? examples/s]\n",
      "Tokenizing dataset: 100%|██████████| 2485/2485 [00:13<00:00, 181.60 examples/s]\n",
      "Tokenizing dataset:   0%|          | 0/2486 [00:00<?, ? examples/s]\n",
      "Tokenizing dataset: 100%|██████████| 2486/2486 [00:13<00:00, 184.39 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Tokenized dataset verification:\n",
      "Features: ['labels', 'input_ids', 'attention_mask']\n",
      "\n",
      "Sample structure:\n",
      "  labels: List/Array of length 27, dtype: <class 'int'>, shape: (27,), sum: 1\n",
      "  input_ids: List/Array of length 512, dtype: <class 'int'>\n",
      "  attention_mask: List/Array of length 512, dtype: <class 'int'>\n",
      "\n",
      "🎯 Labels verification:\n",
      "  Labels dtype: int64\n",
      "  Labels shape: (27,)\n",
      "  Expected shape: (27,)\n",
      "  Labels range: [0.0, 1.0]\n",
      "⚠️ Warning: Labels are not float32, this may cause training issues\n",
      "\n",
      "📊 Dataset sizes after tokenization:\n",
      "  train: 11597 samples\n",
      "  val: 2485 samples\n",
      "  test: 2486 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 🔧 FIXED TOKENIZATION AND DATA FORMAT\n",
    "# This section addresses the data format issues that cause training failures\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    \"\"\"\n",
    "    Proper tokenization function for multi-label classification.\n",
    "    Ensures all outputs are compatible with HuggingFace Trainer.\n",
    "    \"\"\"\n",
    "    # Handle batch vs single example\n",
    "    if isinstance(examples['text'], str):\n",
    "        texts = [examples['text']]\n",
    "        labels = [examples['labels']]\n",
    "    else:\n",
    "        texts = examples['text']\n",
    "        labels = examples['labels']\n",
    "    \n",
    "    # Tokenize the texts\n",
    "    tokenized = tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        padding=True,  # Will be handled by data collator\n",
    "        max_length=512,  # Adjust based on your model's limit\n",
    "        return_tensors=None  # Don't return tensors yet, let data collator handle it\n",
    "    )\n",
    "    \n",
    "    # Ensure labels are float32 for BCEWithLogitsLoss\n",
    "    if isinstance(labels[0], (list, np.ndarray)):\n",
    "        tokenized['labels'] = [np.array(label, dtype=np.float32).tolist() for label in labels]\n",
    "    else:\n",
    "        tokenized['labels'] = [np.array(labels, dtype=np.float32).tolist()]\n",
    "    \n",
    "    return tokenized\n",
    "\n",
    "print(\"🔧 Re-tokenizing dataset with fixed function...\")\n",
    "\n",
    "# Apply the tokenization function\n",
    "tokenized_dataset = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=['text'],  # Remove the problematic text column\n",
    "    desc=\"Tokenizing dataset\"\n",
    ")\n",
    "\n",
    "# Verify the tokenized dataset structure\n",
    "print(\"\\n✅ Tokenized dataset verification:\")\n",
    "print(f\"Features: {list(tokenized_dataset['train'].features.keys())}\")\n",
    "\n",
    "# Check a sample\n",
    "sample = tokenized_dataset[\"train\"][0]\n",
    "print(f\"\\nSample structure:\")\n",
    "for key, value in sample.items():\n",
    "    if isinstance(value, (list, np.ndarray)):\n",
    "        value_info = f\"List/Array of length {len(value)}, dtype: {type(value[0]) if value else 'empty'}\"\n",
    "        if key == 'labels':\n",
    "            value_info += f\", shape: {np.array(value).shape}, sum: {np.sum(value)}\"\n",
    "    else:\n",
    "        value_info = f\"Type: {type(value)}, Value: {value}\"\n",
    "    print(f\"  {key}: {value_info}\")\n",
    "\n",
    "# Verify labels are float\n",
    "sample_labels = np.array(sample['labels'])\n",
    "print(f\"\\n🎯 Labels verification:\")\n",
    "print(f\"  Labels dtype: {sample_labels.dtype}\")\n",
    "print(f\"  Labels shape: {sample_labels.shape}\")\n",
    "print(f\"  Expected shape: ({len(class_name)},)\")\n",
    "print(f\"  Labels range: [{sample_labels.min():.1f}, {sample_labels.max():.1f}]\")\n",
    "\n",
    "if sample_labels.dtype != np.float32:\n",
    "    print(\"⚠️ Warning: Labels are not float32, this may cause training issues\")\n",
    "else:\n",
    "    print(\"✅ Labels are properly formatted as float32\")\n",
    "\n",
    "print(f\"\\n📊 Dataset sizes after tokenization:\")\n",
    "for split_name, split_data in tokenized_dataset.items():\n",
    "    print(f\"  {split_name}: {len(split_data)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e406e206",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1007029/2743133405.py:14: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "/mnt/d/TR-Project/.venv/lib/python3.13/site-packages/torch/_inductor/compile_fx.py:236: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.\n",
      "  warnings.warn(\n",
      "W0621 11:00:46.017000 1007029 torch/_inductor/utils.py:1250] [1/0] Not enough SMs to use max_autotune_gemm mode\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "result type Float can't be cast to the desired output type Long",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[49]\u001b[39m\u001b[32m, line 25\u001b[39m\n",
      "\u001b[32m      1\u001b[39m training_args = TrainingArguments(\n",
      "\u001b[32m      2\u001b[39m \n",
      "\u001b[32m      3\u001b[39m    output_dir=\u001b[33m\"\u001b[39m\u001b[33mmodel_output\u001b[39m\u001b[33m\"\u001b[39m,\n",
      "\u001b[32m   (...)\u001b[39m\u001b[32m     11\u001b[39m    load_best_model_at_end=\u001b[38;5;28;01mTrue\u001b[39;00m,\n",
      "\u001b[32m     12\u001b[39m )\n",
      "\u001b[32m     14\u001b[39m trainer = Trainer(\n",
      "\u001b[32m     15\u001b[39m \n",
      "\u001b[32m     16\u001b[39m    model=model,\n",
      "\u001b[32m   (...)\u001b[39m\u001b[32m     22\u001b[39m    compute_metrics=compute_metrics,\n",
      "\u001b[32m     23\u001b[39m )\n",
      "\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/d/TR-Project/.venv/lib/python3.13/site-packages/transformers/trainer.py:2240\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n",
      "\u001b[32m   2238\u001b[39m         hf_hub_utils.enable_progress_bars()\n",
      "\u001b[32m   2239\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[32m-> \u001b[39m\u001b[32m2240\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[32m   2241\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   2242\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   2243\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   2244\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   2245\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/d/TR-Project/.venv/lib/python3.13/site-packages/transformers/trainer.py:2555\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n",
      "\u001b[32m   2548\u001b[39m context = (\n",
      "\u001b[32m   2549\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n",
      "\u001b[32m   2550\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n",
      "\u001b[32m   2551\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n",
      "\u001b[32m   2552\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n",
      "\u001b[32m   2553\u001b[39m )\n",
      "\u001b[32m   2554\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n",
      "\u001b[32m-> \u001b[39m\u001b[32m2555\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m   2557\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n",
      "\u001b[32m   2558\u001b[39m     args.logging_nan_inf_filter\n",
      "\u001b[32m   2559\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n",
      "\u001b[32m   2560\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n",
      "\u001b[32m   2561\u001b[39m ):\n",
      "\u001b[32m   2562\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n",
      "\u001b[32m   2563\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/d/TR-Project/.venv/lib/python3.13/site-packages/transformers/trainer.py:3745\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(self, model, inputs, num_items_in_batch)\u001b[39m\n",
      "\u001b[32m   3742\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb.reduce_mean().detach().to(\u001b[38;5;28mself\u001b[39m.args.device)\n",
      "\u001b[32m   3744\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.compute_loss_context_manager():\n",
      "\u001b[32m-> \u001b[39m\u001b[32m3745\u001b[39m     loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m   3747\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n",
      "\u001b[32m   3748\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n",
      "\u001b[32m   3749\u001b[39m     \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[32m   3750\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.global_step % \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps == \u001b[32m0\u001b[39m\n",
      "\u001b[32m   3751\u001b[39m ):\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/d/TR-Project/.venv/lib/python3.13/site-packages/transformers/trainer.py:3810\u001b[39m, in \u001b[36mTrainer.compute_loss\u001b[39m\u001b[34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[39m\n",
      "\u001b[32m   3808\u001b[39m         loss_kwargs[\u001b[33m\"\u001b[39m\u001b[33mnum_items_in_batch\u001b[39m\u001b[33m\"\u001b[39m] = num_items_in_batch\n",
      "\u001b[32m   3809\u001b[39m     inputs = {**inputs, **loss_kwargs}\n",
      "\u001b[32m-> \u001b[39m\u001b[32m3810\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m   3811\u001b[39m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n",
      "\u001b[32m   3812\u001b[39m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n",
      "\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.past_index >= \u001b[32m0\u001b[39m:\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/d/TR-Project/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n",
      "\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/d/TR-Project/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n",
      "\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n",
      "\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/d/TR-Project/.venv/lib/python3.13/site-packages/transformers/models/modernbert/modeling_modernbert.py:1215\u001b[39m, in \u001b[36mModernBertForSequenceClassification.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, sliding_window_mask, position_ids, inputs_embeds, labels, indices, cu_seqlens, max_seqlen, batch_size, seq_len, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[39m\n",
      "\u001b[32m   1213\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.problem_type == \u001b[33m\"\u001b[39m\u001b[33mmulti_label_classification\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[32m   1214\u001b[39m         loss_fct = BCEWithLogitsLoss()\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1215\u001b[39m         loss = \u001b[43mloss_fct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m   1217\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n",
      "\u001b[32m   1218\u001b[39m     output = (logits,)\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/d/TR-Project/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n",
      "\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/d/TR-Project/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n",
      "\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n",
      "\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/d/TR-Project/.venv/lib/python3.13/site-packages/torch/nn/modules/loss.py:821\u001b[39m, in \u001b[36mBCEWithLogitsLoss.forward\u001b[39m\u001b[34m(self, input, target)\u001b[39m\n",
      "\u001b[32m    820\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) -> Tensor:\n",
      "\u001b[32m--> \u001b[39m\u001b[32m821\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbinary_cross_entropy_with_logits\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[32m    822\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    823\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpos_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpos_weight\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    826\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    827\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/d/TR-Project/.venv/lib/python3.13/site-packages/torch/nn/functional.py:3643\u001b[39m, in \u001b[36mbinary_cross_entropy_with_logits\u001b[39m\u001b[34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[39m\n",
      "\u001b[32m   3638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (target.size() == \u001b[38;5;28minput\u001b[39m.size()):\n",
      "\u001b[32m   3639\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n",
      "\u001b[32m   3640\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTarget size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget.size()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) must be the same as input size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m.size()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[32m   3641\u001b[39m     )\n",
      "\u001b[32m-> \u001b[39m\u001b[32m3643\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbinary_cross_entropy_with_logits\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[32m   3644\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction_enum\u001b[49m\n",
      "\u001b[32m   3645\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[31mRuntimeError\u001b[39m: result type Float can't be cast to the desired output type Long"
     ]
    }
   ],
   "source": [
    "# Post-Training Evaluation and Testing\n",
    "\n",
    "print(\"🔍 COMPREHENSIVE MODEL EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Evaluate on validation set\n",
    "print(\"\\n📊 Validation Set Evaluation:\")\n",
    "val_results = trainer.evaluate()\n",
    "\n",
    "# Display key metrics\n",
    "key_metrics = [\n",
    "    'eval_f1_micro', 'eval_f1_macro', 'eval_accuracy', 'eval_hamming_loss',\n",
    "    'eval_precision_micro', 'eval_recall_micro', 'eval_roc_auc_macro'\n",
    "]\n",
    "\n",
    "for metric in key_metrics:\n",
    "    if metric in val_results:\n",
    "        print(f\"   {metric}: {val_results[metric]:.4f}\")\n",
    "\n",
    "# Test on test set if available\n",
    "if \"test\" in tokenized_dataset:\n",
    "    print(\"\\n🎯 Test Set Evaluation:\")\n",
    "    test_results = trainer.evaluate(eval_dataset=tokenized_dataset[\"test\"])\n",
    "    \n",
    "    for metric in key_metrics:\n",
    "        if metric in test_results:\n",
    "            print(f\"   {metric}: {test_results[metric]:.4f}\")\n",
    "\n",
    "# Get predictions for detailed analysis\n",
    "print(\"\\n🔬 Detailed Prediction Analysis:\")\n",
    "\n",
    "# Predict on validation set\n",
    "val_predictions = trainer.predict(tokenized_dataset[\"val\"])\n",
    "val_probs = sigmoid(val_predictions.predictions)\n",
    "val_binary = (val_probs > 0.5).astype(int)\n",
    "val_true = val_predictions.label_ids\n",
    "\n",
    "# Use comprehensive evaluation function\n",
    "detailed_metrics = comprehensive_evaluation(\n",
    "    y_true=val_true,\n",
    "    y_pred_proba=val_probs,\n",
    "    y_pred_binary=val_binary\n",
    ")\n",
    "\n",
    "print(\"\\n📈 Comprehensive Metrics Summary:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Group metrics by type\n",
    "metric_groups = {\n",
    "    'Precision': ['precision_micro', 'precision_macro', 'precision_samples', 'precision_weighted'],\n",
    "    'Recall': ['recall_micro', 'recall_macro', 'recall_samples', 'recall_weighted'],\n",
    "    'F1-Score': ['f1_micro', 'f1_macro', 'f1_samples', 'f1_weighted'],\n",
    "    'ROC-AUC': ['roc_auc_macro', 'roc_auc_weighted', 'roc_auc_samples'],\n",
    "    'PR-AUC': ['pr_auc_macro', 'pr_auc_weighted', 'pr_auc_samples'],\n",
    "    'Other': ['accuracy', 'hamming_loss', 'jaccard_macro', 'jaccard_samples']\n",
    "}\n",
    "\n",
    "for group_name, metrics in metric_groups.items():\n",
    "    print(f\"\\n{group_name}:\")\n",
    "    for metric in metrics:\n",
    "        if metric in detailed_metrics:\n",
    "            print(f\"   {metric}: {detailed_metrics[metric]:.4f}\")\n",
    "\n",
    "# Sample predictions analysis\n",
    "print(\"\\n🔍 Sample Predictions Analysis:\")\n",
    "sample_size = min(5, len(val_true))\n",
    "for i in range(sample_size):\n",
    "    print(f\"\\nSample {i+1}:\")\n",
    "    print(f\"   True labels: {val_true[i]}\")\n",
    "    print(f\"   Predicted:   {val_binary[i]}\")\n",
    "    print(f\"   Probabilities: {val_probs[i]}\")\n",
    "    print(f\"   Match: {'✅' if np.array_equal(val_true[i], val_binary[i]) else '❌'}\")\n",
    "\n",
    "# Model performance summary\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"🏆 MODEL PERFORMANCE SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"✅ Best Metric (F1-Micro): {detailed_metrics['f1_micro']:.4f}\")\n",
    "print(f\"📊 Accuracy: {detailed_metrics['accuracy']:.4f}\")\n",
    "print(f\"🔻 Hamming Loss: {detailed_metrics['hamming_loss']:.4f}\")\n",
    "print(f\"🎯 Macro F1: {detailed_metrics['f1_macro']:.4f}\")\n",
    "\n",
    "if detailed_metrics['f1_micro'] > 0.7:\n",
    "    print(\"🎉 Excellent performance! Model is ready for deployment.\")\n",
    "elif detailed_metrics['f1_micro'] > 0.5:\n",
    "    print(\"👍 Good performance! Consider fine-tuning for better results.\")\n",
    "else:\n",
    "    print(\"⚠️ Performance needs improvement. Consider:\")\n",
    "    print(\"   - More training epochs\")\n",
    "    print(\"   - Different learning rate\")\n",
    "    print(\"   - Data augmentation\")\n",
    "    print(\"   - Different model architecture\")\n",
    "\n",
    "print(f\"\\n💾 Model saved to: {training_args.output_dir}\")\n",
    "print(\"🚀 Training and evaluation completed successfully!\")# Enhanced Training Configuration for Multi-label Classification\n",
    "training_args = TrainingArguments(\n",
    "    # Output and logging\n",
    "    output_dir=\"./model_output\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    logging_strategy=\"steps\",\n",
    "    \n",
    "    # Learning parameters\n",
    "    learning_rate=2e-5,\n",
    "    lr_scheduler_type=\"linear\",  # Linear decay\n",
    "    warmup_ratio=0.1,  # 10% warmup\n",
    "    weight_decay=0.01,\n",
    "    \n",
    "    # Batch sizes (adjust based on GPU memory)\n",
    "    per_device_train_batch_size=3,\n",
    "    per_device_eval_batch_size=3,\n",
    "    gradient_accumulation_steps=4,  # Effective batch size = 3 * 4 = 12\n",
    "    \n",
    "    # Training epochs and evaluation\n",
    "    num_train_epochs=3,  # Increased for better convergence\n",
    "    eval_strategy=\"steps\",  # More frequent evaluation\n",
    "    eval_steps=100,  # Evaluate every 100 steps\n",
    "    \n",
    "    # Saving strategy\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    save_total_limit=3,  # Keep only 3 best checkpoints\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_f1_micro\",  # Use micro F1 for model selection\n",
    "    greater_is_better=True,\n",
    "    \n",
    "    # Early stopping and overfitting control\n",
    "    early_stopping_patience=3,  # Stop if no improvement for 3 evaluations\n",
    "    \n",
    "    # Memory and performance optimization\n",
    "    dataloader_pin_memory=True,\n",
    "    dataloader_num_workers=2,\n",
    "    remove_unused_columns=False,  # Keep all columns for multi-label\n",
    "    \n",
    "    # Mixed precision for faster training (if GPU supports it)\n",
    "    fp16=True,  # Enable if using compatible GPU\n",
    "    \n",
    "    # Reproducibility\n",
    "    seed=42,\n",
    "    data_seed=42,\n",
    "    \n",
    "    # Report metrics\n",
    "    report_to=None,  # Disable wandb/tensorboard if not needed\n",
    "    run_name=\"multi_label_posture_classification\",\n",
    ")\n",
    "\n",
    "# Initialize trainer with enhanced configuration\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"val\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"🚀 Starting training with enhanced configuration...\")\n",
    "print(f\"📊 Training samples: {len(tokenized_dataset['train'])}\")\n",
    "print(f\"📊 Validation samples: {len(tokenized_dataset['val'])}\")\n",
    "print(f\"🎯 Target metric: {training_args.metric_for_best_model}\")\n",
    "print(f\"⏱️ Total epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"🔄 Evaluation every: {training_args.eval_steps} steps\")\n",
    "print(f\"💾 Saving every: {training_args.save_steps} steps\")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800059bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5f37e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Converting labels to float32 using datasets.cast_column...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Casting the dataset: 100%|██████████| 11597/11597 [00:00<00:00, 232166.06 examples/s]\n",
      "Casting the dataset: 100%|██████████| 2485/2485 [00:00<00:00, 761458.61 examples/s]\n",
      "Casting the dataset: 100%|██████████| 2486/2486 [00:00<00:00, 839536.21 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Labels conversion verification:\n",
      "  Labels dtype: float64\n",
      "  Labels shape: (27,)\n",
      "  Sample labels: [0.0, 0.0, 0.0, 0.0, 0.0]...\n",
      "  HF Feature type: Sequence(feature=Value(dtype='float32', id=None), length=27, id=None)\n",
      "  PyTorch tensor dtype: torch.float32\n",
      "  PyTorch tensor shape: torch.Size([27])\n",
      "❌ ISSUE: Labels are still float64, expected float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 🔧 EXPLICIT LABEL TYPE CONVERSION\n",
    "# Convert labels to float32 using HuggingFace datasets features\n",
    "\n",
    "from datasets import Sequence, Value\n",
    "import torch\n",
    "\n",
    "print(\"🔄 Converting labels to float32 using datasets.cast_column...\")\n",
    "\n",
    "# Define the proper feature type for multi-label classification\n",
    "# Labels should be a sequence of floats (one per class)\n",
    "label_feature = Sequence(Value(\"float32\"), length=len(class_name))\n",
    "\n",
    "# Cast the labels column to float32 for all splits\n",
    "for split_name in tokenized_dataset.keys():\n",
    "    tokenized_dataset[split_name] = tokenized_dataset[split_name].cast_column(\"labels\", label_feature)\n",
    "\n",
    "# Verify the fix\n",
    "print(f\"\\n✅ Labels conversion verification:\")\n",
    "sample = tokenized_dataset[\"train\"][0]\n",
    "sample_labels = np.array(sample['labels'])\n",
    "print(f\"  Labels dtype: {sample_labels.dtype}\")\n",
    "print(f\"  Labels shape: {sample_labels.shape}\")\n",
    "print(f\"  Sample labels: {sample['labels'][:5]}...\")  # Show first 5 labels\n",
    "print(f\"  HF Feature type: {tokenized_dataset['train'].features['labels']}\")\n",
    "\n",
    "# Test tensor conversion\n",
    "test_labels = torch.tensor(sample['labels'], dtype=torch.float32)\n",
    "print(f\"  PyTorch tensor dtype: {test_labels.dtype}\")\n",
    "print(f\"  PyTorch tensor shape: {test_labels.shape}\")\n",
    "\n",
    "if sample_labels.dtype == np.float32:\n",
    "    print(\"✅ SUCCESS: Labels are now properly formatted as float32\")\n",
    "    print(\"🚀 Ready for training!\")\n",
    "else:\n",
    "    print(f\"❌ ISSUE: Labels are still {sample_labels.dtype}, expected float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "da2f726e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔬 FINAL MODEL EVALUATION\n",
      "============================================================\n",
      "🔍 Checking test set data types...\n",
      "Test labels dtype: float64\n",
      "⚠️ Test set labels need conversion, performing conversion...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Casting the dataset: 100%|██████████| 2486/2486 [00:00<00:00, 141437.29 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Test set labels converted to float32\n",
      "📊 Generating predictions on test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape: (2486, 27)\n",
      "True labels shape: (2486, 27)\n",
      "📊 Calculating comprehensive metrics...\n",
      "\n",
      "🏆 TEST SET RESULTS:\n",
      "==================================================\n",
      "\n",
      "📈 Primary Metrics:\n",
      "  F1_MICRO: 0.8166\n",
      "  F1_MACRO: 0.4383\n",
      "  F1_WEIGHTED: 0.7839\n",
      "  F1_SAMPLES: 0.8150\n",
      "\n",
      "🎯 Precision:\n",
      "  PRECISION_MICRO: 0.8602\n",
      "  PRECISION_MACRO: 0.5993\n",
      "  PRECISION_WEIGHTED: 0.8275\n",
      "  PRECISION_SAMPLES: 0.8644\n",
      "\n",
      "🔍 Recall:\n",
      "  RECALL_MICRO: 0.7772\n",
      "  RECALL_MACRO: 0.4058\n",
      "  RECALL_WEIGHTED: 0.7772\n",
      "  RECALL_SAMPLES: 0.8182\n",
      "\n",
      "📊 Other Metrics:\n",
      "  ACCURACY: 0.6010\n",
      "  HAMMING_LOSS: 0.0197\n",
      "  JACCARD_SAMPLES: 0.7609\n",
      "  JACCARD_MACRO: 0.3490\n",
      "  JACCARD_WEIGHTED: 0.7040\n",
      "\n",
      "📡 AUC Metrics:\n",
      "  ROC_AUC_MACRO: 0.9469\n",
      "  ROC_AUC_WEIGHTED: 0.9684\n",
      "  ROC_AUC_SAMPLES: 0.9853\n",
      "  PR_AUC_MACRO: 0.5811\n",
      "  PR_AUC_WEIGHTED: 0.8353\n",
      "  PR_AUC_SAMPLES: 0.9286\n",
      "\n",
      "📋 PER-CLASS PERFORMANCE:\n",
      "==================================================\n",
      "Appellate Review               | P: 0.915 | R: 0.973 | F1: 0.943 | Support:  663\n",
      "Juvenile Delinquency Proceeding | P: 0.944 | R: 0.895 | F1: 0.919 | Support:   19\n",
      "Motion for Attorney's Fees     | P: 0.724 | R: 0.647 | F1: 0.683 | Support:   85\n",
      "Motion for Contempt            | P: 0.500 | R: 0.571 | F1: 0.533 | Support:   14\n",
      "Motion for Costs               | P: 0.250 | R: 0.087 | F1: 0.129 | Support:   23\n",
      "Motion for Default Judgment/Order of Default | P: 0.875 | R: 0.333 | F1: 0.483 | Support:   21\n",
      "Motion for Judgment as a Matter of Law (JMOL)/Directed Verdict | P: 0.500 | R: 0.030 | F1: 0.057 | Support:   33\n",
      "Motion for New Trial           | P: 0.750 | R: 0.474 | F1: 0.581 | Support:   38\n",
      "Motion for Permanent Injunction | P: 0.000 | R: 0.000 | F1: 0.000 | Support:   15\n",
      "Motion for Preliminary Injunction | P: 0.811 | R: 0.717 | F1: 0.761 | Support:   60\n",
      "Motion for Protective Order    | P: 0.000 | R: 0.000 | F1: 0.000 | Support:   22\n",
      "Motion for Reconsideration     | P: 1.000 | R: 0.034 | F1: 0.067 | Support:   29\n",
      "Motion to Compel Arbitration   | P: 0.929 | R: 0.812 | F1: 0.867 | Support:   32\n",
      "Motion to Dismiss              | P: 0.622 | R: 0.684 | F1: 0.651 | Support:  269\n",
      "Motion to Dismiss for Lack of Jurisdiction | P: 0.667 | R: 0.100 | F1: 0.174 | Support:   20\n",
      "Motion to Dismiss for Lack of Personal Jurisdiction | P: 0.870 | R: 0.606 | F1: 0.714 | Support:   33\n",
      "Motion to Dismiss for Lack of Standing | P: 0.000 | R: 0.000 | F1: 0.000 | Support:   27\n",
      "Motion to Dismiss for Lack of Subject Matter Jurisdiction | P: 0.667 | R: 0.038 | F1: 0.071 | Support:   53\n",
      "Motion to Set Aside or Vacate  | P: 0.000 | R: 0.000 | F1: 0.000 | Support:   16\n",
      "Motion to Transfer or Change Venue | P: 0.000 | R: 0.000 | F1: 0.000 | Support:   20\n",
      "On Appeal                      | P: 0.955 | R: 0.943 | F1: 0.949 | Support: 1412\n",
      "Petition for Divorce or Dissolution | P: 0.538 | R: 0.318 | F1: 0.400 | Support:   22\n",
      "Petition to Terminate Parental Rights | P: 0.818 | R: 0.720 | F1: 0.766 | Support:   25\n",
      "Post-Trial Hearing Motion      | P: 0.750 | R: 0.045 | F1: 0.085 | Support:   67\n",
      "Review of Administrative Decision | P: 0.897 | R: 0.876 | F1: 0.886 | Support:  427\n",
      "Sentencing or Penalty Phase Motion or Objection | P: 0.671 | R: 0.508 | F1: 0.578 | Support:  185\n",
      "Trial or Guilt Phase Motion or Objection | P: 0.530 | R: 0.543 | F1: 0.537 | Support:  162\n",
      "\n",
      "🎯 OVERALL PERFORMANCE SUMMARY:\n",
      "==================================================\n",
      "🔹 Macro Average    | P: 0.599 | R: 0.406 | F1: 0.438\n",
      "🔹 Weighted Average | P: 0.828 | R: 0.777 | F1: 0.784\n",
      "\n",
      "🏆 FINAL ASSESSMENT:\n",
      "==================================================\n",
      "Micro F1 Score: 0.8166\n",
      "Assessment: 🌟 EXCELLENT! Model shows outstanding performance.\n",
      "\n",
      "💾 Model and results saved to: ./model_output\n",
      "🎉 Multi-label legal posture classification training completed successfully!\n",
      "\n",
      "💾 Saving final model...\n",
      "✅ Final model saved to: ./model_output/final_model\n"
     ]
    }
   ],
   "source": [
    "# 🎉 FINAL MODEL EVALUATION\n",
    "# Comprehensive evaluation of the trained multi-label classification model\n",
    "\n",
    "import torch\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "print(\"🔬 FINAL MODEL EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# First, let's check the test set data types and fix if needed\n",
    "print(\"🔍 Checking test set data types...\")\n",
    "test_sample = tokenized_dataset[\"test\"][0]\n",
    "test_labels = np.array(test_sample['labels'])\n",
    "print(f\"Test labels dtype: {test_labels.dtype}\")\n",
    "\n",
    "if test_labels.dtype != np.float32:\n",
    "    print(\"⚠️ Test set labels need conversion, performing conversion...\")\n",
    "    # Re-apply the label conversion to test set\n",
    "    from datasets import Sequence, Value\n",
    "    label_feature = Sequence(Value(\"float32\"), length=len(class_name))\n",
    "    tokenized_dataset[\"test\"] = tokenized_dataset[\"test\"].cast_column(\"labels\", label_feature)\n",
    "    print(\"✅ Test set labels converted to float32\")\n",
    "\n",
    "# Use predict method instead of evaluate to avoid evaluation issues\n",
    "print(\"📊 Generating predictions on test set...\")\n",
    "predictions = trainer.predict(tokenized_dataset[\"test\"])\n",
    "\n",
    "# Convert predictions to probabilities and binary predictions\n",
    "y_pred_proba = torch.sigmoid(torch.tensor(predictions.predictions)).numpy()\n",
    "y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "y_true = predictions.label_ids.astype(int)\n",
    "\n",
    "print(f\"Prediction shape: {y_pred.shape}\")\n",
    "print(f\"True labels shape: {y_true.shape}\")\n",
    "\n",
    "# Calculate comprehensive metrics manually using our evaluation function\n",
    "# Note: Fix the function call order - comprehensive_evaluation(y_true, y_pred_proba, y_pred_binary)\n",
    "print(\"📊 Calculating comprehensive metrics...\")\n",
    "detailed_metrics = comprehensive_evaluation(y_true, y_pred_proba, y_pred_binary=y_pred)\n",
    "\n",
    "print(f\"\\n🏆 TEST SET RESULTS:\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "# Print all the comprehensive metrics\n",
    "metric_groups = {\n",
    "    \"📈 Primary Metrics\": [\"f1_micro\", \"f1_macro\", \"f1_weighted\", \"f1_samples\"],\n",
    "    \"🎯 Precision\": [\"precision_micro\", \"precision_macro\", \"precision_weighted\", \"precision_samples\"],\n",
    "    \"🔍 Recall\": [\"recall_micro\", \"recall_macro\", \"recall_weighted\", \"recall_samples\"],\n",
    "    \"📊 Other Metrics\": [\"accuracy\", \"hamming_loss\", \"jaccard_samples\", \"jaccard_macro\", \"jaccard_weighted\"],\n",
    "    \"📡 AUC Metrics\": [\"roc_auc_macro\", \"roc_auc_weighted\", \"roc_auc_samples\", \n",
    "                       \"pr_auc_macro\", \"pr_auc_weighted\", \"pr_auc_samples\"]\n",
    "}\n",
    "\n",
    "for group_name, metrics in metric_groups.items():\n",
    "    print(f\"\\n{group_name}:\")\n",
    "    for metric in metrics:\n",
    "        if metric in detailed_metrics:\n",
    "            print(f\"  {metric.upper()}: {detailed_metrics[metric]:.4f}\")\n",
    "\n",
    "# Per-class performance\n",
    "print(f\"\\n📋 PER-CLASS PERFORMANCE:\")\n",
    "print(f\"{'='*50}\")\n",
    "class_report = classification_report(\n",
    "    y_true, y_pred, \n",
    "    target_names=class_name, \n",
    "    output_dict=True,\n",
    "    zero_division=0\n",
    ")\n",
    "\n",
    "# Show performance for each class\n",
    "for i, class_label in enumerate(class_name):\n",
    "    if class_label in class_report:\n",
    "        metrics = class_report[class_label]\n",
    "        support = int(metrics['support'])\n",
    "        print(f\"{class_label:30s} | P: {metrics['precision']:.3f} | R: {metrics['recall']:.3f} | F1: {metrics['f1-score']:.3f} | Support: {support:4d}\")\n",
    "\n",
    "# Overall summary\n",
    "print(f\"\\n🎯 OVERALL PERFORMANCE SUMMARY:\")\n",
    "print(f\"{'='*50}\")\n",
    "macro_avg = class_report['macro avg']\n",
    "weighted_avg = class_report['weighted avg']\n",
    "\n",
    "print(f\"🔹 Macro Average    | P: {macro_avg['precision']:.3f} | R: {macro_avg['recall']:.3f} | F1: {macro_avg['f1-score']:.3f}\")\n",
    "print(f\"🔹 Weighted Average | P: {weighted_avg['precision']:.3f} | R: {weighted_avg['recall']:.3f} | F1: {weighted_avg['f1-score']:.3f}\")\n",
    "\n",
    "# Performance assessment\n",
    "f1_micro = detailed_metrics.get('f1_micro', 0)\n",
    "print(f\"\\n🏆 FINAL ASSESSMENT:\")\n",
    "print(f\"{'='*50}\")\n",
    "if f1_micro > 0.8:\n",
    "    assessment = \"🌟 EXCELLENT! Model shows outstanding performance.\"\n",
    "elif f1_micro > 0.7:\n",
    "    assessment = \"✅ VERY GOOD! Model performance is strong and ready for deployment.\"\n",
    "elif f1_micro > 0.6:\n",
    "    assessment = \"👍 GOOD! Model shows solid performance with room for improvement.\"\n",
    "elif f1_micro > 0.5:\n",
    "    assessment = \"⚠️ MODERATE! Consider additional training or data improvements.\"\n",
    "else:\n",
    "    assessment = \"❌ NEEDS IMPROVEMENT! Significant enhancements required.\"\n",
    "\n",
    "print(f\"Micro F1 Score: {f1_micro:.4f}\")\n",
    "print(f\"Assessment: {assessment}\")\n",
    "\n",
    "print(f\"\\n💾 Model and results saved to: {training_args.output_dir}\")\n",
    "print(f\"🎉 Multi-label legal posture classification training completed successfully!\")\n",
    "\n",
    "# Save the best model explicitly\n",
    "print(f\"\\n💾 Saving final model...\")\n",
    "trainer.save_model(f\"{training_args.output_dir}/final_model\")\n",
    "tokenizer.save_pretrained(f\"{training_args.output_dir}/final_model\")\n",
    "print(f\"✅ Final model saved to: {training_args.output_dir}/final_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
