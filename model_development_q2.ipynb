{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e7a5b02",
   "metadata": {},
   "source": [
    "<span style=\"font-weight: bold; font-size: 18px;\">**Multi-Label Posture Classification: Model Development Strategy**<br><br>\n",
    "\n",
    "We propose a comparative evaluation of two complementary modeling approaches to address the multi-label posture prediction task, each offering distinct advantages for legal document classification.\n",
    "\n",
    "**Baseline Approach: Bag-of-Words Models**<br>\n",
    "\n",
    "Our initial baseline leverages traditional bag-of-words representations (TF-IDF, BM25) combined with multi-label classifiers, justified by several key factors:\n",
    "\n",
    "<div style=\"margin-left: 20px;\"><b>‚Ä¢ Computational Efficiency:</b> Lightweight architecture enables rapid prototyping and establishes performance baselines without GPU requirements</div>\n",
    "<div style=\"margin-left: 20px;\"><b>‚Ä¢ Statistical Robustness:</b> Word-frequency features provide interpretable, domain-agnostic representations suitable for legal terminology analysis</div>\n",
    "<div style=\"margin-left: 20px;\"><b>‚Ä¢ Multi-Label Compatibility:</b> Well-established integration with multi-label algorithms (One-vs-Rest, Binary Relevance, Label Powerset)</div>\n",
    "<div style=\"margin-left: 20px;\"><b>‚Ä¢ Baseline Establishment:</b> Provides interpretable performance benchmarks for evaluating more complex architectures</div>\n",
    "\n",
    "**Advanced Approach: Transformer-Based Models (ModernBERT)**<br>\n",
    "\n",
    "Our primary model leverages ModernBERT encoder architecture, specifically designed to address the limitations of traditional BERT for our use case:\n",
    "\n",
    "<div style=\"margin-left: 20px;\"><b>‚Ä¢ Extended Context Coverage:</b> ModernBERT's 8,192-token context window accommodates ~90% of our corpus without truncation, preserving critical legal context that may span entire documents</div>\n",
    "\n",
    "<div style=\"margin-left: 20px;\"><b>‚Ä¢ Contextual Understanding:</b> Unlike bag-of-words approaches, transformer architectures capture:\n",
    "  <div style=\"margin-left: 40px;\">- Long-range dependencies between legal arguments</div>\n",
    "  <div style=\"margin-left: 40px;\">- Positional relationships between procedural elements</div>\n",
    "  <div style=\"margin-left: 40px;\">- Semantic nuances distinguishing similar posture categories</div>\n",
    "</div>\n",
    "\n",
    "<div style=\"margin-left: 20px;\"><b>‚Ä¢ Multi-Label Architecture:</b> The encoder's [CLS] token representation can be effectively coupled with multi-label classification heads, enabling simultaneous prediction of multiple postures</div>\n",
    "\n",
    "<div style=\"margin-left: 20px;\"><b>‚Ä¢ Legal Domain Adaptation:</b> Pre-trained language understanding provides superior handling of complex legal terminology and document structure</div>\n",
    "\n",
    "**Comparative Justification:**<br>\n",
    "\n",
    "This dual-approach strategy enables comprehensive evaluation of feature representation impact on multi-label performance, ranging from traditional statistical methods to state-of-the-art contextual understanding, ultimately identifying the optimal balance between computational efficiency and classification accuracy for legal posture prediction.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce6340b",
   "metadata": {},
   "source": [
    "## Data Preparation for ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f537d7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56de0440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the labels - convert postures to a list format\n",
    "def prepare_labels(postures_str):\n",
    "    \"\"\"Convert posture string to list of postures\"\"\"\n",
    "    if pd.isna(postures_str) or postures_str == '':\n",
    "        return []\n",
    "    return [p.strip() for p in postures_str.split(',') if p.strip()]\n",
    "\n",
    "# Apply to dataframe\n",
    "_dir=os.path.join(os.getcwd(),\"processed_data\")\n",
    "df=pd.read_pickle(os.path.join(_dir, \"data.pkl\"))\n",
    "df['posture_list'] = df['postures'].apply(prepare_labels)\n",
    "\n",
    "# Remove documents with no postures\n",
    "df_ml = df[df['posture_list'].apply(len) > 0].copy()\n",
    "print(f\"Documents with postures: {len(df_ml)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b115e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze posture distribution\n",
    "all_postures_ml = []\n",
    "for postures in df_ml['posture_list']:\n",
    "    all_postures_ml.extend(postures)\n",
    "\n",
    "posture_counts = pd.Series(all_postures_ml).value_counts()\n",
    "print(f\"\\nTotal unique postures: {len(posture_counts)}\")\n",
    "print()\n",
    "print(f\"Most common postures:\")\n",
    "print(posture_counts.head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11f4a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to most common postures (those appearing in at least 100 documents)\n",
    "min_frequency = 100\n",
    "common_postures = posture_counts[posture_counts >= min_frequency].index.tolist()\n",
    "print(f\"\\nPostures with >= {min_frequency} occurrences: {len(common_postures)}\")\n",
    "print(common_postures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d69416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter documents to only include those with common postures\n",
    "def filter_common_postures(posture_list, common_postures):\n",
    "    \"\"\"Keep only postures that are in the common_postures list\"\"\"\n",
    "    return [p for p in posture_list if p in common_postures]\n",
    "\n",
    "df_ml['filtered_postures'] = df_ml['posture_list'].apply(\n",
    "    lambda x: filter_common_postures(x, common_postures)\n",
    ")\n",
    "\n",
    "# Remove documents that have no common postures after filtering\n",
    "df_ml = df_ml[df_ml['filtered_postures'].apply(len) > 0].copy()\n",
    "print(f\"Documents after filtering to common postures: {len(df_ml)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65e4e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Multi-label Classification Setup\n",
    "\n",
    "# Create binary label matrix using MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "y_multilabel = mlb.fit_transform(df_ml['filtered_postures'])\n",
    "\n",
    "print(f\"Label matrix shape: {y_multilabel.shape}\")\n",
    "print(f\"Labels: {mlb.classes_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a1650d",
   "metadata": {},
   "outputs": [],
   "source": [
    "_counts = df_ml['num_postures'].value_counts(dropna=False)\n",
    "_pct = df_ml['num_postures'].value_counts(dropna=False,normalize=True) \n",
    "\n",
    "pd.DataFrame({\n",
    "    'count': _counts,\n",
    "    'percentage': _pct\n",
    "}).sort_index().style.format({'count':'{:,}','percentage':'{:.2%}'}).set_caption(\"Distribution of num_postures\")\\\n",
    "    .set_table_styles([{'selector': 'caption','props': [('color', 'red'),('font-size', '15px')]}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3bf28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare text data\n",
    "X_text = df_ml['full_text'].values\n",
    "\n",
    "# Split the data\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X_text, y_multilabel, \n",
    "    test_size=0.3, # 30% for temp (which will be split into val and test)\n",
    "    random_state=42, \n",
    "    stratify=None\n",
    ")\n",
    "\n",
    " # Split temp into validation and test (50-50 split of the 30%)\n",
    "# # This gives us 15% each\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp,\n",
    "    test_size=0.5,\n",
    "    random_state=42, \n",
    "    stratify=None\n",
    ")\n",
    "\n",
    "print(f\"Total samples: {len(df_ml)}\")\n",
    "print(f\"Training set: {len(X_train)} ({len(X_train)/len(df_ml):.2%})\")\n",
    "print(f\"Validation set: {len(X_val)} ({len(X_val)/len(df_ml):.2%})\")\n",
    "print(f\"Test set: {len(X_test)} ({len(X_test)/len(df_ml):.2%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e4c020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check label distribution\n",
    "train_label_sums = y_train.sum(axis=0)\n",
    "val_label_sums = y_val.sum(axis=0)\n",
    "test_label_sums = y_test.sum(axis=0)\n",
    "\n",
    "print(\"\\nLabel distribution in training set:\")\n",
    "for i, label in enumerate(mlb.classes_):\n",
    "    print(f\"{label}: {train_label_sums[i]} ({train_label_sums[i]/len(y_train)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5177e708",
   "metadata": {},
   "outputs": [],
   "source": [
    "## save preprocess data\n",
    "saved_data=os.path.join(os.getcwd(), 'processed_data')\n",
    "os.makedirs(saved_data, exist_ok=True)\n",
    "# Save using pickle\n",
    "with open(os.path.join(saved_data,'train_arrays.pkl'), 'wb') as f:\n",
    "    pickle.dump({'X_train': X_train, 'y_train': y_train, 'label_train': label_train}, f)\n",
    "\n",
    "with open(os.path.join(saved_data,'val_arrays.pkl'), 'wb') as f:\n",
    "    pickle.dump({'X_val': X_val, 'y_val': y_val, 'label_val': label_val}, f)\n",
    "\n",
    "with open(os.path.join(saved_data,'test_arrays.pkl'), 'wb') as f:\n",
    "    pickle.dump({'X_test': X_test, 'y_test': y_test, 'label_test': label_test}, f)\n",
    "\n",
    "with open(os.path.join(saved_data,'class_name.pkl'), 'wb') as f:\n",
    "    pickle.dump({'class_name': mlb.classes_}, f)\n",
    "\n",
    "print(\"All arrays saved with pickle!\")\n",
    "\n",
    "# To load later:\n",
    "# with open(os.path.join(saved_data,'train_arrays.pkl'), 'rb') as f:\n",
    "#     train_data = pickle.load(f)\n",
    "#     X_train = train_data['X_train']\n",
    "#     y_train = train_data['y_train']\n",
    "#     label_train = train_data['label_train']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4273a31e",
   "metadata": {},
   "source": [
    "## Bag-of-word (TFIDF): Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8244ef62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from lightgbm import early_stopping, log_evaluation\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, hamming_loss\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, f1_score, \n",
    "    roc_auc_score, average_precision_score,\n",
    "    hamming_loss, jaccard_score\n",
    ")\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef25990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TF-IDF vectorizer\n",
    "# Using parameters optimized for legal text\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=10000,  # Limit features for computational efficiency\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 2),  # Include unigrams and bigrams\n",
    "    min_df=5,           # Ignore terms that appear in fewer than 5 documents\n",
    "    max_df=0.95,        # Ignore terms that appear in more than 95% of documents\n",
    "    sublinear_tf=True   # Apply sublinear scaling\n",
    ")\n",
    "\n",
    "print(\"Fitting TF-IDF vectorizer...\")\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_val_tfidf = tfidf.transform(X_val)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "print(f\"TF-IDF matrix shape (train): {X_train_tfidf.shape}\")\n",
    "print(f\"TF-IDF matrix shape (val): {X_val_tfidf.shape}\")\n",
    "print(f\"TF-IDF matrix shape (test): {X_test_tfidf.shape}\")\n",
    "print(f\"Vocabulary size: {len(tfidf.vocabulary_)}\")\n",
    "\n",
    "# Show some sample features\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "print(f\"\\nSample features: {feature_names[:20]}\")\n",
    "print(f\"Last features: {feature_names[-20:]}\")\n",
    "\n",
    "def comprehensive_evaluation(y_true, y_pred_binary, y_pred_proba, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation function for multi-label classification.\n",
    "    \n",
    "    Args:\n",
    "        y_true: Ground truth binary labels (n_samples, n_labels)\n",
    "        y_pred_binary: Predicted binary labels (n_samples, n_labels) \n",
    "        y_pred_proba: Predicted probabilities (n_samples, n_labels)\n",
    "        threshold: Threshold for converting probabilities to binary (default: 0.5)\n",
    "    \n",
    "    Returns:\n",
    "        dict: Comprehensive metrics including all averaging methods\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    from sklearn.metrics import (\n",
    "        precision_score, recall_score, f1_score, accuracy_score,\n",
    "        hamming_loss, jaccard_score, roc_auc_score, average_precision_score\n",
    "    )\n",
    "    \n",
    "    # Ensure inputs are numpy arrays\n",
    "    y_true = np.array(y_true, dtype=int)\n",
    "    y_pred_binary = np.array(y_pred_binary, dtype=int)\n",
    "    y_pred_proba = np.array(y_pred_proba, dtype=float)\n",
    "    \n",
    "    metrics = {}\n",
    "    \n",
    "    try:\n",
    "        # SAMPLES AVERAGE (per-sample then average across samples)\n",
    "        metrics['precision_samples'] = precision_score(y_true, y_pred_binary, average='samples', zero_division=0)\n",
    "        metrics['recall_samples'] = recall_score(y_true, y_pred_binary, average='samples', zero_division=0)\n",
    "        metrics['f1_samples'] = f1_score(y_true, y_pred_binary, average='samples', zero_division=0)\n",
    "        \n",
    "        # MICRO AVERAGE (global average)\n",
    "        metrics['precision_micro'] = precision_score(y_true, y_pred_binary, average='micro', zero_division=0)\n",
    "        metrics['recall_micro'] = recall_score(y_true, y_pred_binary, average='micro', zero_division=0)\n",
    "        metrics['f1_micro'] = f1_score(y_true, y_pred_binary, average='micro', zero_division=0)\n",
    "        \n",
    "        # MACRO AVERAGE (unweighted average across labels)\n",
    "        metrics['precision_macro'] = precision_score(y_true, y_pred_binary, average='macro', zero_division=0)\n",
    "        metrics['recall_macro'] = recall_score(y_true, y_pred_binary, average='macro', zero_division=0)\n",
    "        metrics['f1_macro'] = f1_score(y_true, y_pred_binary, average='macro', zero_division=0)\n",
    "        \n",
    "        # WEIGHTED AVERAGE (weighted by support)\n",
    "        metrics['precision_weighted'] = precision_score(y_true, y_pred_binary, average='weighted', zero_division=0)\n",
    "        metrics['recall_weighted'] = recall_score(y_true, y_pred_binary, average='weighted', zero_division=0)\n",
    "        metrics['f1_weighted'] = f1_score(y_true, y_pred_binary, average='weighted', zero_division=0)\n",
    "        \n",
    "        # ACCURACY METRICS\n",
    "        metrics['accuracy'] = accuracy_score(y_true, y_pred_binary)\n",
    "        metrics['hamming_loss'] = hamming_loss(y_true, y_pred_binary)\n",
    "        \n",
    "        # JACCARD (IoU) METRICS \n",
    "        metrics['jaccard_samples'] = jaccard_score(y_true, y_pred_binary, average='samples', zero_division=0)\n",
    "        metrics['jaccard_macro'] = jaccard_score(y_true, y_pred_binary, average='macro', zero_division=0)\n",
    "        metrics['jaccard_weighted'] = jaccard_score(y_true, y_pred_binary, average='weighted', zero_division=0)\n",
    "        \n",
    "        # ROC-AUC METRICS (using probabilities)\n",
    "        try:\n",
    "            metrics['roc_auc_micro'] = roc_auc_score(y_true, y_pred_proba, average='micro')\n",
    "            metrics['roc_auc_macro'] = roc_auc_score(y_true, y_pred_proba, average='macro')\n",
    "            metrics['roc_auc_weighted'] = roc_auc_score(y_true, y_pred_proba, average='weighted')\n",
    "            metrics['roc_auc_samples'] = roc_auc_score(y_true, y_pred_proba, average='samples')\n",
    "        except ValueError as e:\n",
    "            print(f\"Warning: ROC-AUC calculation failed: {e}\")\n",
    "            metrics['roc_auc_micro'] = 0.0\n",
    "            metrics['roc_auc_macro'] = 0.0\n",
    "            metrics['roc_auc_weighted'] = 0.0\n",
    "            metrics['roc_auc_samples'] = 0.0\n",
    "        \n",
    "        # PR-AUC METRICS (using probabilities)\n",
    "        try:\n",
    "            metrics['pr_auc_micro'] = average_precision_score(y_true, y_pred_proba, average='micro')\n",
    "            metrics['pr_auc_macro'] = average_precision_score(y_true, y_pred_proba, average='macro')\n",
    "            metrics['pr_auc_weighted'] = average_precision_score(y_true, y_pred_proba, average='weighted')\n",
    "            metrics['pr_auc_samples'] = average_precision_score(y_true, y_pred_proba, average='samples')\n",
    "        except ValueError as e:\n",
    "            print(f\"Warning: PR-AUC calculation failed: {e}\")\n",
    "            metrics['pr_auc_micro'] = 0.0\n",
    "            metrics['pr_auc_macro'] = 0.0\n",
    "            metrics['pr_auc_weighted'] = 0.0\n",
    "            metrics['pr_auc_samples'] = 0.0\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in comprehensive_evaluation: {e}\")\n",
    "        # Return minimal metrics if calculation fails\n",
    "        metrics = {\n",
    "            'precision_micro': 0.0, 'recall_micro': 0.0, 'f1_micro': 0.0,\n",
    "            'precision_macro': 0.0, 'recall_macro': 0.0, 'f1_macro': 0.0,\n",
    "            'accuracy': 0.0, 'hamming_loss': 1.0\n",
    "        }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "print(\"‚úÖ Comprehensive evaluation function updated and ready to use!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f09a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_evaluation(y_true, y_pred_proba, y_pred_binary=None, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation for multi-label classification with all averaging methods\n",
    "    \"\"\"\n",
    "    if y_pred_binary is None:\n",
    "        y_pred_binary = (y_pred_proba >= threshold).astype(int)\n",
    "    \n",
    "    metrics = {}\n",
    "    \n",
    "    # SAMPLES AVERAGE (per-sample then average across samples)\n",
    "    metrics['precision_samples'] = precision_score(y_true, y_pred_binary, average='samples', zero_division=0)\n",
    "    metrics['recall_samples'] = recall_score(y_true, y_pred_binary, average='samples', zero_division=0)\n",
    "    metrics['f1_samples'] = f1_score(y_true, y_pred_binary, average='samples', zero_division=0)\n",
    "    \n",
    "    # MICRO AVERAGE (global aggregation)\n",
    "    metrics['precision_micro'] = precision_score(y_true, y_pred_binary, average='micro', zero_division=0)\n",
    "    metrics['recall_micro'] = recall_score(y_true, y_pred_binary, average='micro', zero_division=0)\n",
    "    metrics['f1_micro'] = f1_score(y_true, y_pred_binary, average='micro', zero_division=0)\n",
    "    \n",
    "    # MACRO AVERAGE (unweighted average across labels)\n",
    "    metrics['precision_macro'] = precision_score(y_true, y_pred_binary, average='macro', zero_division=0)\n",
    "    metrics['recall_macro'] = recall_score(y_true, y_pred_binary, average='macro', zero_division=0)\n",
    "    metrics['f1_macro'] = f1_score(y_true, y_pred_binary, average='macro', zero_division=0)\n",
    "    \n",
    "    # WEIGHTED AVERAGE (weighted by support/frequency)\n",
    "    metrics['precision_weighted'] = precision_score(y_true, y_pred_binary, average='weighted', zero_division=0)\n",
    "    metrics['recall_weighted'] = recall_score(y_true, y_pred_binary, average='weighted', zero_division=0)\n",
    "    metrics['f1_weighted'] = f1_score(y_true, y_pred_binary, average='weighted', zero_division=0)\n",
    "    \n",
    "    # ROC-AUC (multiple averaging methods)\n",
    "    try:\n",
    "        metrics['roc_auc_macro'] = roc_auc_score(y_true, y_pred_proba, average='macro')\n",
    "        metrics['roc_auc_weighted'] = roc_auc_score(y_true, y_pred_proba, average='weighted')\n",
    "        metrics['roc_auc_samples'] = roc_auc_score(y_true, y_pred_proba, average='samples')\n",
    "    except ValueError as e:\n",
    "        print(f\"ROC-AUC calculation failed: {e}\")\n",
    "        metrics['roc_auc_macro'] = 0.0\n",
    "        metrics['roc_auc_weighted'] = 0.0\n",
    "        metrics['roc_auc_samples'] = 0.0\n",
    "    \n",
    "    # Precision-Recall AUC (multiple averaging methods)\n",
    "    try:\n",
    "        metrics['pr_auc_macro'] = average_precision_score(y_true, y_pred_proba, average='macro')\n",
    "        metrics['pr_auc_weighted'] = average_precision_score(y_true, y_pred_proba, average='weighted')\n",
    "        metrics['pr_auc_samples'] = average_precision_score(y_true, y_pred_proba, average='samples')\n",
    "    except ValueError as e:\n",
    "        print(f\"PR-AUC calculation failed: {e}\")\n",
    "        metrics['pr_auc_macro'] = 0.0\n",
    "        metrics['pr_auc_weighted'] = 0.0\n",
    "        metrics['pr_auc_samples'] = 0.0\n",
    "    \n",
    "    # Hamming Loss (inherently micro-averaged)\n",
    "    metrics['hamming_loss'] = hamming_loss(y_true, y_pred_binary)\n",
    "    \n",
    "    # Jaccard Score (multiple averaging methods)\n",
    "    metrics['jaccard_samples'] = jaccard_score(y_true, y_pred_binary, average='samples', zero_division=0)\n",
    "    metrics['jaccard_macro'] = jaccard_score(y_true, y_pred_binary, average='macro', zero_division=0)\n",
    "    metrics['jaccard_weighted'] = jaccard_score(y_true, y_pred_binary, average='weighted', zero_division=0)\n",
    "    \n",
    "    # Note: micro average for Jaccard in multi-label is not directly supported in sklearn\n",
    "    # but can be calculated manually if needed\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c020c01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define models to test with optimized hyperparameters and validation-aware training\n",
    "# models = {\n",
    "#     'Logistic Regression': OneVsRestClassifier(\n",
    "#         LogisticRegression(\n",
    "#             random_state=42, \n",
    "#             max_iter=1000,\n",
    "#             C=1.0,\n",
    "#             solver='liblinear'\n",
    "#         )\n",
    "#     ),\n",
    "#     'Random Forest': OneVsRestClassifier(\n",
    "#         RandomForestClassifier(\n",
    "#             n_estimators=100, \n",
    "#             random_state=42, \n",
    "#             n_jobs=-1,\n",
    "#             max_depth=10,\n",
    "#             min_samples_split=5,\n",
    "#             min_samples_leaf=2,\n",
    "#             # Additional overfitting control\n",
    "#             min_impurity_decrease=0.0001,\n",
    "#             max_features='sqrt'\n",
    "#         )\n",
    "#     ),\n",
    "#     'XGBoost': OneVsRestClassifier(\n",
    "#         xgb.XGBClassifier(\n",
    "#             random_state=42,\n",
    "#             n_estimators=100,\n",
    "#             max_depth=6,\n",
    "#             learning_rate=0.1,\n",
    "#             subsample=0.8,\n",
    "#             colsample_bytree=0.8,\n",
    "#             eval_metric='logloss',\n",
    "#             verbosity=0,\n",
    "#             # Early stopping will be handled in training loop\n",
    "#             early_stopping_rounds=10\n",
    "#         )\n",
    "#     ),\n",
    "#     'LightGBM': OneVsRestClassifier(\n",
    "#         lgb.LGBMClassifier(\n",
    "#             random_state=42,\n",
    "#             n_estimators=100,\n",
    "#             max_depth=6,\n",
    "#             learning_rate=0.1,\n",
    "#             subsample=0.8,\n",
    "#             colsample_bytree=0.8,\n",
    "#             verbosity=-1,\n",
    "#             # Early stopping will be handled in training loop\n",
    "#             early_stopping_rounds=10\n",
    "#         )\n",
    "#     )\n",
    "# }\n",
    "\n",
    "# # Enhanced training function with validation monitoring\n",
    "# def train_with_validation_control(model, X_train, y_train, X_val, y_val, model_name):\n",
    "#     \"\"\"\n",
    "#     Train model with validation monitoring to control overfitting\n",
    "#     \"\"\"\n",
    "#     print(f\"\\nTraining {model_name} with validation control...\")\n",
    "    \n",
    "#     if model_name in ['XGBoost', 'LightGBM']:\n",
    "#         # For tree-based models, we can use early stopping\n",
    "#         if model_name == 'XGBoost':\n",
    "#             # XGBoost with early stopping\n",
    "#             for i, estimator in enumerate(model.estimators_):\n",
    "#                 print(f\"  Training label {i+1}/{len(model.estimators_)}\")\n",
    "                \n",
    "#                 # Get single label\n",
    "#                 y_train_single = y_train[:, i]\n",
    "#                 y_val_single = y_val[:, i]\n",
    "                \n",
    "#                 # Only train if there are positive samples\n",
    "#                 if y_train_single.sum() > 0:\n",
    "#                     estimator.fit(\n",
    "#                         X_train, y_train_single,\n",
    "#                         eval_set=[(X_val, y_val_single)],\n",
    "#                         verbose=False\n",
    "#                     )\n",
    "#                 else:\n",
    "#                     # For labels with no positive samples, create a dummy classifier\n",
    "#                     estimator.fit(X_train[:10], y_train_single[:10])\n",
    "        \n",
    "#         elif model_name == 'LightGBM':\n",
    "#             # LightGBM with early stopping\n",
    "#             for i, estimator in enumerate(model.estimators_):\n",
    "#                 print(f\"  Training label {i+1}/{len(model.estimators_)}\")\n",
    "                \n",
    "#                 # Get single label\n",
    "#                 y_train_single = y_train[:, i]\n",
    "#                 y_val_single = y_val[:, i]\n",
    "                \n",
    "#                 # Only train if there are positive samples\n",
    "#                 if y_train_single.sum() > 0:\n",
    "#                     estimator.fit(\n",
    "#                         X_train, y_train_single,\n",
    "#                         eval_set=[(X_val, y_val_single)],\n",
    "#                         callbacks=[\n",
    "#                             early_stopping(10, verbose=False),\n",
    "#                             log_evaluation(0)  # No logging\n",
    "#                         ]\n",
    "#                     )\n",
    "#                 else:\n",
    "#                     # For labels with no positive samples, create a dummy classifier\n",
    "#                     estimator.fit(X_train[:10], y_train_single[:10])\n",
    "#     else:\n",
    "#         # For other models, use regular training\n",
    "#         model.fit(X_train, y_train)\n",
    "    \n",
    "#     return model\n",
    "\n",
    "# # Store results with validation tracking\n",
    "# results = {}\n",
    "# validation_scores = {}\n",
    "\n",
    "# print(\"Training and evaluating models with validation control...\")\n",
    "# print(\"=\"*60)\n",
    "# print(\"Models to evaluate:\")\n",
    "# for name in models.keys():\n",
    "#     print(f\"  ‚Ä¢ {name}\")\n",
    "# print()\n",
    "\n",
    "# for name, model in models.items():\n",
    "#     # Train with validation control\n",
    "#     if name in ['XGBoost', 'LightGBM']:\n",
    "#         # For tree-based models, we need to handle OneVsRestClassifier manually\n",
    "#         # to implement early stopping properly\n",
    "#         trained_model = OneVsRestClassifier(\n",
    "#             model.estimator,\n",
    "#             n_jobs=1  # Sequential to handle early stopping\n",
    "#         )\n",
    "#         trained_model.fit(X_train_tfidf, y_train)\n",
    "#     else:\n",
    "#         trained_model = model\n",
    "#         trained_model.fit(X_train_tfidf, y_train)\n",
    "    \n",
    "#     # Make predictions on all sets\n",
    "#     y_pred_train = trained_model.predict(X_train_tfidf)\n",
    "#     y_pred_val = trained_model.predict(X_val_tfidf)\n",
    "#     y_pred_test = trained_model.predict(X_test_tfidf)\n",
    "    \n",
    "#     # Calculate metrics for all sets\n",
    "#     train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "#     val_accuracy = accuracy_score(y_val, y_pred_val)\n",
    "#     test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "    \n",
    "#     train_hamming = hamming_loss(y_train, y_pred_train)\n",
    "#     val_hamming = hamming_loss(y_val, y_pred_val)\n",
    "#     test_hamming = hamming_loss(y_test, y_pred_test)\n",
    "    \n",
    "#     # Calculate F1 scores\n",
    "#     train_f1_micro = f1_score(y_train, y_pred_train, average='micro')\n",
    "#     val_f1_micro = f1_score(y_val, y_pred_val, average='micro')\n",
    "#     test_f1_micro = f1_score(y_test, y_pred_test, average='micro')\n",
    "    \n",
    "#     # Store results\n",
    "#     results[name] = {\n",
    "#         'model': trained_model,\n",
    "#         'train_accuracy': train_accuracy,\n",
    "#         'val_accuracy': val_accuracy,\n",
    "#         'test_accuracy': test_accuracy,\n",
    "#         'train_hamming_loss': train_hamming,\n",
    "#         'val_hamming_loss': val_hamming,\n",
    "#         'test_hamming_loss': test_hamming,\n",
    "#         'train_f1_micro': train_f1_micro,\n",
    "#         'val_f1_micro': val_f1_micro,\n",
    "#         'test_f1_micro': test_f1_micro,\n",
    "#         'y_pred_test': y_pred_test,\n",
    "#         'y_pred_val': y_pred_val\n",
    "#     }\n",
    "    \n",
    "#     # Check for overfitting\n",
    "#     accuracy_gap = train_accuracy - val_accuracy\n",
    "#     f1_gap = train_f1_micro - val_f1_micro\n",
    "    \n",
    "#     overfitting_status = \"‚úÖ Good\" if accuracy_gap < 0.05 else \"‚ö†Ô∏è Moderate\" if accuracy_gap < 0.1 else \"üö® High\"\n",
    "    \n",
    "#     print(f\"\\n{name} Results:\")\n",
    "#     print(f\"  Train Accuracy: {train_accuracy:.4f}\")\n",
    "#     print(f\"  Val Accuracy:   {val_accuracy:.4f}\")\n",
    "#     print(f\"  Test Accuracy:  {test_accuracy:.4f}\")\n",
    "#     print(f\"  Train-Val Gap:  {accuracy_gap:.4f} ({overfitting_status})\")\n",
    "#     print(f\"  Train F1:       {train_f1_micro:.4f}\")\n",
    "#     print(f\"  Val F1:         {val_f1_micro:.4f}\")\n",
    "#     print(f\"  Test F1:        {test_f1_micro:.4f}\")\n",
    "#     print(f\"  F1 Gap:         {f1_gap:.4f}\")\n",
    "\n",
    "# print(\"\\n\" + \"=\"*80)\n",
    "# print(\"Model Comparison with Overfitting Analysis:\")\n",
    "# print(f\"{'Model':<15} | {'Test Acc':<8} | {'Val Acc':<8} | {'Gap':<6} | {'Status':<12} | {'Performance':<12}\")\n",
    "# print(\"-\" * 85)\n",
    "\n",
    "# # Sort results by validation accuracy (better indicator than test accuracy)\n",
    "# sorted_results = sorted(results.items(), key=lambda x: x[1]['val_accuracy'], reverse=True)\n",
    "\n",
    "# for name, result in sorted_results:\n",
    "#     gap = result['train_accuracy'] - result['val_accuracy']\n",
    "#     status = \"Good\" if gap < 0.05 else \"Moderate\" if gap < 0.1 else \"High\"\n",
    "#     performance = \"ü•á Best\" if name == sorted_results[0][0] else \"ü•à Good\" if result['val_accuracy'] > 0.55 else \"‚ö†Ô∏è Poor\"\n",
    "#     print(f\"{name:<15} | {result['test_accuracy']:<8.4f} | {result['val_accuracy']:<8.4f} | {gap:<6.4f} | {status:<12} | {performance}\")\n",
    "\n",
    "# # Identify best model based on validation performance\n",
    "# best_model_name = sorted_results[0][0]\n",
    "# best_model = sorted_results[0][1]['model']\n",
    "# print(f\"\\nüèÜ Best performing model (based on validation): {best_model_name}\")\n",
    "# print(f\"   Validation Accuracy: {sorted_results[0][1]['val_accuracy']:.4f}\")\n",
    "# print(f\"   Test Accuracy: {sorted_results[0][1]['test_accuracy']:.4f}\")\n",
    "# print(f\"   Overfitting Gap: {sorted_results[0][1]['train_accuracy'] - sorted_results[0][1]['val_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3da6407",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, f1_score, \n",
    "    roc_auc_score, average_precision_score,\n",
    "    hamming_loss, jaccard_score, accuracy_score\n",
    ")\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from tqdm import tqdm\n",
    "\n",
    "class Train_XGBoost(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"XGBoost classifier with validation-based early stopping for multi-label\"\"\"\n",
    "    \n",
    "    def __init__(self, **xgb_params):\n",
    "        self.xgb_params = xgb_params\n",
    "        self.models_ = []\n",
    "        self.n_classes_ = None\n",
    "        \n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        if len(y.shape) == 1:\n",
    "            y = y.reshape(-1, 1)\n",
    "        if X_val is not None and len(y_val.shape) == 1:\n",
    "            y_val = y_val.reshape(-1, 1)\n",
    "            \n",
    "        self.n_classes_ = y.shape[1]\n",
    "        self.models_ = []\n",
    "        \n",
    "        for i in tqdm(range(self.n_classes_), total=self.n_classes_, leave=True, position=0):\n",
    "            \n",
    "            y_single = y[:, i]\n",
    "            \n",
    "            # Skip if no positive samples\n",
    "            if y_single.sum() == 0:\n",
    "                self.models_.append(None)\n",
    "                continue\n",
    "            \n",
    "            model = xgb.XGBClassifier(**self.xgb_params)\n",
    "            \n",
    "            if X_val is not None and y_val is not None:\n",
    "                y_val_single = y_val[:, i]\n",
    "                model.fit(\n",
    "                    X, y_single,\n",
    "                    eval_set=[(X_val, y_val_single)],\n",
    "                    verbose=False\n",
    "                )\n",
    "            else:\n",
    "                model.fit(X, y_single)\n",
    "            \n",
    "            self.models_.append(model)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predictions = np.zeros((X.shape[0], self.n_classes_))\n",
    "        \n",
    "        for i, model in enumerate(self.models_):\n",
    "            if model is not None:\n",
    "                predictions[:, i] = model.predict(X)\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        probabilities = np.zeros((X.shape[0], self.n_classes_))\n",
    "        \n",
    "        for i, model in enumerate(self.models_):\n",
    "            if model is not None:\n",
    "                proba = model.predict_proba(X)\n",
    "                # Handle case where only one class is present\n",
    "                if proba.shape[1] == 1:\n",
    "                    probabilities[:, i] = 0  # All negative class\n",
    "                else:\n",
    "                    probabilities[:, i] = proba[:, 1]  # Positive class probability\n",
    "        \n",
    "        return probabilities\n",
    "\n",
    "class Train_LGBM(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"LightGBM classifier with validation-based early stopping for multi-label\"\"\"\n",
    "    \n",
    "    def __init__(self, **lgb_params):\n",
    "        self.lgb_params = lgb_params\n",
    "        self.models_ = []\n",
    "        self.n_classes_ = None\n",
    "        \n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        if len(y.shape) == 1:\n",
    "            y = y.reshape(-1, 1)\n",
    "        if X_val is not None and len(y_val.shape) == 1:\n",
    "            y_val = y_val.reshape(-1, 1)\n",
    "            \n",
    "        self.n_classes_ = y.shape[1]\n",
    "        self.models_ = []\n",
    "        \n",
    "        for i in tqdm(range(self.n_classes_), total=self.n_classes_, leave=True, position=0):\n",
    "            \n",
    "            y_single = y[:, i]\n",
    "            \n",
    "            # Skip if no positive samples\n",
    "            if y_single.sum() == 0:\n",
    "                self.models_.append(None)\n",
    "                continue\n",
    "            \n",
    "            model = lgb.LGBMClassifier(**self.lgb_params)\n",
    "            \n",
    "            if X_val is not None and y_val is not None:\n",
    "                y_val_single = y_val[:, i]\n",
    "                model.fit(\n",
    "                    X, y_single,\n",
    "                    eval_set=[(X_val, y_val_single)],\n",
    "                    callbacks=[\n",
    "                        lgb.early_stopping(10, verbose=False),\n",
    "                        lgb.log_evaluation(0)\n",
    "                    ]\n",
    "                )\n",
    "            else:\n",
    "                model.fit(X, y_single)\n",
    "            \n",
    "            self.models_.append(model)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predictions = np.zeros((X.shape[0], self.n_classes_))\n",
    "        \n",
    "        for i, model in enumerate(self.models_):\n",
    "            if model is not None:\n",
    "                predictions[:, i] = model.predict(X)\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        probabilities = np.zeros((X.shape[0], self.n_classes_))\n",
    "        \n",
    "        for i, model in enumerate(self.models_):\n",
    "            if model is not None:\n",
    "                proba = model.predict_proba(X)\n",
    "                # Handle case where only one class is present\n",
    "                if proba.shape[1] == 1:\n",
    "                    probabilities[:, i] = 0  # All negative class\n",
    "                else:\n",
    "                    probabilities[:, i] = proba[:, 1]  # Positive class probability\n",
    "        \n",
    "        return probabilities\n",
    "\n",
    "class Train_logistic(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"Logistic Regression classifier with validation monitoring for multi-label\"\"\"\n",
    "    \n",
    "    def __init__(self, **lr_params):\n",
    "        self.lr_params = lr_params\n",
    "        self.models_ = []\n",
    "        self.n_classes_ = None\n",
    "        self.validation_scores_ = []\n",
    "        \n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        if len(y.shape) == 1:\n",
    "            y = y.reshape(-1, 1)\n",
    "        if X_val is not None and len(y_val.shape) == 1:\n",
    "            y_val = y_val.reshape(-1, 1)\n",
    "            \n",
    "        self.n_classes_ = y.shape[1]\n",
    "        self.models_ = []\n",
    "        self.validation_scores_ = []\n",
    "        \n",
    "        for i in tqdm(range(self.n_classes_), total=self.n_classes_, leave=True, position=0):\n",
    "            \n",
    "            y_single = y[:, i]\n",
    "            \n",
    "            # Skip if no positive samples\n",
    "            if y_single.sum() == 0:\n",
    "                self.models_.append(None)\n",
    "                self.validation_scores_.append(0.0)\n",
    "                continue\n",
    "            \n",
    "            model = LogisticRegression(**self.lr_params)\n",
    "            model.fit(X, y_single)\n",
    "            \n",
    "            # Calculate validation score if validation data provided\n",
    "            if X_val is not None and y_val is not None:\n",
    "                y_val_single = y_val[:, i]\n",
    "                val_score = model.score(X_val, y_val_single)\n",
    "                self.validation_scores_.append(val_score)\n",
    "            else:\n",
    "                self.validation_scores_.append(None)\n",
    "            \n",
    "            self.models_.append(model)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predictions = np.zeros((X.shape[0], self.n_classes_))\n",
    "        \n",
    "        for i, model in enumerate(self.models_):\n",
    "            if model is not None:\n",
    "                predictions[:, i] = model.predict(X)\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        probabilities = np.zeros((X.shape[0], self.n_classes_))\n",
    "        \n",
    "        for i, model in enumerate(self.models_):\n",
    "            if model is not None:\n",
    "                proba = model.predict_proba(X)\n",
    "                # Handle case where only one class is present\n",
    "                if proba.shape[1] == 1:\n",
    "                    probabilities[:, i] = 0  # All negative class\n",
    "                else:\n",
    "                    probabilities[:, i] = proba[:, 1]  # Positive class probability\n",
    "        \n",
    "        return probabilities\n",
    "    \n",
    "    def get_validation_scores(self):\n",
    "        \"\"\"Return validation scores for each label\"\"\"\n",
    "        return self.validation_scores_\n",
    "\n",
    "class Train_RandomForest(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"Random Forest classifier with validation monitoring for multi-label\"\"\"\n",
    "    \n",
    "    def __init__(self, **rf_params):\n",
    "        self.rf_params = rf_params\n",
    "        self.models_ = []\n",
    "        self.n_classes_ = None\n",
    "        self.validation_scores_ = []\n",
    "        self.feature_importances_ = []\n",
    "        \n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        if len(y.shape) == 1:\n",
    "            y = y.reshape(-1, 1)\n",
    "        if X_val is not None and len(y_val.shape) == 1:\n",
    "            y_val = y_val.reshape(-1, 1)\n",
    "            \n",
    "        self.n_classes_ = y.shape[1]\n",
    "        self.models_ = []\n",
    "        self.validation_scores_ = []\n",
    "        self.feature_importances_ = []\n",
    "        \n",
    "        for i in tqdm(range(self.n_classes_), total=self.n_classes_, leave=True, position=0):\n",
    "            \n",
    "            y_single = y[:, i]\n",
    "            \n",
    "            # Skip if no positive samples\n",
    "            if y_single.sum() == 0:\n",
    "                self.models_.append(None)\n",
    "                self.validation_scores_.append(0.0)\n",
    "                self.feature_importances_.append(None)\n",
    "                continue\n",
    "            \n",
    "            model = RandomForestClassifier(**self.rf_params)\n",
    "            model.fit(X, y_single)\n",
    "            \n",
    "            # Store feature importances\n",
    "            self.feature_importances_.append(model.feature_importances_)\n",
    "            \n",
    "            # Calculate validation score if validation data provided\n",
    "            if X_val is not None and y_val is not None:\n",
    "                y_val_single = y_val[:, i]\n",
    "                val_score = model.score(X_val, y_val_single)\n",
    "                self.validation_scores_.append(val_score)\n",
    "            else:\n",
    "                self.validation_scores_.append(None)\n",
    "            \n",
    "            self.models_.append(model)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predictions = np.zeros((X.shape[0], self.n_classes_))\n",
    "        \n",
    "        for i, model in enumerate(self.models_):\n",
    "            if model is not None:\n",
    "                predictions[:, i] = model.predict(X)\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        probabilities = np.zeros((X.shape[0], self.n_classes_))\n",
    "        \n",
    "        for i, model in enumerate(self.models_):\n",
    "            if model is not None:\n",
    "                proba = model.predict_proba(X)\n",
    "                # Handle case where only one class is present\n",
    "                if proba.shape[1] == 1:\n",
    "                    probabilities[:, i] = 0  # All negative class\n",
    "                else:\n",
    "                    probabilities[:, i] = proba[:, 1]  # Positive class probability\n",
    "        \n",
    "        return probabilities\n",
    "    \n",
    "    def get_validation_scores(self):\n",
    "        \"\"\"Return validation scores for each label\"\"\"\n",
    "        return self.validation_scores_\n",
    "    \n",
    "    def get_feature_importances(self):\n",
    "        \"\"\"Return feature importances for each label\"\"\"\n",
    "        return self.feature_importances_\n",
    "\n",
    "def training_function_with_validation(X_train, y_train, X_val, y_val, model_type='lightgbm'):\n",
    "    \"\"\"\n",
    "    Enhanced training function with proper validation control for multi-label classification\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Training {model_type} with validation control...\")\n",
    "    print(f\"X_train shape: {X_train.shape}\")\n",
    "    print(f\"y_train shape: {y_train.shape}\")\n",
    "    print(f\"X_val shape: {X_val.shape}\")\n",
    "    print(f\"y_val shape: {y_val.shape}\")\n",
    "    \n",
    "    if model_type == 'lightgbm':\n",
    "        model = Train_LGBM(\n",
    "            random_state=42,\n",
    "            n_estimators=200,  # More estimators for early stopping\n",
    "            max_depth=6,\n",
    "            learning_rate=0.1,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            verbosity=-1,\n",
    "            early_stopping_rounds=10\n",
    "        )\n",
    "    elif model_type == 'xgboost':\n",
    "        model = Train_XGBoost(\n",
    "            random_state=42,\n",
    "            n_estimators=200,  # More estimators for early stopping\n",
    "            max_depth=6,\n",
    "            learning_rate=0.1,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            eval_metric='logloss',\n",
    "            verbosity=0,\n",
    "            early_stopping_rounds=10\n",
    "        )\n",
    "    elif model_type == 'logistic':\n",
    "        model = Train_logistic(\n",
    "            random_state=42,\n",
    "            max_iter=1000,\n",
    "            C=1.0,\n",
    "            solver='liblinear',\n",
    "            class_weight='balanced'  # Handle class imbalance\n",
    "        )\n",
    "    elif model_type == 'randomforest':\n",
    "        model = Train_RandomForest(\n",
    "            random_state=42,\n",
    "            n_estimators=100,\n",
    "            max_depth=10,\n",
    "            min_samples_split=5,\n",
    "            min_samples_leaf=2,\n",
    "            max_features='sqrt',\n",
    "            class_weight='balanced',  # Handle class imbalance\n",
    "            n_jobs=-1\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"Supported model types: 'lightgbm', 'xgboost', 'logistic', 'randomforest'\")\n",
    "    \n",
    "    # Fit with validation data\n",
    "    model.fit(X_train, y_train, X_val, y_val)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_val = model.predict(X_val)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_acc = accuracy_score(y_train, y_pred_train)\n",
    "    val_acc = accuracy_score(y_val, y_pred_val)\n",
    "    train_f1 = f1_score(y_train, y_pred_train, average='micro')\n",
    "    val_f1 = f1_score(y_val, y_pred_val, average='micro')\n",
    "    \n",
    "    # Calculate hamming loss (lower is better)\n",
    "    train_hamming = hamming_loss(y_train, y_pred_train)\n",
    "    val_hamming = hamming_loss(y_val, y_pred_val)\n",
    "    \n",
    "    # Calculate overfitting gaps for different metrics\n",
    "    accuracy_gap = train_acc - val_acc\n",
    "    f1_gap = train_f1 - val_f1\n",
    "    hamming_gap = val_hamming - train_hamming  # Note: val - train because lower hamming is better\n",
    "    \n",
    "    print(f\"Training completed!\")\n",
    "    print(f\"Train Accuracy: {train_acc:.4f}\")\n",
    "    print(f\"Val Accuracy: {val_acc:.4f}\")\n",
    "    print(f\"Train F1: {train_f1:.4f}\")\n",
    "    print(f\"Val F1: {val_f1:.4f}\")\n",
    "    print(f\"Train Hamming Loss: {train_hamming:.4f}\")\n",
    "    print(f\"Val Hamming Loss: {val_hamming:.4f}\")\n",
    "    print(f\"Overfitting Gap (Accuracy): {accuracy_gap:.4f}\")\n",
    "    print(f\"Overfitting Gap (F1): {f1_gap:.4f}\")\n",
    "    print(f\"Overfitting Gap (Hamming): {hamming_gap:.4f}\")\n",
    "    \n",
    "    return model, {\n",
    "        'train_accuracy': train_acc,\n",
    "        'val_accuracy': val_acc,\n",
    "        'train_f1': train_f1,\n",
    "        'val_f1': val_f1,\n",
    "        'train_hamming_loss': train_hamming,\n",
    "        'val_hamming_loss': val_hamming,\n",
    "        'accuracy_gap': accuracy_gap,\n",
    "        'f1_gap': f1_gap,\n",
    "        'hamming_gap': hamming_gap,\n",
    "        'overfitting_gap': hamming_gap  # Use hamming gap as primary overfitting indicator\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9918bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Model Comparison with Validation Control\n",
    "\n",
    "def compare_all_models(X_train, y_train, X_val, y_val, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Train and compare all models with validation control\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"üöÄ COMPREHENSIVE MODEL COMPARISON WITH VALIDATION CONTROL\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    models_to_test = ['logistic', 'randomforest', 'lightgbm', 'xgboost']\n",
    "    results = {}\n",
    "    \n",
    "    for model_type in models_to_test:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"üîß Training {model_type.upper()} Model\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        try:\n",
    "            # Train model with validation\n",
    "            model, metrics = training_function_with_validation(\n",
    "                X_train, y_train, X_val, y_val, model_type=model_type\n",
    "            )\n",
    "            \n",
    "            # Test on unseen data\n",
    "            y_pred_test = model.predict(X_test)\n",
    "            test_acc = accuracy_score(y_test, y_pred_test)\n",
    "            test_f1 = f1_score(y_test, y_pred_test, average='micro')\n",
    "            test_hamming = hamming_loss(y_test, y_pred_test)\n",
    "            \n",
    "            # Store all results\n",
    "            results[model_type] = {\n",
    "                'model': model,\n",
    "                'train_accuracy': metrics['train_accuracy'],\n",
    "                'val_accuracy': metrics['val_accuracy'],\n",
    "                'test_accuracy': test_acc,\n",
    "                'train_f1': metrics['train_f1'],\n",
    "                'val_f1': metrics['val_f1'],\n",
    "                'test_f1': test_f1,\n",
    "                'train_hamming_loss': metrics['train_hamming_loss'],\n",
    "                'val_hamming_loss': metrics['val_hamming_loss'],\n",
    "                'test_hamming_loss': test_hamming,\n",
    "                'accuracy_gap': metrics['accuracy_gap'],\n",
    "                'f1_gap': metrics['f1_gap'],\n",
    "                'hamming_gap': metrics['hamming_gap'],\n",
    "                'overfitting_gap': metrics['overfitting_gap']  # Based on hamming loss\n",
    "            }\n",
    "            \n",
    "            print(f\"‚úÖ {model_type.upper()} completed successfully!\")\n",
    "            print(f\"   Test Accuracy: {test_acc:.4f}\")\n",
    "            print(f\"   Test F1: {test_f1:.4f}\")\n",
    "            print(f\"   Test Hamming Loss: {test_hamming:.4f}\")\n",
    "            print(f\"   Overfitting Gap (Hamming): {metrics['overfitting_gap']:.4f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error training {model_type}: {str(e)}\")\n",
    "            results[model_type] = None\n",
    "    \n",
    "    return results\n",
    "\n",
    "def analyze_model_results(results):\n",
    "    \"\"\"\n",
    "    Analyze and display comprehensive results\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*100}\")\n",
    "    print(\"üìä COMPREHENSIVE MODEL ANALYSIS\")\n",
    "    print(f\"{'='*100}\")\n",
    "    \n",
    "    # Filter successful results\n",
    "    successful_results = {k: v for k, v in results.items() if v is not None}\n",
    "    \n",
    "    if not successful_results:\n",
    "        print(\"‚ùå No models trained successfully!\")\n",
    "        return\n",
    "    \n",
    "    # Display detailed comparison table\n",
    "    print(f\"\\n{'Model':<15} | {'Train Acc':<9} | {'Val Acc':<9} | {'Test Acc':<9} | {'Train Ham':<9} | {'Val Ham':<8} | {'Test Ham':<8} | {'Ham Gap':<8} | {'Status'}\")\n",
    "    print(\"-\" * 105)\n",
    "    \n",
    "    # Sort by validation accuracy (best practice)\n",
    "    sorted_results = sorted(successful_results.items(), \n",
    "                          key=lambda x: x[1]['val_accuracy'], reverse=True)\n",
    "    \n",
    "    for rank, (model_name, result) in enumerate(sorted_results, 1):\n",
    "        hamming_gap = result['hamming_gap']\n",
    "        \n",
    "        # Determine overfitting status based on hamming gap\n",
    "        # For hamming loss, positive gap means validation is worse (overfitting)\n",
    "        if hamming_gap < 0.01:\n",
    "            status = \"‚úÖ Excellent\"\n",
    "        elif hamming_gap < 0.02:\n",
    "            status = \"üü¢ Good\"\n",
    "        elif hamming_gap < 0.04:\n",
    "            status = \"üü° Moderate\"\n",
    "        else:\n",
    "            status = \"üî¥ High\"\n",
    "        \n",
    "        rank_emoji = \"ü•á\" if rank == 1 else \"ü•à\" if rank == 2 else \"ü•â\" if rank == 3 else \"4Ô∏è‚É£\"\n",
    "        \n",
    "        print(f\"{model_name.upper():<15} | {result['train_accuracy']:<9.4f} | {result['val_accuracy']:<9.4f} | \"\n",
    "              f\"{result['test_accuracy']:<9.4f} | {result['train_hamming_loss']:<9.4f} | {result['val_hamming_loss']:<8.4f} | \"\n",
    "              f\"{result['test_hamming_loss']:<8.4f} | {hamming_gap:<8.4f} | {status}\")\n",
    "    \n",
    "    # Identify best models\n",
    "    best_model = sorted_results[0]\n",
    "    print(f\"\\nüèÜ BEST MODEL (Based on Validation Performance): {best_model[0].upper()}\")\n",
    "    print(f\"   üìà Validation Accuracy: {best_model[1]['val_accuracy']:.4f}\")\n",
    "    print(f\"   üéØ Test Accuracy: {best_model[1]['test_accuracy']:.4f}\")\n",
    "    print(f\"   üìä Test F1 Score: {best_model[1]['test_f1']:.4f}\")\n",
    "    print(f\"   üîª Test Hamming Loss: {best_model[1]['test_hamming_loss']:.4f}\")\n",
    "    print(f\"   ‚öñÔ∏è Overfitting Gap (Hamming): {best_model[1]['overfitting_gap']:.4f}\")\n",
    "    print(f\"   üìè Accuracy Gap: {best_model[1]['accuracy_gap']:.4f}\")\n",
    "    print(f\"   üìà F1 Gap: {best_model[1]['f1_gap']:.4f}\")\n",
    "    \n",
    "    # Best test performance (might be different from best validation)\n",
    "    best_test = max(successful_results.items(), key=lambda x: x[1]['test_accuracy'])\n",
    "    if best_test[0] != best_model[0]:\n",
    "        print(f\"\\nüéØ BEST TEST PERFORMANCE: {best_test[0].upper()}\")\n",
    "        print(f\"   Test Accuracy: {best_test[1]['test_accuracy']:.4f}\")\n",
    "        print(f\"   (Note: Choose model based on validation, not test performance)\")\n",
    "    \n",
    "    # Best hamming loss performance\n",
    "    best_hamming = min(successful_results.items(), key=lambda x: x[1]['test_hamming_loss'])\n",
    "    if best_hamming[0] != best_model[0]:\n",
    "        print(f\"\\nüîª BEST HAMMING LOSS PERFORMANCE: {best_hamming[0].upper()}\")\n",
    "        print(f\"   Test Hamming Loss: {best_hamming[1]['test_hamming_loss']:.4f}\")\n",
    "        print(f\"   (Lower hamming loss = better multi-label performance)\")\n",
    "    \n",
    "    # Model-specific insights\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"üîç MODEL-SPECIFIC INSIGHTS:\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    for model_name, result in successful_results.items():\n",
    "        if hasattr(result['model'], 'get_validation_scores'):\n",
    "            val_scores = result['model'].get_validation_scores()\n",
    "            if val_scores and any(score for score in val_scores if score is not None):\n",
    "                valid_scores = [s for s in val_scores if s is not None and s > 0]\n",
    "                if valid_scores:\n",
    "                    avg_label_score = np.mean(valid_scores)\n",
    "                    print(f\"{model_name.upper()}:\")\n",
    "                    print(f\"   Average per-label validation score: {avg_label_score:.4f}\")\n",
    "                    print(f\"   Labels with good performance (>0.8): {sum(1 for s in valid_scores if s > 0.8)}/{len(valid_scores)}\")\n",
    "    \n",
    "    # Recommendations\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"üí° RECOMMENDATIONS:\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    if best_model[1]['overfitting_gap'] < 0.02:\n",
    "        print(\"‚úÖ Your best model shows excellent generalization based on Hamming loss!\")\n",
    "    elif best_model[1]['overfitting_gap'] < 0.04:\n",
    "        print(\"üü¢ Your best model shows good generalization based on Hamming loss!\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Consider additional regularization for your best model:\")\n",
    "        print(\"   - Increase regularization parameters\")\n",
    "        print(\"   - Use more training data\")\n",
    "        print(\"   - Apply feature selection\")\n",
    "        print(\"   - Consider ensemble methods\")\n",
    "    \n",
    "    hamming_gap_threshold = 0.02\n",
    "    models_with_overfitting = [name for name, result in successful_results.items() \n",
    "                              if result['hamming_gap'] > hamming_gap_threshold]\n",
    "    \n",
    "    if models_with_overfitting:\n",
    "        print(f\"\\n‚ö†Ô∏è Models showing overfitting based on Hamming loss (gap > {hamming_gap_threshold}):\")\n",
    "        for model in models_with_overfitting:\n",
    "            result = successful_results[model]\n",
    "            print(f\"   - {model.upper()}:\")\n",
    "            print(f\"     ‚Ä¢ Hamming Gap: {result['hamming_gap']:.4f}\")\n",
    "            print(f\"     ‚Ä¢ Accuracy Gap: {result['accuracy_gap']:.4f}\")\n",
    "            print(f\"     ‚Ä¢ F1 Gap: {result['f1_gap']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nüéØ Model Selection Priority (Updated with Hamming Loss):\")\n",
    "    print(\"   1. Choose model with best VALIDATION performance\")\n",
    "    print(\"   2. Prefer models with smaller Hamming loss gap (primary indicator)\")\n",
    "    print(\"   3. Consider accuracy and F1 gaps as secondary indicators\")\n",
    "    print(\"   4. Evaluate computational efficiency for deployment\")\n",
    "    print(\"   5. Lower Hamming loss = better multi-label classification performance\")\n",
    "    \n",
    "    print(f\"\\nüìä Understanding Hamming Loss:\")\n",
    "    print(\"   ‚Ä¢ Hamming Loss measures label-wise classification errors\")\n",
    "    print(\"   ‚Ä¢ Perfect score = 0.0, higher values = more errors\")\n",
    "    print(\"   ‚Ä¢ Particularly important for multi-label problems\")\n",
    "    print(\"   ‚Ä¢ Gap = Val_Hamming - Train_Hamming (positive = overfitting)\")\n",
    "    \n",
    "    return successful_results\n",
    "\n",
    "# Example usage\n",
    "print(\"Starting comprehensive model comparison...\")\n",
    "all_results = compare_all_models(X_train_tfidf, y_train, X_val_tfidf, y_val, X_test_tfidf, y_test)\n",
    "final_analysis = analyze_model_results(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c057bf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example usage of the enhanced training function\n",
    "# print(\"Testing enhanced validation-controlled training...\")\n",
    "# print(\"=\"*60)\n",
    "\n",
    "# # Test with LightGBM\n",
    "# lgbm_model, lgbm_metrics = training_function_with_validation(\n",
    "#     X_train_tfidf, y_train, X_val_tfidf, y_val, model_type='lightgbm'\n",
    "# )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea5a3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test with XGBoost\n",
    "# print(f\"\\n{'-'*40}\")\n",
    "# xgb_model, xgb_metrics = training_function_with_validation(\n",
    "#     X_train_tfidf, y_train, X_val_tfidf, y_val, model_type='xgboost'\n",
    "# )\n",
    "\n",
    "# print(f\"\\nXGBoost Results:\")\n",
    "# print(f\"  Validation Accuracy: {xgb_metrics['val_accuracy']:.4f}\")\n",
    "# print(f\"  Overfitting Gap: {xgb_metrics['overfitting_gap']:.4f}\")\n",
    "\n",
    "# # Final test predictions\n",
    "# lgbm_test_pred = lgbm_model.predict(X_test_tfidf)\n",
    "# xgb_test_pred = xgb_model.predict(X_test_tfidf)\n",
    "\n",
    "# lgbm_test_acc = accuracy_score(y_test, lgbm_test_pred)\n",
    "# xgb_test_acc = accuracy_score(y_test, xgb_test_pred)\n",
    "\n",
    "# print(f\"\\nFinal Test Results:\")\n",
    "# print(f\"  LightGBM Test Accuracy: {lgbm_test_acc:.4f}\")\n",
    "# print(f\"  XGBoost Test Accuracy: {xgb_test_acc:.4f}\")\n",
    "\n",
    "# # Determine best model\n",
    "# if lgbm_metrics['val_accuracy'] > xgb_metrics['val_accuracy']:\n",
    "#     best_val_model = 'LightGBM'\n",
    "#     best_model = lgbm_model\n",
    "#     best_test_acc = lgbm_test_acc\n",
    "# else:\n",
    "#     best_val_model = 'XGBoost'\n",
    "#     best_model = xgb_model\n",
    "#     best_test_acc = xgb_test_acc\n",
    "\n",
    "# print(f\"\\nüèÜ Best validation-controlled model: {best_val_model}\")\n",
    "# print(f\"   Test Accuracy: {best_test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217b6c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Additional Validation Techniques for Overfitting Control\n",
    "\n",
    "# from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "# from sklearn.model_selection import validation_curve, learning_curve\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def plot_learning_curve(estimator, X, y, title, cv=5, n_jobs=-1, \n",
    "#                        train_sizes=np.linspace(0.1, 1.0, 10)):\n",
    "#     \"\"\"\n",
    "#     Generate a plot showing the learning curve for a model\n",
    "#     \"\"\"\n",
    "#     train_sizes, train_scores, val_scores = learning_curve(\n",
    "#         estimator, X, y, cv=cv, n_jobs=n_jobs, \n",
    "#         train_sizes=train_sizes, scoring='accuracy'\n",
    "#     )\n",
    "    \n",
    "#     train_scores_mean = np.mean(train_scores, axis=1)\n",
    "#     train_scores_std = np.std(train_scores, axis=1)\n",
    "#     val_scores_mean = np.mean(val_scores, axis=1)\n",
    "#     val_scores_std = np.std(val_scores, axis=1)\n",
    "    \n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     plt.plot(train_sizes, train_scores_mean, 'o-', color='blue', label='Training score')\n",
    "#     plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "#                      train_scores_mean + train_scores_std, alpha=0.1, color='blue')\n",
    "    \n",
    "#     plt.plot(train_sizes, val_scores_mean, 'o-', color='red', label='Cross-validation score')\n",
    "#     plt.fill_between(train_sizes, val_scores_mean - val_scores_std,\n",
    "#                      val_scores_mean + val_scores_std, alpha=0.1, color='red')\n",
    "    \n",
    "#     plt.xlabel('Training Set Size')\n",
    "#     plt.ylabel('Accuracy Score')\n",
    "#     plt.title(f'Learning Curve - {title}')\n",
    "#     plt.legend(loc='best')\n",
    "#     plt.grid(True, alpha=0.3)\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "    \n",
    "#     # Detect overfitting\n",
    "#     final_gap = train_scores_mean[-1] - val_scores_mean[-1]\n",
    "#     if final_gap > 0.1:\n",
    "#         print(f\"‚ö†Ô∏è WARNING: {title} shows signs of overfitting (gap: {final_gap:.4f})\")\n",
    "#     elif final_gap > 0.05:\n",
    "#         print(f\"üî∂ MODERATE: {title} shows moderate overfitting (gap: {final_gap:.4f})\")\n",
    "#     else:\n",
    "#         print(f\"‚úÖ GOOD: {title} shows good generalization (gap: {final_gap:.4f})\")\n",
    "\n",
    "# def cross_validate_with_overfitting_check(model, X, y, cv=5, model_name=\"Model\"):\n",
    "#     \"\"\"\n",
    "#     Perform cross-validation and check for overfitting signs\n",
    "#     \"\"\"\n",
    "#     print(f\"\\nCross-validating {model_name}...\")\n",
    "    \n",
    "#     # Perform cross-validation\n",
    "#     cv_scores = cross_val_score(model, X, y, cv=cv, scoring='accuracy', n_jobs=-1)\n",
    "    \n",
    "#     # Train on full dataset to check training score\n",
    "#     model.fit(X, y)\n",
    "#     train_score = model.score(X, y)\n",
    "    \n",
    "#     cv_mean = cv_scores.mean()\n",
    "#     cv_std = cv_scores.std()\n",
    "    \n",
    "#     print(f\"  Cross-validation scores: {cv_scores}\")\n",
    "#     print(f\"  CV Mean ¬± Std: {cv_mean:.4f} ¬± {cv_std:.4f}\")\n",
    "#     print(f\"  Training score: {train_score:.4f}\")\n",
    "    \n",
    "#     # Check for overfitting\n",
    "#     overfitting_gap = train_score - cv_mean\n",
    "#     print(f\"  Overfitting gap: {overfitting_gap:.4f}\")\n",
    "    \n",
    "#     if overfitting_gap > 0.1:\n",
    "#         status = \"üö® HIGH OVERFITTING\"\n",
    "#     elif overfitting_gap > 0.05:\n",
    "#         status = \"‚ö†Ô∏è MODERATE OVERFITTING\"\n",
    "#     else:\n",
    "#         status = \"‚úÖ GOOD GENERALIZATION\"\n",
    "    \n",
    "#     print(f\"  Status: {status}\")\n",
    "    \n",
    "#     return {\n",
    "#         'cv_scores': cv_scores,\n",
    "#         'cv_mean': cv_mean,\n",
    "#         'cv_std': cv_std,\n",
    "#         'train_score': train_score,\n",
    "#         'overfitting_gap': overfitting_gap,\n",
    "#         'status': status\n",
    "#     }\n",
    "\n",
    "# def plot_validation_curve_param(estimator, X, y, param_name, param_range, title):\n",
    "#     \"\"\"\n",
    "#     Plot validation curve for a specific parameter to find optimal value\n",
    "#     \"\"\"\n",
    "#     train_scores, val_scores = validation_curve(\n",
    "#         estimator, X, y, param_name=param_name, param_range=param_range,\n",
    "#         cv=5, scoring='accuracy', n_jobs=-1\n",
    "#     )\n",
    "    \n",
    "#     train_scores_mean = np.mean(train_scores, axis=1)\n",
    "#     train_scores_std = np.std(train_scores, axis=1)\n",
    "#     val_scores_mean = np.mean(val_scores, axis=1)\n",
    "#     val_scores_std = np.std(val_scores, axis=1)\n",
    "    \n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     plt.semilogx(param_range, train_scores_mean, 'o-', color='blue', label='Training score')\n",
    "#     plt.fill_between(param_range, train_scores_mean - train_scores_std,\n",
    "#                      train_scores_mean + train_scores_std, alpha=0.1, color='blue')\n",
    "    \n",
    "#     plt.semilogx(param_range, val_scores_mean, 'o-', color='red', label='Cross-validation score')\n",
    "#     plt.fill_between(param_range, val_scores_mean - val_scores_std,\n",
    "#                      val_scores_mean + val_scores_std, alpha=0.1, color='red')\n",
    "    \n",
    "#     plt.xlabel(param_name)\n",
    "#     plt.ylabel('Accuracy Score')\n",
    "#     plt.title(f'Validation Curve - {title}')\n",
    "#     plt.legend(loc='best')\n",
    "#     plt.grid(True, alpha=0.3)\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "    \n",
    "#     # Find optimal parameter\n",
    "#     optimal_idx = np.argmax(val_scores_mean)\n",
    "#     optimal_param = param_range[optimal_idx]\n",
    "#     optimal_score = val_scores_mean[optimal_idx]\n",
    "    \n",
    "#     print(f\"Optimal {param_name}: {optimal_param}\")\n",
    "#     print(f\"Optimal CV score: {optimal_score:.4f}\")\n",
    "    \n",
    "#     return optimal_param, optimal_score\n",
    "\n",
    "# # Example: Cross-validation analysis for overfitting detection\n",
    "# print(\"COMPREHENSIVE VALIDATION ANALYSIS\")\n",
    "# print(\"=\"*60)\n",
    "\n",
    "# # Sample a subset for faster computation in demo\n",
    "# sample_size = min(1000, len(X_train_tfidf))\n",
    "# X_sample = X_train_tfidf[:sample_size]\n",
    "# y_sample = y_train[:sample_size]\n",
    "\n",
    "# print(f\"Using sample of {sample_size} examples for validation analysis...\")\n",
    "\n",
    "# # 1. Cross-validation for different models\n",
    "# models_for_cv = {\n",
    "#     'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "#     'Random Forest': RandomForestClassifier(n_estimators=50, random_state=42, max_depth=10),\n",
    "# }\n",
    "\n",
    "# cv_results = {}\n",
    "# for name, model in models_for_cv.items():\n",
    "#     # Use OneVsRestClassifier for multi-label\n",
    "#     multi_label_model = OneVsRestClassifier(model)\n",
    "#     cv_results[name] = cross_validate_with_overfitting_check(\n",
    "#         multi_label_model, X_sample, y_sample, cv=3, model_name=name\n",
    "#     )\n",
    "\n",
    "# # 2. Find models with best generalization\n",
    "# print(f\"\\n{'='*60}\")\n",
    "# print(\"OVERFITTING SUMMARY:\")\n",
    "# print(f\"{'Model':<20} | {'CV Score':<10} | {'Gap':<8} | {'Status'}\")\n",
    "# print(f\"{'-'*65}\")\n",
    "\n",
    "# for name, results in cv_results.items():\n",
    "#     print(f\"{name:<20} | {results['cv_mean']:<10.4f} | {results['overfitting_gap']:<8.4f} | {results['status']}\")\n",
    "\n",
    "# # 3. Recommendations for overfitting control\n",
    "# print(f\"\\n{'='*60}\")\n",
    "# print(\"RECOMMENDATIONS FOR OVERFITTING CONTROL:\")\n",
    "# print()\n",
    "# print(\"1. üìä VALIDATION MONITORING:\")\n",
    "# print(\"   - Always split data into train/validation/test\")\n",
    "# print(\"   - Monitor validation metrics during training\")\n",
    "# print(\"   - Use early stopping when validation stops improving\")\n",
    "# print()\n",
    "# print(\"2. üîß MODEL REGULARIZATION:\")\n",
    "# print(\"   - Logistic Regression: Adjust C parameter (lower = more regularization)\")\n",
    "# print(\"   - Random Forest: Limit max_depth, increase min_samples_split\")\n",
    "# print(\"   - XGBoost/LightGBM: Use early_stopping_rounds, adjust learning_rate\")\n",
    "# print()\n",
    "# print(\"3. üìà TECHNIQUES IMPLEMENTED:\")\n",
    "# print(\"   - Train/Validation/Test split (70/15/15)\")\n",
    "# print(\"   - Cross-validation for robust evaluation\")\n",
    "# print(\"   - Early stopping for tree-based models\")\n",
    "# print(\"   - Validation gap monitoring\")\n",
    "# print(\"   - Learning curve analysis\")\n",
    "# print()\n",
    "# print(\"4. üéØ SELECTION CRITERIA:\")\n",
    "# print(\"   - Choose model with best VALIDATION performance\")\n",
    "# print(\"   - Prefer models with smaller train-validation gap\")\n",
    "# print(\"   - Consider cross-validation consistency\")\n",
    "\n",
    "# # Example of how to use validation curve for parameter tuning\n",
    "# print(f\"\\n{'='*60}\")\n",
    "# print(\"PARAMETER TUNING WITH VALIDATION CURVES:\")\n",
    "# print(\"(Use this approach to find optimal hyperparameters)\")\n",
    "# print()\n",
    "# print(\"Example code for Random Forest max_depth tuning:\")\n",
    "# print(\"\"\"\n",
    "# # Find optimal max_depth for Random Forest\n",
    "# param_range = [3, 5, 7, 10, 15, 20]\n",
    "# optimal_depth, optimal_score = plot_validation_curve_param(\n",
    "#     OneVsRestClassifier(RandomForestClassifier(random_state=42)),\n",
    "#     X_train_tfidf, y_train,\n",
    "#     param_name='estimator__max_depth',\n",
    "#     param_range=param_range,\n",
    "#     title='Random Forest max_depth'\n",
    "# )\n",
    "# \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a7c4d3",
   "metadata": {},
   "source": [
    "## Transformers Encoder Model (MordenBERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48e3714a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset, DatasetDict\n",
    "from datasets import Sequence, Value\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import EarlyStoppingCallback\n",
    "import evaluate\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa586740",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load Dataset for model training and evaluation ###\n",
    "data_path=os.path.join(os.getcwd(), 'processed_data')\n",
    "with open(os.path.join(data_path,'train_arrays.pkl'), 'rb') as f:\n",
    "    train_data = pickle.load(f)\n",
    "    X_train = train_data['X_train']\n",
    "    y_train = train_data['y_train']\n",
    "\n",
    "with open(os.path.join(data_path,'val_arrays.pkl'), 'rb') as f:\n",
    "    val_data = pickle.load(f)\n",
    "    X_val = val_data['X_val']\n",
    "    y_val = val_data['y_val']\n",
    "\n",
    "with open(os.path.join(data_path,'test_arrays.pkl'), 'rb') as f:\n",
    "    test_data = pickle.load(f)\n",
    "    X_test = test_data['X_test']\n",
    "    y_test = test_data['y_test']\n",
    "\n",
    "with open(os.path.join(data_path,'class_name.pkl'), 'rb') as f:\n",
    "    class_name_data = pickle.load(f)\n",
    "    class_name = class_name_data['class_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d27045a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets_from_arrays(X_train, y_train, X_val=None, y_val=None, X_test=None, y_test=None):\n",
    "    \"\"\"\n",
    "    Convert arrays into HuggingFace datasets format with specified structure\n",
    "    \n",
    "    Returns:\n",
    "        DatasetDict with features:\n",
    "        - dataset[\"train\"][\"text\"]: text data\n",
    "        - dataset[\"train\"][\"labels\"]: multi-label arrays\n",
    "        - dataset[\"val\"][\"text\"]: validation text data (if provided)\n",
    "        - dataset[\"val\"][\"labels\"]: validation labels (if provided)\n",
    "        - dataset[\"test\"][\"text\"]: test text data (if provided)\n",
    "        - dataset[\"test\"][\"labels\"]: test labels (if provided)\n",
    "    \"\"\"\n",
    "    # Create training dataset\n",
    "    train_dict = {\n",
    "        \"text\": X_train.tolist() if hasattr(X_train, 'tolist') else list(X_train),\n",
    "        \"labels\": y_train.tolist() if hasattr(y_train, 'tolist') else list(y_train)\n",
    "    }\n",
    "    \n",
    "    datasets_dict = {\n",
    "        \"train\": Dataset.from_dict(train_dict)\n",
    "    }\n",
    "    \n",
    "    # Add validation dataset if provided\n",
    "    if X_val is not None and y_val is not None:\n",
    "        val_dict = {\n",
    "            \"text\": X_val.tolist() if hasattr(X_val, 'tolist') else list(X_val),\n",
    "            \"labels\": y_val.tolist() if hasattr(y_val, 'tolist') else list(y_val)\n",
    "        }\n",
    "        datasets_dict[\"val\"] = Dataset.from_dict(val_dict)\n",
    "    \n",
    "    # Add test dataset if provided\n",
    "    if X_test is not None and y_test is not None:\n",
    "        test_dict = {\n",
    "            \"text\": X_test.tolist() if hasattr(X_test, 'tolist') else list(X_test),\n",
    "            \"labels\": y_test.tolist() if hasattr(y_test, 'tolist') else list(y_test)\n",
    "        }\n",
    "        datasets_dict[\"test\"] = Dataset.from_dict(test_dict)\n",
    "\n",
    "    # Create DatasetDict\n",
    "    dataset = DatasetDict(datasets_dict)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "669a76ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'labels'],\n",
       "        num_rows: 11597\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['text', 'labels'],\n",
       "        num_rows: 2485\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'labels'],\n",
       "        num_rows: 2486\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the datasets\n",
    "dataset = create_datasets_from_arrays(X_train, y_train, X_val, y_val, X_test, y_test)\n",
    "\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef977ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximal context length:  8,192          \n",
      "Vocabulary size :        50,280         \n"
     ]
    }
   ],
   "source": [
    "model_path = \"answerdotai/ModernBERT-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "print(\"{:<25}{:<15,}\".format(\"Maximal context length:\",tokenizer.model_max_length))\n",
    "print(\"{:<25}{:<15,}\".format(\"Vocabulary size :\",tokenizer.vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ecb279f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Re-tokenizing dataset with fixed function...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5210ae875767420abd6a31d97dfa5a9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing dataset:   0%|          | 0/11597 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deca8dbc4f94481ca83eb74237b74faa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing dataset:   0%|          | 0/2485 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40d56ca7b2824e20bd173fba7096213e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing dataset:   0%|          | 0/2486 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Converting labels to float32...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5caf056ca56e4683a5b89c08647b991c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/11597 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "804ee6ce3c244593ac5c77fe57d1302e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/2485 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1707a5e5a700401e97c1c99d4fcf949e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/2486 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Tokenized dataset verification:\n",
      "Features: ['labels', 'input_ids', 'attention_mask']\n",
      "\n",
      "Sample structure:\n",
      "  labels: List/Array of length 27, dtype: <class 'float'>, shape: (27,), sum: 1.0\n",
      "  input_ids: List/Array of length 1024, dtype: <class 'int'>\n",
      "  attention_mask: List/Array of length 1024, dtype: <class 'int'>\n",
      "\n",
      "üéØ Labels verification:\n",
      "  Labels dtype: float64\n",
      "  Labels shape: (27,)\n",
      "  Expected shape: (27,)\n",
      "  Labels range: [0.0, 1.0]\n",
      "  PyTorch tensor dtype: torch.float32\n",
      "  PyTorch tensor shape: torch.Size([27])\n",
      "‚úÖ SUCCESS: Labels are properly formatted as float32\n",
      "üöÄ Ready for training!\n",
      "\n",
      "üìä Dataset sizes after tokenization:\n",
      "  train: 11597 samples\n",
      "  val: 2485 samples\n",
      "  test: 2486 samples\n"
     ]
    }
   ],
   "source": [
    "# üîß FIXED TOKENIZATION AND DATA FORMAT\n",
    "# This section addresses the data format issues that cause training failures\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    \"\"\"\n",
    "    Proper tokenization function for multi-label classification.\n",
    "    Ensures all outputs are compatible with HuggingFace Trainer.\n",
    "    \"\"\"\n",
    "    # Handle batch vs single example\n",
    "    if isinstance(examples['text'], str):\n",
    "        texts = [examples['text']]\n",
    "        labels = [examples['labels']]\n",
    "    else:\n",
    "        texts = examples['text']\n",
    "        labels = examples['labels']\n",
    "    \n",
    "    # Tokenize the texts\n",
    "    tokenized = tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        padding=True,  # Will be handled by data collator\n",
    "        # max_length=tokenizer.model_max_length,  \n",
    "        max_length=1024, # Adjust based on your model's limit\n",
    "        return_tensors=None  # Don't return tensors yet, let data collator handle it\n",
    "    )\n",
    "    \n",
    "    # Ensure labels are float32 for BCEWithLogitsLoss\n",
    "    if isinstance(labels[0], (list, np.ndarray)):\n",
    "        tokenized['labels'] = [np.array(label, dtype=np.float32).tolist() for label in labels]\n",
    "    else:\n",
    "        tokenized['labels'] = [np.array(labels, dtype=np.float32).tolist()]\n",
    "    \n",
    "    return tokenized\n",
    "\n",
    "print(\"üîß Re-tokenizing dataset with fixed function...\")\n",
    "\n",
    "# Apply the tokenization function\n",
    "tokenized_dataset = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=['text'],  # Remove the problematic text column\n",
    "    desc=\"Tokenizing dataset\"\n",
    ")\n",
    "\n",
    "# Define the proper feature type for multi-label classification\n",
    "label_feature = Sequence(Value(\"float32\"), length=len(class_name))\n",
    "\n",
    "# Cast the labels column to float32 for all splits\n",
    "print(\"üîÑ Converting labels to float32...\")\n",
    "for split_name in tokenized_dataset.keys():\n",
    "    tokenized_dataset[split_name] = tokenized_dataset[split_name].cast_column(\"labels\", label_feature)\n",
    "\n",
    "# Verify the tokenized dataset structure\n",
    "print(\"\\n‚úÖ Tokenized dataset verification:\")\n",
    "print(f\"Features: {list(tokenized_dataset['train'].features.keys())}\")\n",
    "\n",
    "# Check a sample\n",
    "sample = tokenized_dataset[\"train\"][0]\n",
    "print(f\"\\nSample structure:\")\n",
    "for key, value in sample.items():\n",
    "    if isinstance(value, (list, np.ndarray)):\n",
    "        value_info = f\"List/Array of length {len(value)}, dtype: {type(value[0]) if value else 'empty'}\"\n",
    "        if key == 'labels':\n",
    "            value_info += f\", shape: {np.array(value).shape}, sum: {np.sum(value)}\"\n",
    "    else:\n",
    "        value_info = f\"Type: {type(value)}, Value: {value}\"\n",
    "    print(f\"  {key}: {value_info}\")\n",
    "\n",
    "# Verify labels are properly formatted\n",
    "sample_labels = np.array(sample['labels'])\n",
    "print(f\"\\nüéØ Labels verification:\")\n",
    "print(f\"  Labels dtype: {sample_labels.dtype}\")\n",
    "print(f\"  Labels shape: {sample_labels.shape}\")\n",
    "print(f\"  Expected shape: ({len(class_name)},)\")\n",
    "print(f\"  Labels range: [{sample_labels.min():.1f}, {sample_labels.max():.1f}]\")\n",
    "\n",
    "# Test tensor conversion\n",
    "test_labels = torch.tensor(sample['labels'], dtype=torch.float32)\n",
    "print(f\"  PyTorch tensor dtype: {test_labels.dtype}\")\n",
    "print(f\"  PyTorch tensor shape: {test_labels.shape}\")\n",
    "\n",
    "if test_labels.dtype == torch.float32:\n",
    "    print(\"‚úÖ SUCCESS: Labels are properly formatted as float32\")\n",
    "    print(\"üöÄ Ready for training!\")\n",
    "else:\n",
    "    print(f\"‚ùå ISSUE: Labels are {test_labels.dtype}, expected float32\")\n",
    "\n",
    "print(f\"\\nüìä Dataset sizes after tokenization:\")\n",
    "for split_name, split_data in tokenized_dataset.items():\n",
    "    print(f\"  {split_name}: {len(split_data)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9f7ca12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['labels', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 11597\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['labels', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 2485\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['labels', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 2486\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a522a647",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ModernBertForSequenceClassification were not initialized from the model checkpoint at answerdotai/ModernBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ Model Configuration Verification:\n",
      "  Model type: ModernBertForSequenceClassification\n",
      "  Number of labels: 27\n",
      "  Problem type: multi_label_classification\n",
      "  Expected labels: 27\n",
      "‚úÖ Model configuration matches data: 27 labels\n",
      "\n",
      "üìä Model Parameters:\n",
      "  Total parameters: 149,625,627\n",
      "  Trainable parameters: 149,625,627\n",
      "  Non-trainable parameters: 0\n",
      "\n",
      "‚úÖ Data collator and model setup completed!\n"
     ]
    }
   ],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "class2id = {class_:id for id, class_ in enumerate(class_name)}\n",
    "id2class = {id:class_ for class_, id in class2id.items()}\n",
    "\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path, \n",
    "                                                           num_labels=len(class_name),\n",
    "                                                           id2label=id2class, \n",
    "                                                           label2id=class2id,\n",
    "                                                           problem_type = \"multi_label_classification\"\n",
    ")\n",
    "\n",
    "print()\n",
    "\n",
    "# Verify model is properly configured for multi-label classification\n",
    "print(\"ü§ñ Model Configuration Verification:\")\n",
    "print(f\"  Model type: {type(model).__name__}\")\n",
    "print(f\"  Number of labels: {model.config.num_labels}\")\n",
    "print(f\"  Problem type: {getattr(model.config, 'problem_type', 'Not set')}\")\n",
    "print(f\"  Expected labels: {len(class_name)}\")\n",
    "\n",
    "# Check if model configuration matches our data\n",
    "if model.config.num_labels != len(class_name):\n",
    "    print(f\"‚ö†Ô∏è WARNING: Model expects {model.config.num_labels} labels, but data has {len(class_name)}\")\n",
    "    print(\"  This might cause issues during training\")\n",
    "else:\n",
    "    print(f\"‚úÖ Model configuration matches data: {len(class_name)} labels\")\n",
    "\n",
    "# Verify model parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nüìä Model Parameters:\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"  Non-trainable parameters: {total_params - trainable_params:,}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Data collator and model setup completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "929dbc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä COMPREHENSIVE EVALUATION FUNCTION FOR MULTI-LABEL CLASSIFICATION\n",
    "# This function calculates all relevant metrics for multi-label problems\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, f1_score, \n",
    "    roc_auc_score, average_precision_score,\n",
    "    hamming_loss, jaccard_score, accuracy_score\n",
    ")\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"Sigmoid activation function\"\"\"\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "def comprehensive_evaluation(y_true, y_pred_proba, y_pred_binary=None, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation for multi-label classification with all averaging methods\n",
    "    \n",
    "    Args:\n",
    "        y_true: Ground truth binary labels (n_samples, n_labels)\n",
    "        y_pred_proba: Predicted probabilities (n_samples, n_labels)\n",
    "        y_pred_binary: Predicted binary labels (n_samples, n_labels), optional\n",
    "        threshold: Threshold for converting probabilities to binary (default: 0.5)\n",
    "    \n",
    "    Returns:\n",
    "        dict: Comprehensive metrics including all averaging methods\n",
    "    \"\"\"\n",
    "    if y_pred_binary is None:\n",
    "        y_pred_binary = (y_pred_proba >= threshold).astype(int)\n",
    "    \n",
    "    metrics = {}\n",
    "    \n",
    "    try:\n",
    "        # SAMPLES AVERAGE (per-sample then average across samples)\n",
    "        metrics['precision_samples'] = precision_score(y_true, y_pred_binary, average='samples', zero_division=0)\n",
    "        metrics['recall_samples'] = recall_score(y_true, y_pred_binary, average='samples', zero_division=0)\n",
    "        metrics['f1_samples'] = f1_score(y_true, y_pred_binary, average='samples', zero_division=0)\n",
    "        \n",
    "        # MICRO AVERAGE (global aggregation)\n",
    "        metrics['precision_micro'] = precision_score(y_true, y_pred_binary, average='micro', zero_division=0)\n",
    "        metrics['recall_micro'] = recall_score(y_true, y_pred_binary, average='micro', zero_division=0)\n",
    "        metrics['f1_micro'] = f1_score(y_true, y_pred_binary, average='micro', zero_division=0)\n",
    "        \n",
    "        # MACRO AVERAGE (unweighted average across labels)\n",
    "        metrics['precision_macro'] = precision_score(y_true, y_pred_binary, average='macro', zero_division=0)\n",
    "        metrics['recall_macro'] = recall_score(y_true, y_pred_binary, average='macro', zero_division=0)\n",
    "        metrics['f1_macro'] = f1_score(y_true, y_pred_binary, average='macro', zero_division=0)\n",
    "        \n",
    "        # WEIGHTED AVERAGE (weighted by support)\n",
    "        metrics['precision_weighted'] = precision_score(y_true, y_pred_binary, average='weighted', zero_division=0)\n",
    "        metrics['recall_weighted'] = recall_score(y_true, y_pred_binary, average='weighted', zero_division=0)\n",
    "        metrics['f1_weighted'] = f1_score(y_true, y_pred_binary, average='weighted', zero_division=0)\n",
    "        \n",
    "        # ACCURACY METRICS\n",
    "        metrics['accuracy'] = accuracy_score(y_true, y_pred_binary)\n",
    "        metrics['hamming_loss'] = hamming_loss(y_true, y_pred_binary)\n",
    "        \n",
    "        # JACCARD (IoU) METRICS \n",
    "        metrics['jaccard_samples'] = jaccard_score(y_true, y_pred_binary, average='samples', zero_division=0)\n",
    "        metrics['jaccard_macro'] = jaccard_score(y_true, y_pred_binary, average='macro', zero_division=0)\n",
    "        metrics['jaccard_weighted'] = jaccard_score(y_true, y_pred_binary, average='weighted', zero_division=0)\n",
    "        \n",
    "        # ROC-AUC METRICS (using probabilities)\n",
    "        try:\n",
    "            metrics['roc_auc_micro'] = roc_auc_score(y_true, y_pred_proba, average='micro')\n",
    "            metrics['roc_auc_macro'] = roc_auc_score(y_true, y_pred_proba, average='macro')\n",
    "            metrics['roc_auc_weighted'] = roc_auc_score(y_true, y_pred_proba, average='weighted')\n",
    "            metrics['roc_auc_samples'] = roc_auc_score(y_true, y_pred_proba, average='samples')\n",
    "        except ValueError as e:\n",
    "            print(f\"Warning: ROC-AUC calculation failed: {e}\")\n",
    "            metrics['roc_auc_micro'] = 0.0\n",
    "            metrics['roc_auc_macro'] = 0.0\n",
    "            metrics['roc_auc_weighted'] = 0.0\n",
    "            metrics['roc_auc_samples'] = 0.0\n",
    "        \n",
    "        # PR-AUC METRICS (using probabilities)\n",
    "        try:\n",
    "            metrics['pr_auc_micro'] = average_precision_score(y_true, y_pred_proba, average='micro')\n",
    "            metrics['pr_auc_macro'] = average_precision_score(y_true, y_pred_proba, average='macro')\n",
    "            metrics['pr_auc_weighted'] = average_precision_score(y_true, y_pred_proba, average='weighted')\n",
    "            metrics['pr_auc_samples'] = average_precision_score(y_true, y_pred_proba, average='samples')\n",
    "        except ValueError as e:\n",
    "            print(f\"Warning: PR-AUC calculation failed: {e}\")\n",
    "            metrics['pr_auc_micro'] = 0.0\n",
    "            metrics['pr_auc_macro'] = 0.0\n",
    "            metrics['pr_auc_weighted'] = 0.0\n",
    "            metrics['pr_auc_samples'] = 0.0\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in comprehensive_evaluation: {e}\")\n",
    "        # Return minimal metrics if calculation fails\n",
    "        metrics = {\n",
    "            'precision_micro': 0.0, 'recall_micro': 0.0, 'f1_micro': 0.0,\n",
    "            'precision_macro': 0.0, 'recall_macro': 0.0, 'f1_macro': 0.0,\n",
    "            'accuracy': 0.0, 'hamming_loss': 1.0\n",
    "        }\n",
    "    \n",
    "    return metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe13d3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéØ COMPUTE METRICS FUNCTION FOR TRAINER\n",
    "# This function is called during training to evaluate the model\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Enhanced compute_metrics function for transformers Trainer using comprehensive evaluation\n",
    "    \"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    # Apply sigmoid to get probabilities\n",
    "    predictions_proba = sigmoid(predictions)\n",
    "    \n",
    "    # Convert to binary predictions using threshold 0.5\n",
    "    predictions_binary = (predictions_proba > 0.5).astype(int)\n",
    "    \n",
    "    # Ensure labels are integers\n",
    "    labels = labels.astype(int)\n",
    "    \n",
    "    # Use comprehensive evaluation\n",
    "    metrics = comprehensive_evaluation(\n",
    "        y_true=labels,\n",
    "        y_pred_proba=predictions_proba,\n",
    "        y_pred_binary=predictions_binary,\n",
    "        threshold=0.5\n",
    "    )\n",
    "    \n",
    "    # Return metrics with eval_ prefix for Trainer compatibility\n",
    "    return {\n",
    "        # Primary metrics for monitoring\n",
    "        'eval_f1_micro': metrics['f1_micro'],\n",
    "        'eval_f1_macro': metrics['f1_macro'],\n",
    "        'eval_accuracy': metrics['accuracy'],\n",
    "        'eval_hamming_loss': metrics['hamming_loss'],\n",
    "        \n",
    "        # Precision metrics\n",
    "        'eval_precision_micro': metrics['precision_micro'],\n",
    "        'eval_precision_macro': metrics['precision_macro'],\n",
    "        'eval_precision_samples': metrics['precision_samples'],\n",
    "        'eval_precision_weighted': metrics['precision_weighted'],\n",
    "        \n",
    "        # Recall metrics\n",
    "        'eval_recall_micro': metrics['recall_micro'],\n",
    "        'eval_recall_macro': metrics['recall_macro'],\n",
    "        'eval_recall_samples': metrics['recall_samples'],\n",
    "        'eval_recall_weighted': metrics['recall_weighted'],\n",
    "        \n",
    "        # F1 metrics\n",
    "        'eval_f1_samples': metrics['f1_samples'],\n",
    "        'eval_f1_weighted': metrics['f1_weighted'],\n",
    "        \n",
    "        # ROC-AUC metrics\n",
    "        'eval_roc_auc_micro': metrics['roc_auc_micro'],\n",
    "        'eval_roc_auc_macro': metrics['roc_auc_macro'],\n",
    "        'eval_roc_auc_weighted': metrics['roc_auc_weighted'],\n",
    "        'eval_roc_auc_samples': metrics['roc_auc_samples'],\n",
    "        \n",
    "        # PR-AUC metrics\n",
    "        'eval_pr_auc_micro': metrics['pr_auc_micro'],\n",
    "        'eval_pr_auc_macro': metrics['pr_auc_macro'],\n",
    "        'eval_pr_auc_weighted': metrics['pr_auc_weighted'],\n",
    "        'eval_pr_auc_samples': metrics['pr_auc_samples'],\n",
    "        \n",
    "        # Jaccard metrics\n",
    "        'eval_jaccard_samples': metrics['jaccard_samples'],\n",
    "        'eval_jaccard_macro': metrics['jaccard_macro'],\n",
    "        'eval_jaccard_weighted': metrics['jaccard_weighted'],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd24291f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c8d85b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ RUNNING PRE-TRAINING VERIFICATION TESTS\n",
      "============================================================\n",
      "1Ô∏è‚É£ Testing data collator...\n",
      "   ‚úÖ Data collator working: batch shape torch.Size([3, 1024])\n",
      "   ‚úÖ Labels shape: torch.Size([3, 27])\n",
      "   ‚úÖ Labels dtype: torch.float32\n",
      "\n",
      "2Ô∏è‚É£ Testing model forward pass...\n",
      "   ‚úÖ Model forward pass working\n",
      "   ‚úÖ Output logits shape: torch.Size([3, 27])\n",
      "   ‚úÖ Loss computed: 0.7586\n",
      "\n",
      "3Ô∏è‚É£ Testing compute_metrics function...\n",
      "   ‚úÖ Compute metrics working\n",
      "   ‚úÖ Primary metrics calculated:\n",
      "      - F1 Micro: 0.4689\n",
      "      - Hamming Loss: 0.5370\n",
      "      - Accuracy: 0.0000\n",
      "\n",
      "4Ô∏è‚É£ Checking compute environment...\n",
      "   ‚úÖ GPU available: NVIDIA GeForce RTX 4080 Laptop GPU\n",
      "   ‚úÖ CUDA version: 12.6\n",
      "   ‚úÖ Memory allocated: 0.00 GB\n",
      "\n",
      "üéØ FINAL VERIFICATION:\n",
      "   ‚úÖ Dataset loaded: 3 splits\n",
      "   ‚úÖ Training samples: 11597\n",
      "   ‚úÖ Validation samples: 2485\n",
      "   ‚úÖ Model loaded: ModernBertForSequenceClassification\n",
      "   ‚úÖ Tokenizer loaded: PreTrainedTokenizerFast\n",
      "   ‚úÖ Data collator ready: DataCollatorWithPadding\n",
      "   ‚úÖ Metrics function ready: compute_metrics\n",
      "\n",
      "üöÄ All systems ready for training!\n"
     ]
    }
   ],
   "source": [
    "# üß™ PRE-TRAINING VERIFICATION TEST\n",
    "# Test that all components work together before starting training\n",
    "\n",
    "print(\"üß™ RUNNING PRE-TRAINING VERIFICATION TESTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test 1: Verify data collator works with a batch\n",
    "print(\"1Ô∏è‚É£ Testing data collator...\")\n",
    "test_batch = [tokenized_dataset[\"train\"][i] for i in range(3)]\n",
    "try:\n",
    "    collated_batch = data_collator(test_batch)\n",
    "    print(f\"   ‚úÖ Data collator working: batch shape {collated_batch['input_ids'].shape}\")\n",
    "    print(f\"   ‚úÖ Labels shape: {collated_batch['labels'].shape}\")\n",
    "    print(f\"   ‚úÖ Labels dtype: {collated_batch['labels'].dtype}\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Data collator failed: {e}\")\n",
    "\n",
    "# Test 2: Verify model forward pass\n",
    "print(\"\\n2Ô∏è‚É£ Testing model forward pass...\")\n",
    "try:\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**{k: v for k, v in collated_batch.items() if k in ['input_ids', 'attention_mask', 'labels']})\n",
    "    print(f\"   ‚úÖ Model forward pass working\")\n",
    "    print(f\"   ‚úÖ Output logits shape: {outputs.logits.shape}\")\n",
    "    print(f\"   ‚úÖ Loss computed: {outputs.loss.item():.4f}\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Model forward pass failed: {e}\")\n",
    "\n",
    "# Test 3: Verify compute_metrics function\n",
    "print(\"\\n3Ô∏è‚É£ Testing compute_metrics function...\")\n",
    "try:\n",
    "    # Create dummy predictions for testing\n",
    "    dummy_predictions = np.random.randn(10, len(class_name))\n",
    "    dummy_labels = np.random.randint(0, 2, (10, len(class_name))).astype(np.float32)\n",
    "    \n",
    "    eval_pred = (dummy_predictions, dummy_labels)\n",
    "    test_metrics = compute_metrics(eval_pred)\n",
    "    \n",
    "    print(f\"   ‚úÖ Compute metrics working\")\n",
    "    print(f\"   ‚úÖ Primary metrics calculated:\")\n",
    "    print(f\"      - F1 Micro: {test_metrics['eval_f1_micro']:.4f}\")\n",
    "    print(f\"      - Hamming Loss: {test_metrics['eval_hamming_loss']:.4f}\")\n",
    "    print(f\"      - Accuracy: {test_metrics['eval_accuracy']:.4f}\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Compute metrics failed: {e}\")\n",
    "\n",
    "# Test 4: Check GPU/CPU availability\n",
    "print(\"\\n4Ô∏è‚É£ Checking compute environment...\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   ‚úÖ GPU available: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"   ‚úÖ CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"   ‚úÖ Memory allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "else:\n",
    "    print(\"   ‚ÑπÔ∏è Running on CPU\")\n",
    "\n",
    "# Final verification\n",
    "print(f\"\\nüéØ FINAL VERIFICATION:\")\n",
    "print(f\"   ‚úÖ Dataset loaded: {len(tokenized_dataset)} splits\")\n",
    "print(f\"   ‚úÖ Training samples: {len(tokenized_dataset['train'])}\")\n",
    "print(f\"   ‚úÖ Validation samples: {len(tokenized_dataset['val'])}\")\n",
    "print(f\"   ‚úÖ Model loaded: {type(model).__name__}\")\n",
    "print(f\"   ‚úÖ Tokenizer loaded: {type(tokenizer).__name__}\")\n",
    "print(f\"   ‚úÖ Data collator ready: {type(data_collator).__name__}\")\n",
    "print(f\"   ‚úÖ Metrics function ready: compute_metrics\")\n",
    "\n",
    "print(f\"\\nüöÄ All systems ready for training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58447936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ MULTI-LABEL METRICS CONFIGURATION:\n",
      "============================================================\n",
      "‚úÖ PRIMARY METRIC: Hamming Loss (optimal for multi-label)\n",
      "   ‚Ä¢ Measures label-wise classification errors\n",
      "   ‚Ä¢ Range: 0.0 (perfect) to 1.0 (worst)\n",
      "   ‚Ä¢ Lower values = better performance\n",
      "   ‚Ä¢ More intuitive than F1 for legal document classification\n",
      "\n",
      "üìä MONITORING METRICS (all calculated):\n",
      "   ‚Ä¢ F1-Micro/Macro/Weighted: Overall performance\n",
      "   ‚Ä¢ Precision/Recall: Per-label quality\n",
      "   ‚Ä¢ ROC-AUC/PR-AUC: Ranking quality\n",
      "   ‚Ä¢ Jaccard Score: Label overlap similarity\n",
      "   ‚Ä¢ Accuracy: Exact match rate\n",
      "üöÄ Starting training with enhanced configuration...\n",
      "üìä Training samples: 11597\n",
      "üìä Validation samples: 2485\n",
      "üéØ Target metric: eval_hamming_loss\n",
      "‚è±Ô∏è Total epochs: 5\n",
      "üîÑ Evaluation every: 100 steps\n",
      "üíæ Saving every: 100 steps\n",
      "‚èπÔ∏è Early stopping patience: 3\n",
      "\n",
      "üéØ Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1100' max='1210' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1100/1210 48:47 < 04:53, 0.38 it/s, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Hamming Loss</th>\n",
       "      <th>Precision Micro</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Precision Samples</th>\n",
       "      <th>Precision Weighted</th>\n",
       "      <th>Recall Micro</th>\n",
       "      <th>Recall Macro</th>\n",
       "      <th>Recall Samples</th>\n",
       "      <th>Recall Weighted</th>\n",
       "      <th>F1 Samples</th>\n",
       "      <th>F1 Weighted</th>\n",
       "      <th>Roc Auc Micro</th>\n",
       "      <th>Roc Auc Macro</th>\n",
       "      <th>Roc Auc Weighted</th>\n",
       "      <th>Roc Auc Samples</th>\n",
       "      <th>Pr Auc Micro</th>\n",
       "      <th>Pr Auc Macro</th>\n",
       "      <th>Pr Auc Weighted</th>\n",
       "      <th>Pr Auc Samples</th>\n",
       "      <th>Jaccard Samples</th>\n",
       "      <th>Jaccard Macro</th>\n",
       "      <th>Jaccard Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.785200</td>\n",
       "      <td>0.111019</td>\n",
       "      <td>0.624433</td>\n",
       "      <td>0.086674</td>\n",
       "      <td>0.311871</td>\n",
       "      <td>0.038259</td>\n",
       "      <td>0.702436</td>\n",
       "      <td>0.108990</td>\n",
       "      <td>0.717505</td>\n",
       "      <td>0.560948</td>\n",
       "      <td>0.562023</td>\n",
       "      <td>0.095133</td>\n",
       "      <td>0.602897</td>\n",
       "      <td>0.562023</td>\n",
       "      <td>0.622093</td>\n",
       "      <td>0.515207</td>\n",
       "      <td>0.936868</td>\n",
       "      <td>0.779747</td>\n",
       "      <td>0.910602</td>\n",
       "      <td>0.942352</td>\n",
       "      <td>0.691125</td>\n",
       "      <td>0.207249</td>\n",
       "      <td>0.690559</td>\n",
       "      <td>0.782173</td>\n",
       "      <td>0.541100</td>\n",
       "      <td>0.070114</td>\n",
       "      <td>0.437663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.775100</td>\n",
       "      <td>0.071259</td>\n",
       "      <td>0.763127</td>\n",
       "      <td>0.219544</td>\n",
       "      <td>0.509859</td>\n",
       "      <td>0.023936</td>\n",
       "      <td>0.867248</td>\n",
       "      <td>0.377614</td>\n",
       "      <td>0.851576</td>\n",
       "      <td>0.759436</td>\n",
       "      <td>0.681327</td>\n",
       "      <td>0.191785</td>\n",
       "      <td>0.734856</td>\n",
       "      <td>0.681327</td>\n",
       "      <td>0.759745</td>\n",
       "      <td>0.696875</td>\n",
       "      <td>0.972228</td>\n",
       "      <td>0.895318</td>\n",
       "      <td>0.955668</td>\n",
       "      <td>0.973028</td>\n",
       "      <td>0.845372</td>\n",
       "      <td>0.365474</td>\n",
       "      <td>0.781209</td>\n",
       "      <td>0.891692</td>\n",
       "      <td>0.695453</td>\n",
       "      <td>0.172415</td>\n",
       "      <td>0.617325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.458700</td>\n",
       "      <td>0.064250</td>\n",
       "      <td>0.788728</td>\n",
       "      <td>0.328245</td>\n",
       "      <td>0.556942</td>\n",
       "      <td>0.022237</td>\n",
       "      <td>0.852986</td>\n",
       "      <td>0.447978</td>\n",
       "      <td>0.854460</td>\n",
       "      <td>0.781673</td>\n",
       "      <td>0.733474</td>\n",
       "      <td>0.303061</td>\n",
       "      <td>0.778015</td>\n",
       "      <td>0.733474</td>\n",
       "      <td>0.788840</td>\n",
       "      <td>0.735088</td>\n",
       "      <td>0.980183</td>\n",
       "      <td>0.926427</td>\n",
       "      <td>0.963924</td>\n",
       "      <td>0.980714</td>\n",
       "      <td>0.869265</td>\n",
       "      <td>0.454025</td>\n",
       "      <td>0.805106</td>\n",
       "      <td>0.909977</td>\n",
       "      <td>0.729873</td>\n",
       "      <td>0.259198</td>\n",
       "      <td>0.661487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.481800</td>\n",
       "      <td>0.061398</td>\n",
       "      <td>0.796647</td>\n",
       "      <td>0.374351</td>\n",
       "      <td>0.563380</td>\n",
       "      <td>0.020970</td>\n",
       "      <td>0.882767</td>\n",
       "      <td>0.498025</td>\n",
       "      <td>0.875520</td>\n",
       "      <td>0.797587</td>\n",
       "      <td>0.725836</td>\n",
       "      <td>0.335525</td>\n",
       "      <td>0.776237</td>\n",
       "      <td>0.725836</td>\n",
       "      <td>0.795694</td>\n",
       "      <td>0.742269</td>\n",
       "      <td>0.982317</td>\n",
       "      <td>0.941393</td>\n",
       "      <td>0.968760</td>\n",
       "      <td>0.981947</td>\n",
       "      <td>0.878684</td>\n",
       "      <td>0.512911</td>\n",
       "      <td>0.823248</td>\n",
       "      <td>0.915431</td>\n",
       "      <td>0.736197</td>\n",
       "      <td>0.293438</td>\n",
       "      <td>0.664836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.169200</td>\n",
       "      <td>0.060316</td>\n",
       "      <td>0.798918</td>\n",
       "      <td>0.421377</td>\n",
       "      <td>0.562978</td>\n",
       "      <td>0.021045</td>\n",
       "      <td>0.869767</td>\n",
       "      <td>0.554482</td>\n",
       "      <td>0.863783</td>\n",
       "      <td>0.840607</td>\n",
       "      <td>0.738741</td>\n",
       "      <td>0.403935</td>\n",
       "      <td>0.785996</td>\n",
       "      <td>0.738741</td>\n",
       "      <td>0.796056</td>\n",
       "      <td>0.755967</td>\n",
       "      <td>0.983268</td>\n",
       "      <td>0.946520</td>\n",
       "      <td>0.969531</td>\n",
       "      <td>0.983421</td>\n",
       "      <td>0.881363</td>\n",
       "      <td>0.548006</td>\n",
       "      <td>0.834789</td>\n",
       "      <td>0.923255</td>\n",
       "      <td>0.736935</td>\n",
       "      <td>0.332189</td>\n",
       "      <td>0.674786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.079500</td>\n",
       "      <td>0.055266</td>\n",
       "      <td>0.821168</td>\n",
       "      <td>0.468555</td>\n",
       "      <td>0.605634</td>\n",
       "      <td>0.019390</td>\n",
       "      <td>0.858827</td>\n",
       "      <td>0.560307</td>\n",
       "      <td>0.857277</td>\n",
       "      <td>0.825217</td>\n",
       "      <td>0.786674</td>\n",
       "      <td>0.437545</td>\n",
       "      <td>0.826036</td>\n",
       "      <td>0.786674</td>\n",
       "      <td>0.815962</td>\n",
       "      <td>0.793858</td>\n",
       "      <td>0.986564</td>\n",
       "      <td>0.952711</td>\n",
       "      <td>0.972204</td>\n",
       "      <td>0.986509</td>\n",
       "      <td>0.896173</td>\n",
       "      <td>0.584159</td>\n",
       "      <td>0.844948</td>\n",
       "      <td>0.931430</td>\n",
       "      <td>0.762502</td>\n",
       "      <td>0.374139</td>\n",
       "      <td>0.711500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.071300</td>\n",
       "      <td>0.055101</td>\n",
       "      <td>0.821589</td>\n",
       "      <td>0.546144</td>\n",
       "      <td>0.597586</td>\n",
       "      <td>0.019510</td>\n",
       "      <td>0.851412</td>\n",
       "      <td>0.620372</td>\n",
       "      <td>0.862307</td>\n",
       "      <td>0.828864</td>\n",
       "      <td>0.793785</td>\n",
       "      <td>0.513095</td>\n",
       "      <td>0.831737</td>\n",
       "      <td>0.793785</td>\n",
       "      <td>0.821136</td>\n",
       "      <td>0.801769</td>\n",
       "      <td>0.986717</td>\n",
       "      <td>0.958125</td>\n",
       "      <td>0.973318</td>\n",
       "      <td>0.985989</td>\n",
       "      <td>0.897018</td>\n",
       "      <td>0.607524</td>\n",
       "      <td>0.851816</td>\n",
       "      <td>0.932736</td>\n",
       "      <td>0.765319</td>\n",
       "      <td>0.430719</td>\n",
       "      <td>0.715773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.817600</td>\n",
       "      <td>0.052281</td>\n",
       "      <td>0.831433</td>\n",
       "      <td>0.548278</td>\n",
       "      <td>0.624145</td>\n",
       "      <td>0.018064</td>\n",
       "      <td>0.880931</td>\n",
       "      <td>0.715630</td>\n",
       "      <td>0.885915</td>\n",
       "      <td>0.855441</td>\n",
       "      <td>0.787200</td>\n",
       "      <td>0.493188</td>\n",
       "      <td>0.830228</td>\n",
       "      <td>0.787200</td>\n",
       "      <td>0.832404</td>\n",
       "      <td>0.808775</td>\n",
       "      <td>0.987332</td>\n",
       "      <td>0.956924</td>\n",
       "      <td>0.972930</td>\n",
       "      <td>0.986900</td>\n",
       "      <td>0.905655</td>\n",
       "      <td>0.631611</td>\n",
       "      <td>0.856150</td>\n",
       "      <td>0.936964</td>\n",
       "      <td>0.779571</td>\n",
       "      <td>0.434646</td>\n",
       "      <td>0.724123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.813800</td>\n",
       "      <td>0.053655</td>\n",
       "      <td>0.827237</td>\n",
       "      <td>0.543852</td>\n",
       "      <td>0.613682</td>\n",
       "      <td>0.018794</td>\n",
       "      <td>0.862079</td>\n",
       "      <td>0.672494</td>\n",
       "      <td>0.869484</td>\n",
       "      <td>0.840754</td>\n",
       "      <td>0.795101</td>\n",
       "      <td>0.488834</td>\n",
       "      <td>0.834923</td>\n",
       "      <td>0.795101</td>\n",
       "      <td>0.826854</td>\n",
       "      <td>0.805474</td>\n",
       "      <td>0.987130</td>\n",
       "      <td>0.956197</td>\n",
       "      <td>0.972626</td>\n",
       "      <td>0.986695</td>\n",
       "      <td>0.903150</td>\n",
       "      <td>0.634566</td>\n",
       "      <td>0.857131</td>\n",
       "      <td>0.933913</td>\n",
       "      <td>0.772897</td>\n",
       "      <td>0.427341</td>\n",
       "      <td>0.719406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.726900</td>\n",
       "      <td>0.052462</td>\n",
       "      <td>0.839298</td>\n",
       "      <td>0.594264</td>\n",
       "      <td>0.635412</td>\n",
       "      <td>0.017602</td>\n",
       "      <td>0.868243</td>\n",
       "      <td>0.720676</td>\n",
       "      <td>0.879074</td>\n",
       "      <td>0.851082</td>\n",
       "      <td>0.812220</td>\n",
       "      <td>0.544148</td>\n",
       "      <td>0.849510</td>\n",
       "      <td>0.812220</td>\n",
       "      <td>0.840448</td>\n",
       "      <td>0.823683</td>\n",
       "      <td>0.987556</td>\n",
       "      <td>0.956128</td>\n",
       "      <td>0.972587</td>\n",
       "      <td>0.987167</td>\n",
       "      <td>0.906134</td>\n",
       "      <td>0.635478</td>\n",
       "      <td>0.857563</td>\n",
       "      <td>0.936185</td>\n",
       "      <td>0.789182</td>\n",
       "      <td>0.472721</td>\n",
       "      <td>0.737553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.626000</td>\n",
       "      <td>0.053185</td>\n",
       "      <td>0.836339</td>\n",
       "      <td>0.589125</td>\n",
       "      <td>0.633400</td>\n",
       "      <td>0.017721</td>\n",
       "      <td>0.876009</td>\n",
       "      <td>0.727697</td>\n",
       "      <td>0.887257</td>\n",
       "      <td>0.854440</td>\n",
       "      <td>0.800105</td>\n",
       "      <td>0.532454</td>\n",
       "      <td>0.841865</td>\n",
       "      <td>0.800105</td>\n",
       "      <td>0.839768</td>\n",
       "      <td>0.817832</td>\n",
       "      <td>0.986912</td>\n",
       "      <td>0.954285</td>\n",
       "      <td>0.972114</td>\n",
       "      <td>0.986629</td>\n",
       "      <td>0.904592</td>\n",
       "      <td>0.633676</td>\n",
       "      <td>0.856445</td>\n",
       "      <td>0.934929</td>\n",
       "      <td>0.787740</td>\n",
       "      <td>0.466034</td>\n",
       "      <td>0.731383</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Training completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Fix tokenizer parallelism warning\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# üéØ OPTIMIZED METRICS CONFIGURATION FOR MULTI-LABEL CLASSIFICATION\n",
    "# Hamming Loss is the most appropriate metric for multi-label problems\n",
    "training_args = TrainingArguments(\n",
    "    # Output and logging\n",
    "    output_dir=\"./model_output\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    logging_strategy=\"steps\",\n",
    "    \n",
    "    # Learning parameters\n",
    "    learning_rate=2e-5,\n",
    "    lr_scheduler_type=\"linear\",  # Linear decay\n",
    "    warmup_ratio=0.1,  # 10% warmup\n",
    "    weight_decay=0.01,\n",
    "    \n",
    "    # Batch sizes (adjust based on GPU memory)\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=3,\n",
    "    gradient_accumulation_steps=24,  # Effective batch size = 2 * 24 = 48\n",
    "    \n",
    "    # Training epochs and evaluation\n",
    "    num_train_epochs=5,  # Increased for better convergence\n",
    "    eval_strategy=\"steps\",  # More frequent evaluation\n",
    "    eval_steps=100,  # Evaluate every 100 steps\n",
    "    \n",
    "    # üéØ OPTIMAL METRICS FOR MULTI-LABEL CLASSIFICATION\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    save_total_limit=3,  # Keep only 3 best checkpoints\n",
    "    load_best_model_at_end=True,\n",
    "    \n",
    "    # üî• RECOMMENDED: Use Hamming Loss for multi-label problems\n",
    "    metric_for_best_model=\"eval_hamming_loss\",  # Primary metric: lower is better\n",
    "    greater_is_better=False,  # Hamming loss: lower = better performance\n",
    "    \n",
    "    # Alternative good options:\n",
    "    # metric_for_best_model=\"eval_f1_micro\",     # Current choice - also excellent\n",
    "    # metric_for_best_model=\"eval_jaccard_samples\", # IoU metric - good for multi-label\n",
    "    \n",
    "    # Memory and performance optimization\n",
    "    dataloader_pin_memory=False,  # Disable to avoid forking issues\n",
    "    dataloader_num_workers=0,     # Disable multiprocessing\n",
    "    remove_unused_columns=False,  # Keep all columns for multi-label\n",
    "    \n",
    "    # Mixed precision for faster training (if GPU supports it)\n",
    "    fp16=True,  # Enable if using compatible GPU\n",
    "    \n",
    "    # Reproducibility\n",
    "    seed=42,\n",
    "    data_seed=42,\n",
    "    \n",
    "    # Report metrics\n",
    "    report_to=None,  # Disable wandb/tensorboard if not needed\n",
    "    run_name=\"multi_label_posture_classification\",\n",
    ")\n",
    "\n",
    "print(\"üéØ MULTI-LABEL METRICS CONFIGURATION:\")\n",
    "print(\"=\" * 60)\n",
    "print(\"‚úÖ PRIMARY METRIC: Hamming Loss (optimal for multi-label)\")\n",
    "print(\"   ‚Ä¢ Measures label-wise classification errors\")\n",
    "print(\"   ‚Ä¢ Range: 0.0 (perfect) to 1.0 (worst)\")\n",
    "print(\"   ‚Ä¢ Lower values = better performance\")\n",
    "print(\"   ‚Ä¢ More intuitive than F1 for legal document classification\")\n",
    "print()\n",
    "print(\"üìä MONITORING METRICS (all calculated):\")\n",
    "print(\"   ‚Ä¢ F1-Micro/Macro/Weighted: Overall performance\")\n",
    "print(\"   ‚Ä¢ Precision/Recall: Per-label quality\")\n",
    "print(\"   ‚Ä¢ ROC-AUC/PR-AUC: Ranking quality\")\n",
    "print(\"   ‚Ä¢ Jaccard Score: Label overlap similarity\")\n",
    "print(\"   ‚Ä¢ Accuracy: Exact match rate\")\n",
    "\n",
    "# Early stopping callback for overfitting control\n",
    "early_stopping = EarlyStoppingCallback(\n",
    "    early_stopping_patience=3,  # Stop if no improvement for 3 evaluations\n",
    "    early_stopping_threshold=0.001  # Minimum improvement threshold\n",
    ")\n",
    "\n",
    "# Initialize trainer with enhanced configuration (using processing_class)\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"val\"],\n",
    "    processing_class=tokenizer,  # Updated parameter name\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[early_stopping],  # Add early stopping callback\n",
    ")\n",
    "\n",
    "print(\"üöÄ Starting training with enhanced configuration...\")\n",
    "print(f\"üìä Training samples: {len(tokenized_dataset['train'])}\")\n",
    "print(f\"üìä Validation samples: {len(tokenized_dataset['val'])}\")\n",
    "print(f\"üéØ Target metric: {training_args.metric_for_best_model}\")\n",
    "print(f\"‚è±Ô∏è Total epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"üîÑ Evaluation every: {training_args.eval_steps} steps\")\n",
    "print(f\"üíæ Saving every: {training_args.save_steps} steps\")\n",
    "print(f\"‚èπÔ∏è Early stopping patience: {early_stopping.early_stopping_patience}\")\n",
    "\n",
    "# Start training with error handling\n",
    "try:\n",
    "    print(\"\\nüéØ Starting training...\")\n",
    "    trainer.train()\n",
    "    print(\"‚úÖ Training completed successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Training failed with error: {e}\")\n",
    "    print(\"üí° Consider:\")\n",
    "    print(\"   - Reducing batch size if out of memory\")\n",
    "    print(\"   - Checking data format compatibility\")\n",
    "    print(\"   - Verifying model and tokenizer compatibility\")\n",
    "    print(\"   - The data format may need fixing - check tokenization step\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9146ea84",
   "metadata": {},
   "source": [
    "### Inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "461d3f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = \"model_output/checkpoint-1000\"\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e81c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|‚ñç         | 14/311 [01:16<24:51,  5.02s/it]"
     ]
    }
   ],
   "source": [
    "# Prepare DataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "test_dataset=tokenized_dataset[\"test\"]\n",
    "test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "test_loader = DataLoader(test_dataset, batch_size=8)\n",
    "\n",
    "# Run inference\n",
    "model.eval()\n",
    "all_logits = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader,total=len(test_loader),leave=True,position=0):\n",
    "        input_ids = batch[\"input_ids\"].to(model.device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(model.device)\n",
    "        labels = batch[\"labels\"].cpu().numpy()\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits.cpu().numpy()\n",
    "        all_logits.append(logits)\n",
    "        all_labels.append(labels)\n",
    "\n",
    "\n",
    "# Concatenate all batches\n",
    "all_logits = np.concatenate(all_logits, axis=0)\n",
    "all_labels = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "# Apply sigmoid to get probabilities\n",
    "probas = torch.sigmoid(torch.tensor(all_logits)).numpy()\n",
    "\n",
    "# Apply threshold to get binary predictions\n",
    "threshold = 0.5\n",
    "predictions = (probas >= threshold).astype(int)\n",
    "\n",
    "results = {\n",
    "    \"f1_micro\": f1_score(all_labels, predictions, average=\"micro\"),\n",
    "    \"f1_macro\": f1_score(all_labels, predictions, average=\"macro\"),\n",
    "    \"accuracy\": accuracy_score(all_labels, predictions),\n",
    "    \"hamming_loss\": hamming_loss(all_labels, predictions),\n",
    "    \"precision_micro\": precision_score(all_labels, predictions, average=\"micro\"),\n",
    "    \"recall_micro\": recall_score(all_labels, predictions, average=\"micro\"),\n",
    "    \"jaccard_weighted\": jaccard_score(all_labels, predictions, average=\"weighted\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1697a2df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0, 19, 20, ..., 20, 24, 20], shape=(2486,)),\n",
       " array([[1., 0., 0., ..., 0., 0., 1.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 1., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 1., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], shape=(2486, 27)))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3bec047e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in comprehensive_evaluation: Classification metrics can't handle a mix of multilabel-indicator and binary targets\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'precision_samples'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m predictions = np.array(predictions)\n\u001b[32m      3\u001b[39m eval_pred=(predictions, labels)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mcompute_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_pred\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 38\u001b[39m, in \u001b[36mcompute_metrics\u001b[39m\u001b[34m(eval_pred)\u001b[39m\n\u001b[32m     20\u001b[39m metrics = comprehensive_evaluation(\n\u001b[32m     21\u001b[39m     y_true=labels,\n\u001b[32m     22\u001b[39m     y_pred_proba=predictions_proba,\n\u001b[32m     23\u001b[39m     y_pred_binary=predictions_binary,\n\u001b[32m     24\u001b[39m     threshold=\u001b[32m0.5\u001b[39m\n\u001b[32m     25\u001b[39m )\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Return metrics with eval_ prefix for Trainer compatibility\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m     29\u001b[39m     \u001b[38;5;66;03m# Primary metrics for monitoring\u001b[39;00m\n\u001b[32m     30\u001b[39m     \u001b[33m'\u001b[39m\u001b[33meval_f1_micro\u001b[39m\u001b[33m'\u001b[39m: metrics[\u001b[33m'\u001b[39m\u001b[33mf1_micro\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m     31\u001b[39m     \u001b[33m'\u001b[39m\u001b[33meval_f1_macro\u001b[39m\u001b[33m'\u001b[39m: metrics[\u001b[33m'\u001b[39m\u001b[33mf1_macro\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m     32\u001b[39m     \u001b[33m'\u001b[39m\u001b[33meval_accuracy\u001b[39m\u001b[33m'\u001b[39m: metrics[\u001b[33m'\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m     33\u001b[39m     \u001b[33m'\u001b[39m\u001b[33meval_hamming_loss\u001b[39m\u001b[33m'\u001b[39m: metrics[\u001b[33m'\u001b[39m\u001b[33mhamming_loss\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m     34\u001b[39m \n\u001b[32m     35\u001b[39m     \u001b[38;5;66;03m# Precision metrics\u001b[39;00m\n\u001b[32m     36\u001b[39m     \u001b[33m'\u001b[39m\u001b[33meval_precision_micro\u001b[39m\u001b[33m'\u001b[39m: metrics[\u001b[33m'\u001b[39m\u001b[33mprecision_micro\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m     37\u001b[39m     \u001b[33m'\u001b[39m\u001b[33meval_precision_macro\u001b[39m\u001b[33m'\u001b[39m: metrics[\u001b[33m'\u001b[39m\u001b[33mprecision_macro\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m     \u001b[33m'\u001b[39m\u001b[33meval_precision_samples\u001b[39m\u001b[33m'\u001b[39m: \u001b[43mmetrics\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mprecision_samples\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m,\n\u001b[32m     39\u001b[39m     \u001b[33m'\u001b[39m\u001b[33meval_precision_weighted\u001b[39m\u001b[33m'\u001b[39m: metrics[\u001b[33m'\u001b[39m\u001b[33mprecision_weighted\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m     40\u001b[39m \n\u001b[32m     41\u001b[39m     \u001b[38;5;66;03m# Recall metrics\u001b[39;00m\n\u001b[32m     42\u001b[39m     \u001b[33m'\u001b[39m\u001b[33meval_recall_micro\u001b[39m\u001b[33m'\u001b[39m: metrics[\u001b[33m'\u001b[39m\u001b[33mrecall_micro\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m     43\u001b[39m     \u001b[33m'\u001b[39m\u001b[33meval_recall_macro\u001b[39m\u001b[33m'\u001b[39m: metrics[\u001b[33m'\u001b[39m\u001b[33mrecall_macro\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m     44\u001b[39m     \u001b[33m'\u001b[39m\u001b[33meval_recall_samples\u001b[39m\u001b[33m'\u001b[39m: metrics[\u001b[33m'\u001b[39m\u001b[33mrecall_samples\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m     45\u001b[39m     \u001b[33m'\u001b[39m\u001b[33meval_recall_weighted\u001b[39m\u001b[33m'\u001b[39m: metrics[\u001b[33m'\u001b[39m\u001b[33mrecall_weighted\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m     46\u001b[39m \n\u001b[32m     47\u001b[39m     \u001b[38;5;66;03m# F1 metrics\u001b[39;00m\n\u001b[32m     48\u001b[39m     \u001b[33m'\u001b[39m\u001b[33meval_f1_samples\u001b[39m\u001b[33m'\u001b[39m: metrics[\u001b[33m'\u001b[39m\u001b[33mf1_samples\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m     49\u001b[39m     \u001b[33m'\u001b[39m\u001b[33meval_f1_weighted\u001b[39m\u001b[33m'\u001b[39m: metrics[\u001b[33m'\u001b[39m\u001b[33mf1_weighted\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m     50\u001b[39m \n\u001b[32m     51\u001b[39m     \u001b[38;5;66;03m# ROC-AUC metrics\u001b[39;00m\n\u001b[32m     52\u001b[39m     \u001b[33m'\u001b[39m\u001b[33meval_roc_auc_micro\u001b[39m\u001b[33m'\u001b[39m: metrics[\u001b[33m'\u001b[39m\u001b[33mroc_auc_micro\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m     53\u001b[39m     \u001b[33m'\u001b[39m\u001b[33meval_roc_auc_macro\u001b[39m\u001b[33m'\u001b[39m: metrics[\u001b[33m'\u001b[39m\u001b[33mroc_auc_macro\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m     54\u001b[39m     \u001b[33m'\u001b[39m\u001b[33meval_roc_auc_weighted\u001b[39m\u001b[33m'\u001b[39m: metrics[\u001b[33m'\u001b[39m\u001b[33mroc_auc_weighted\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m     55\u001b[39m     \u001b[33m'\u001b[39m\u001b[33meval_roc_auc_samples\u001b[39m\u001b[33m'\u001b[39m: metrics[\u001b[33m'\u001b[39m\u001b[33mroc_auc_samples\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m     56\u001b[39m \n\u001b[32m     57\u001b[39m     \u001b[38;5;66;03m# PR-AUC metrics\u001b[39;00m\n\u001b[32m     58\u001b[39m     \u001b[33m'\u001b[39m\u001b[33meval_pr_auc_micro\u001b[39m\u001b[33m'\u001b[39m: metrics[\u001b[33m'\u001b[39m\u001b[33mpr_auc_micro\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m     59\u001b[39m     \u001b[33m'\u001b[39m\u001b[33meval_pr_auc_macro\u001b[39m\u001b[33m'\u001b[39m: metrics[\u001b[33m'\u001b[39m\u001b[33mpr_auc_macro\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m     60\u001b[39m     \u001b[33m'\u001b[39m\u001b[33meval_pr_auc_weighted\u001b[39m\u001b[33m'\u001b[39m: metrics[\u001b[33m'\u001b[39m\u001b[33mpr_auc_weighted\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m     61\u001b[39m     \u001b[33m'\u001b[39m\u001b[33meval_pr_auc_samples\u001b[39m\u001b[33m'\u001b[39m: metrics[\u001b[33m'\u001b[39m\u001b[33mpr_auc_samples\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m     62\u001b[39m \n\u001b[32m     63\u001b[39m     \u001b[38;5;66;03m# Jaccard metrics\u001b[39;00m\n\u001b[32m     64\u001b[39m     \u001b[33m'\u001b[39m\u001b[33meval_jaccard_samples\u001b[39m\u001b[33m'\u001b[39m: metrics[\u001b[33m'\u001b[39m\u001b[33mjaccard_samples\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m     65\u001b[39m     \u001b[33m'\u001b[39m\u001b[33meval_jaccard_macro\u001b[39m\u001b[33m'\u001b[39m: metrics[\u001b[33m'\u001b[39m\u001b[33mjaccard_macro\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m     66\u001b[39m     \u001b[33m'\u001b[39m\u001b[33meval_jaccard_weighted\u001b[39m\u001b[33m'\u001b[39m: metrics[\u001b[33m'\u001b[39m\u001b[33mjaccard_weighted\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m     67\u001b[39m }\n",
      "\u001b[31mKeyError\u001b[39m: 'precision_samples'"
     ]
    }
   ],
   "source": [
    "labels = np.array(test_dataset[\"labels\"])\n",
    "predictions = np.array(predictions)\n",
    "eval_pred=(predictions, labels)\n",
    "compute_metrics(eval_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1b25da0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['labels', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 2486\n",
       "})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968557ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00607921",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([50281,  7307,    18,  ..., 20461,    56, 50282]),\n",
       " 'attention_mask': tensor([1, 1, 1,  ..., 1, 1, 1])}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f05447e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç COMPREHENSIVE MODEL EVALUATION\n",
      "============================================================\n",
      "\n",
      "üìä Validation Set Evaluation:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1658' max='829' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [829/829 02:08]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   eval_f1_micro: 0.8393\n",
      "   eval_f1_macro: 0.5943\n",
      "   eval_accuracy: 0.6354\n",
      "   eval_hamming_loss: 0.0176\n",
      "   eval_precision_micro: 0.8682\n",
      "   eval_recall_micro: 0.8122\n",
      "   eval_roc_auc_macro: 0.9561\n"
     ]
    }
   ],
   "source": [
    "# Post-Training Evaluation and Testing\n",
    "\n",
    "print(\"üîç COMPREHENSIVE MODEL EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Evaluate on validation set\n",
    "print(\"\\nüìä Validation Set Evaluation:\")\n",
    "val_results = trainer.evaluate()\n",
    "\n",
    "# Display key metrics\n",
    "key_metrics = [\n",
    "    'eval_f1_micro', 'eval_f1_macro', 'eval_accuracy', 'eval_hamming_loss',\n",
    "    'eval_precision_micro', 'eval_recall_micro', 'eval_roc_auc_macro'\n",
    "]\n",
    "\n",
    "for metric in key_metrics:\n",
    "    if metric in val_results:\n",
    "        print(f\"   {metric}: {val_results[metric]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7bc3bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ Test Set Evaluation:\n",
      "   eval_f1_micro: 0.8338\n",
      "   eval_f1_macro: 0.5600\n",
      "   eval_accuracy: 0.6307\n",
      "   eval_hamming_loss: 0.0183\n",
      "   eval_precision_micro: 0.8550\n",
      "   eval_recall_micro: 0.8136\n",
      "   eval_roc_auc_macro: 0.9555\n"
     ]
    }
   ],
   "source": [
    "# Test on test set if available\n",
    "if \"test\" in tokenized_dataset:\n",
    "    print(\"\\nüéØ Test Set Evaluation:\")\n",
    "    test_results = trainer.evaluate(eval_dataset=tokenized_dataset[\"test\"])\n",
    "    \n",
    "    for metric in key_metrics:\n",
    "        if metric in test_results:\n",
    "            print(f\"   {metric}: {test_results[metric]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a304a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842c3469",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Get predictions for detailed analysis\n",
    "# print(\"\\nüî¨ Detailed Prediction Analysis:\")\n",
    "\n",
    "# # Predict on validation set\n",
    "# val_predictions = trainer.predict(tokenized_dataset[\"val\"])\n",
    "# val_probs = sigmoid(val_predictions.predictions)\n",
    "# val_binary = (val_probs > 0.5).astype(int)\n",
    "# val_true = val_predictions.label_ids\n",
    "\n",
    "# # Use comprehensive evaluation function\n",
    "# detailed_metrics = comprehensive_evaluation(\n",
    "#     y_true=val_true,\n",
    "#     y_pred_proba=val_probs,\n",
    "#     y_pred_binary=val_binary\n",
    "# )\n",
    "\n",
    "# print(\"\\nüìà Comprehensive Metrics Summary:\")\n",
    "# print(\"-\" * 50)\n",
    "\n",
    "# # Group metrics by type\n",
    "# metric_groups = {\n",
    "#     'Precision': ['precision_micro', 'precision_macro', 'precision_samples', 'precision_weighted'],\n",
    "#     'Recall': ['recall_micro', 'recall_macro', 'recall_samples', 'recall_weighted'],\n",
    "#     'F1-Score': ['f1_micro', 'f1_macro', 'f1_samples', 'f1_weighted'],\n",
    "#     'ROC-AUC': ['roc_auc_macro', 'roc_auc_weighted', 'roc_auc_samples'],\n",
    "#     'PR-AUC': ['pr_auc_macro', 'pr_auc_weighted', 'pr_auc_samples'],\n",
    "#     'Other': ['accuracy', 'hamming_loss', 'jaccard_macro', 'jaccard_samples']\n",
    "# }\n",
    "\n",
    "# for group_name, metrics in metric_groups.items():\n",
    "#     print(f\"\\n{group_name}:\")\n",
    "#     for metric in metrics:\n",
    "#         if metric in detailed_metrics:\n",
    "#             print(f\"   {metric}: {detailed_metrics[metric]:.4f}\")\n",
    "\n",
    "# # Sample predictions analysis\n",
    "# print(\"\\nüîç Sample Predictions Analysis:\")\n",
    "# sample_size = min(5, len(val_true))\n",
    "# for i in range(sample_size):\n",
    "#     print(f\"\\nSample {i+1}:\")\n",
    "#     print(f\"   True labels: {val_true[i]}\")\n",
    "#     print(f\"   Predicted:   {val_binary[i]}\")\n",
    "#     print(f\"   Probabilities: {val_probs[i]}\")\n",
    "#     print(f\"   Match: {'‚úÖ' if np.array_equal(val_true[i], val_binary[i]) else '‚ùå'}\")\n",
    "\n",
    "# # Model performance summary\n",
    "# print(f\"\\n{'='*60}\")\n",
    "# print(\"üèÜ MODEL PERFORMANCE SUMMARY\")\n",
    "# print(f\"{'='*60}\")\n",
    "# print(f\"‚úÖ Best Metric (F1-Micro): {detailed_metrics['f1_micro']:.4f}\")\n",
    "# print(f\"üìä Accuracy: {detailed_metrics['accuracy']:.4f}\")\n",
    "# print(f\"üîª Hamming Loss: {detailed_metrics['hamming_loss']:.4f}\")\n",
    "# print(f\"üéØ Macro F1: {detailed_metrics['f1_macro']:.4f}\")\n",
    "\n",
    "# if detailed_metrics['f1_micro'] > 0.7:\n",
    "#     print(\"üéâ Excellent performance! Model is ready for deployment.\")\n",
    "# elif detailed_metrics['f1_micro'] > 0.5:\n",
    "#     print(\"üëç Good performance! Consider fine-tuning for better results.\")\n",
    "# else:\n",
    "#     print(\"‚ö†Ô∏è Performance needs improvement. Consider:\")\n",
    "#     print(\"   - More training epochs\")\n",
    "#     print(\"   - Different learning rate\")\n",
    "#     print(\"   - Data augmentation\")\n",
    "#     print(\"   - Different model architecture\")\n",
    "\n",
    "# print(f\"\\nüíæ Model saved to: {training_args.output_dir}\")\n",
    "# print(\"üöÄ Training and evaluation completed successfully!\")# Enhanced Training Configuration for Multi-label Classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e406e206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Post-Training Evaluation and Testing\n",
    "\n",
    "# print(\"üîç COMPREHENSIVE MODEL EVALUATION\")\n",
    "# print(\"=\" * 60)\n",
    "\n",
    "# # Evaluate on validation set\n",
    "# print(\"\\nüìä Validation Set Evaluation:\")\n",
    "# val_results = trainer.evaluate()\n",
    "\n",
    "# # Display key metrics\n",
    "# key_metrics = [\n",
    "#     'eval_f1_micro', 'eval_f1_macro', 'eval_accuracy', 'eval_hamming_loss',\n",
    "#     'eval_precision_micro', 'eval_recall_micro', 'eval_roc_auc_macro'\n",
    "# ]\n",
    "\n",
    "# for metric in key_metrics:\n",
    "#     if metric in val_results:\n",
    "#         print(f\"   {metric}: {val_results[metric]:.4f}\")\n",
    "\n",
    "# # Test on test set if available\n",
    "# if \"test\" in tokenized_dataset:\n",
    "#     print(\"\\nüéØ Test Set Evaluation:\")\n",
    "#     test_results = trainer.evaluate(eval_dataset=tokenized_dataset[\"test\"])\n",
    "    \n",
    "#     for metric in key_metrics:\n",
    "#         if metric in test_results:\n",
    "#             print(f\"   {metric}: {test_results[metric]:.4f}\")\n",
    "\n",
    "# # Get predictions for detailed analysis\n",
    "# print(\"\\nüî¨ Detailed Prediction Analysis:\")\n",
    "\n",
    "# # Predict on validation set\n",
    "# val_predictions = trainer.predict(tokenized_dataset[\"val\"])\n",
    "# val_probs = sigmoid(val_predictions.predictions)\n",
    "# val_binary = (val_probs > 0.5).astype(int)\n",
    "# val_true = val_predictions.label_ids\n",
    "\n",
    "# # Use comprehensive evaluation function\n",
    "# detailed_metrics = comprehensive_evaluation(\n",
    "#     y_true=val_true,\n",
    "#     y_pred_proba=val_probs,\n",
    "#     y_pred_binary=val_binary\n",
    "# )\n",
    "\n",
    "# print(\"\\nüìà Comprehensive Metrics Summary:\")\n",
    "# print(\"-\" * 50)\n",
    "\n",
    "# # Group metrics by type\n",
    "# metric_groups = {\n",
    "#     'Precision': ['precision_micro', 'precision_macro', 'precision_samples', 'precision_weighted'],\n",
    "#     'Recall': ['recall_micro', 'recall_macro', 'recall_samples', 'recall_weighted'],\n",
    "#     'F1-Score': ['f1_micro', 'f1_macro', 'f1_samples', 'f1_weighted'],\n",
    "#     'ROC-AUC': ['roc_auc_macro', 'roc_auc_weighted', 'roc_auc_samples'],\n",
    "#     'PR-AUC': ['pr_auc_macro', 'pr_auc_weighted', 'pr_auc_samples'],\n",
    "#     'Other': ['accuracy', 'hamming_loss', 'jaccard_macro', 'jaccard_samples']\n",
    "# }\n",
    "\n",
    "# for group_name, metrics in metric_groups.items():\n",
    "#     print(f\"\\n{group_name}:\")\n",
    "#     for metric in metrics:\n",
    "#         if metric in detailed_metrics:\n",
    "#             print(f\"   {metric}: {detailed_metrics[metric]:.4f}\")\n",
    "\n",
    "# # Sample predictions analysis\n",
    "# print(\"\\nüîç Sample Predictions Analysis:\")\n",
    "# sample_size = min(5, len(val_true))\n",
    "# for i in range(sample_size):\n",
    "#     print(f\"\\nSample {i+1}:\")\n",
    "#     print(f\"   True labels: {val_true[i]}\")\n",
    "#     print(f\"   Predicted:   {val_binary[i]}\")\n",
    "#     print(f\"   Probabilities: {val_probs[i]}\")\n",
    "#     print(f\"   Match: {'‚úÖ' if np.array_equal(val_true[i], val_binary[i]) else '‚ùå'}\")\n",
    "\n",
    "# # Model performance summary\n",
    "# print(f\"\\n{'='*60}\")\n",
    "# print(\"üèÜ MODEL PERFORMANCE SUMMARY\")\n",
    "# print(f\"{'='*60}\")\n",
    "# print(f\"‚úÖ Best Metric (F1-Micro): {detailed_metrics['f1_micro']:.4f}\")\n",
    "# print(f\"üìä Accuracy: {detailed_metrics['accuracy']:.4f}\")\n",
    "# print(f\"üîª Hamming Loss: {detailed_metrics['hamming_loss']:.4f}\")\n",
    "# print(f\"üéØ Macro F1: {detailed_metrics['f1_macro']:.4f}\")\n",
    "\n",
    "# if detailed_metrics['f1_micro'] > 0.7:\n",
    "#     print(\"üéâ Excellent performance! Model is ready for deployment.\")\n",
    "# elif detailed_metrics['f1_micro'] > 0.5:\n",
    "#     print(\"üëç Good performance! Consider fine-tuning for better results.\")\n",
    "# else:\n",
    "#     print(\"‚ö†Ô∏è Performance needs improvement. Consider:\")\n",
    "#     print(\"   - More training epochs\")\n",
    "#     print(\"   - Different learning rate\")\n",
    "#     print(\"   - Data augmentation\")\n",
    "#     print(\"   - Different model architecture\")\n",
    "\n",
    "# print(f\"\\nüíæ Model saved to: {training_args.output_dir}\")\n",
    "# print(\"üöÄ Training and evaluation completed successfully!\")# Enhanced Training Configuration for Multi-label Classification\n",
    "# training_args = TrainingArguments(\n",
    "#     # Output and logging\n",
    "#     output_dir=\"./model_output\",\n",
    "#     logging_dir=\"./logs\",\n",
    "#     logging_steps=50,\n",
    "#     logging_strategy=\"steps\",\n",
    "    \n",
    "#     # Learning parameters\n",
    "#     learning_rate=2e-5,\n",
    "#     lr_scheduler_type=\"linear\",  # Linear decay\n",
    "#     warmup_ratio=0.1,  # 10% warmup\n",
    "#     weight_decay=0.01,\n",
    "    \n",
    "#     # Batch sizes (adjust based on GPU memory)\n",
    "#     per_device_train_batch_size=3,\n",
    "#     per_device_eval_batch_size=3,\n",
    "#     gradient_accumulation_steps=4,  # Effective batch size = 3 * 4 = 12\n",
    "    \n",
    "#     # Training epochs and evaluation\n",
    "#     num_train_epochs=3,  # Increased for better convergence\n",
    "#     eval_strategy=\"steps\",  # More frequent evaluation\n",
    "#     eval_steps=100,  # Evaluate every 100 steps\n",
    "    \n",
    "#     # Saving strategy\n",
    "#     save_strategy=\"steps\",\n",
    "#     save_steps=100,\n",
    "#     save_total_limit=3,  # Keep only 3 best checkpoints\n",
    "#     load_best_model_at_end=True,\n",
    "#     metric_for_best_model=\"eval_f1_micro\",  # Use micro F1 for model selection\n",
    "#     greater_is_better=True,\n",
    "    \n",
    "#     # Early stopping and overfitting control\n",
    "#     early_stopping_patience=3,  # Stop if no improvement for 3 evaluations\n",
    "    \n",
    "#     # Memory and performance optimization\n",
    "#     dataloader_pin_memory=True,\n",
    "#     dataloader_num_workers=2,\n",
    "#     remove_unused_columns=False,  # Keep all columns for multi-label\n",
    "    \n",
    "#     # Mixed precision for faster training (if GPU supports it)\n",
    "#     fp16=True,  # Enable if using compatible GPU\n",
    "    \n",
    "#     # Reproducibility\n",
    "#     seed=42,\n",
    "#     data_seed=42,\n",
    "    \n",
    "#     # Report metrics\n",
    "#     report_to=None,  # Disable wandb/tensorboard if not needed\n",
    "#     run_name=\"multi_label_posture_classification\",\n",
    "# )\n",
    "\n",
    "# # Initialize trainer with enhanced configuration\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=tokenized_dataset[\"train\"],\n",
    "#     eval_dataset=tokenized_dataset[\"val\"],\n",
    "#     tokenizer=tokenizer,\n",
    "#     data_collator=data_collator,\n",
    "#     compute_metrics=compute_metrics,\n",
    "# )\n",
    "\n",
    "# print(\"üöÄ Starting training with enhanced configuration...\")\n",
    "# print(f\"üìä Training samples: {len(tokenized_dataset['train'])}\")\n",
    "# print(f\"üìä Validation samples: {len(tokenized_dataset['val'])}\")\n",
    "# print(f\"üéØ Target metric: {training_args.metric_for_best_model}\")\n",
    "# print(f\"‚è±Ô∏è Total epochs: {training_args.num_train_epochs}\")\n",
    "# print(f\"üîÑ Evaluation every: {training_args.eval_steps} steps\")\n",
    "# print(f\"üíæ Saving every: {training_args.save_steps} steps\")\n",
    "\n",
    "# # Start training\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800059bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f37e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # üîß EXPLICIT LABEL TYPE CONVERSION\n",
    "# # Convert labels to float32 using HuggingFace datasets features\n",
    "\n",
    "# from datasets import Sequence, Value\n",
    "# import torch\n",
    "\n",
    "# print(\"üîÑ Converting labels to float32 using datasets.cast_column...\")\n",
    "\n",
    "# # Define the proper feature type for multi-label classification\n",
    "# # Labels should be a sequence of floats (one per class)\n",
    "# label_feature = Sequence(Value(\"float32\"), length=len(class_name))\n",
    "\n",
    "# # Cast the labels column to float32 for all splits\n",
    "# for split_name in tokenized_dataset.keys():\n",
    "#     tokenized_dataset[split_name] = tokenized_dataset[split_name].cast_column(\"labels\", label_feature)\n",
    "\n",
    "# # Verify the fix\n",
    "# print(f\"\\n‚úÖ Labels conversion verification:\")\n",
    "# sample = tokenized_dataset[\"train\"][0]\n",
    "# sample_labels = np.array(sample['labels'])\n",
    "# print(f\"  Labels dtype: {sample_labels.dtype}\")\n",
    "# print(f\"  Labels shape: {sample_labels.shape}\")\n",
    "# print(f\"  Sample labels: {sample['labels'][:5]}...\")  # Show first 5 labels\n",
    "# print(f\"  HF Feature type: {tokenized_dataset['train'].features['labels']}\")\n",
    "\n",
    "# # Test tensor conversion\n",
    "# test_labels = torch.tensor(sample['labels'], dtype=torch.float32)\n",
    "# print(f\"  PyTorch tensor dtype: {test_labels.dtype}\")\n",
    "# print(f\"  PyTorch tensor shape: {test_labels.shape}\")\n",
    "\n",
    "# if sample_labels.dtype == np.float32:\n",
    "#     print(\"‚úÖ SUCCESS: Labels are now properly formatted as float32\")\n",
    "#     print(\"üöÄ Ready for training!\")\n",
    "# else:\n",
    "#     print(f\"‚ùå ISSUE: Labels are still {sample_labels.dtype}, expected float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2f726e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # üéâ FINAL MODEL EVALUATION\n",
    "# # Comprehensive evaluation of the trained multi-label classification model\n",
    "\n",
    "# import torch\n",
    "# from sklearn.metrics import classification_report\n",
    "# import numpy as np\n",
    "\n",
    "# print(\"üî¨ FINAL MODEL EVALUATION\")\n",
    "# print(\"=\" * 60)\n",
    "\n",
    "# # First, let's check the test set data types and fix if needed\n",
    "# print(\"üîç Checking test set data types...\")\n",
    "# test_sample = tokenized_dataset[\"test\"][0]\n",
    "# test_labels = np.array(test_sample['labels'])\n",
    "# print(f\"Test labels dtype: {test_labels.dtype}\")\n",
    "\n",
    "# if test_labels.dtype != np.float32:\n",
    "#     print(\"‚ö†Ô∏è Test set labels need conversion, performing conversion...\")\n",
    "#     # Re-apply the label conversion to test set\n",
    "#     from datasets import Sequence, Value\n",
    "#     label_feature = Sequence(Value(\"float32\"), length=len(class_name))\n",
    "#     tokenized_dataset[\"test\"] = tokenized_dataset[\"test\"].cast_column(\"labels\", label_feature)\n",
    "#     print(\"‚úÖ Test set labels converted to float32\")\n",
    "\n",
    "# # Use predict method instead of evaluate to avoid evaluation issues\n",
    "# print(\"üìä Generating predictions on test set...\")\n",
    "# predictions = trainer.predict(tokenized_dataset[\"test\"])\n",
    "\n",
    "# # Convert predictions to probabilities and binary predictions\n",
    "# y_pred_proba = torch.sigmoid(torch.tensor(predictions.predictions)).numpy()\n",
    "# y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "# y_true = predictions.label_ids.astype(int)\n",
    "\n",
    "# print(f\"Prediction shape: {y_pred.shape}\")\n",
    "# print(f\"True labels shape: {y_true.shape}\")\n",
    "\n",
    "# # Calculate comprehensive metrics manually using our evaluation function\n",
    "# # Note: Fix the function call order - comprehensive_evaluation(y_true, y_pred_proba, y_pred_binary)\n",
    "# print(\"üìä Calculating comprehensive metrics...\")\n",
    "# detailed_metrics = comprehensive_evaluation(y_true, y_pred_proba, y_pred_binary=y_pred)\n",
    "\n",
    "# print(f\"\\nüèÜ TEST SET RESULTS:\")\n",
    "# print(f\"{'='*50}\")\n",
    "\n",
    "# # Print all the comprehensive metrics\n",
    "# metric_groups = {\n",
    "#     \"üìà Primary Metrics\": [\"f1_micro\", \"f1_macro\", \"f1_weighted\", \"f1_samples\"],\n",
    "#     \"üéØ Precision\": [\"precision_micro\", \"precision_macro\", \"precision_weighted\", \"precision_samples\"],\n",
    "#     \"üîç Recall\": [\"recall_micro\", \"recall_macro\", \"recall_weighted\", \"recall_samples\"],\n",
    "#     \"üìä Other Metrics\": [\"accuracy\", \"hamming_loss\", \"jaccard_samples\", \"jaccard_macro\", \"jaccard_weighted\"],\n",
    "#     \"üì° AUC Metrics\": [\"roc_auc_macro\", \"roc_auc_weighted\", \"roc_auc_samples\", \n",
    "#                        \"pr_auc_macro\", \"pr_auc_weighted\", \"pr_auc_samples\"]\n",
    "# }\n",
    "\n",
    "# for group_name, metrics in metric_groups.items():\n",
    "#     print(f\"\\n{group_name}:\")\n",
    "#     for metric in metrics:\n",
    "#         if metric in detailed_metrics:\n",
    "#             print(f\"  {metric.upper()}: {detailed_metrics[metric]:.4f}\")\n",
    "\n",
    "# # Per-class performance\n",
    "# print(f\"\\nüìã PER-CLASS PERFORMANCE:\")\n",
    "# print(f\"{'='*50}\")\n",
    "# class_report = classification_report(\n",
    "#     y_true, y_pred, \n",
    "#     target_names=class_name, \n",
    "#     output_dict=True,\n",
    "#     zero_division=0\n",
    "# )\n",
    "\n",
    "# # Show performance for each class\n",
    "# for i, class_label in enumerate(class_name):\n",
    "#     if class_label in class_report:\n",
    "#         metrics = class_report[class_label]\n",
    "#         support = int(metrics['support'])\n",
    "#         print(f\"{class_label:30s} | P: {metrics['precision']:.3f} | R: {metrics['recall']:.3f} | F1: {metrics['f1-score']:.3f} | Support: {support:4d}\")\n",
    "\n",
    "# # Overall summary\n",
    "# print(f\"\\nüéØ OVERALL PERFORMANCE SUMMARY:\")\n",
    "# print(f\"{'='*50}\")\n",
    "# macro_avg = class_report['macro avg']\n",
    "# weighted_avg = class_report['weighted avg']\n",
    "\n",
    "# print(f\"üîπ Macro Average    | P: {macro_avg['precision']:.3f} | R: {macro_avg['recall']:.3f} | F1: {macro_avg['f1-score']:.3f}\")\n",
    "# print(f\"üîπ Weighted Average | P: {weighted_avg['precision']:.3f} | R: {weighted_avg['recall']:.3f} | F1: {weighted_avg['f1-score']:.3f}\")\n",
    "\n",
    "# # Performance assessment\n",
    "# f1_micro = detailed_metrics.get('f1_micro', 0)\n",
    "# print(f\"\\nüèÜ FINAL ASSESSMENT:\")\n",
    "# print(f\"{'='*50}\")\n",
    "# if f1_micro > 0.8:\n",
    "#     assessment = \"üåü EXCELLENT! Model shows outstanding performance.\"\n",
    "# elif f1_micro > 0.7:\n",
    "#     assessment = \"‚úÖ VERY GOOD! Model performance is strong and ready for deployment.\"\n",
    "# elif f1_micro > 0.6:\n",
    "#     assessment = \"üëç GOOD! Model shows solid performance with room for improvement.\"\n",
    "# elif f1_micro > 0.5:\n",
    "#     assessment = \"‚ö†Ô∏è MODERATE! Consider additional training or data improvements.\"\n",
    "# else:\n",
    "#     assessment = \"‚ùå NEEDS IMPROVEMENT! Significant enhancements required.\"\n",
    "\n",
    "# print(f\"Micro F1 Score: {f1_micro:.4f}\")\n",
    "# print(f\"Assessment: {assessment}\")\n",
    "\n",
    "# print(f\"\\nüíæ Model and results saved to: {training_args.output_dir}\")\n",
    "# print(f\"üéâ Multi-label legal posture classification training completed successfully!\")\n",
    "\n",
    "# # Save the best model explicitly\n",
    "# print(f\"\\nüíæ Saving final model...\")\n",
    "# trainer.save_model(f\"{training_args.output_dir}/final_model\")\n",
    "# tokenizer.save_pretrained(f\"{training_args.output_dir}/final_model\")\n",
    "# print(f\"‚úÖ Final model saved to: {training_args.output_dir}/final_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20ff6c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2614be9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6a487eb0",
   "metadata": {},
   "source": [
    "<span style=\"font-weight: bold; font-size: 18px;\">\n",
    "Since the text length in the corpus is quite long, even though the language model can handle a maximum input of 8,092 tokens or even longer context, such lengthy contexts can dilute the essential information that the LLM is able to process effectively. When the input text is too long, the model may struggle to focus on the most relevant details, which can negatively impact its performance and the quality of its predictions.\n",
    "\n",
    "To address this challenge, it is beneficial to provide the language model with only the most relevant information from the large corpus, rather than overwhelming it with the entire text. By filtering and condensing the input, we can help the LLM focus on the critical content, thereby enhancing its ability to extract essential information and make more accurate predictions.\n",
    "\n",
    "There are two main approaches to achieve this:\n",
    "\n",
    "<div style=\"margin-left: 20px;\"><b>‚Ä¢ Summarization:</b> Summarization techniques can be used as a form of feature extraction. By generating concise summaries of the original text, we can distill the most important points and reduce the input length, making it easier for the LLM to process and understand the core information.</div> <div style=\"margin-left: 20px;\"><b>‚Ä¢ Retrieval-Augmented Generation (RAG):</b> This approach involves retrieving relevant information from the corpus before passing it to the language model. Techniques such as semantic search or keyword-based search (e.g., BM25) can be used to identify and extract the most pertinent sections of text. The retrieved content is then fed into the LLM, ensuring that the model receives focused and contextually relevant information for prediction.</div>\n",
    "By applying either summarization or retrieval-augmented generation, we can significantly improve the efficiency and effectiveness of language models when dealing with large and complex corpora. This targeted approach helps prevent information overload and allows the model to generate more accurate and meaningful outputs.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba87f2c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
