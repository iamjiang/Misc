{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a830a948",
   "metadata": {},
   "source": [
    "<span style=\"font-weight: bold; font-size: 18px;\">\n",
    "<p>\n",
    "<b>Since the text length in the corpus is quite long, even though the language model can handle a maximum input of 8,092 tokens or even longer context, such lengthy contexts can dilute the essential information that the LLM is able to process effectively. When the input text is too long, the model may struggle to focus on the most relevant details, which can negatively impact its performance and the quality of its predictions.</b>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "To address this challenge, it is beneficial to provide the language model with only the most relevant information from the large corpus, rather than overwhelming it with the entire text. By filtering and condensing the input, we can help the LLM focus on the critical content, thereby enhancing its ability to extract essential information and make more accurate predictions.\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "There are two main approaches to achieve this:<br>\n",
    "<div style=\"margin-left: 20px;\"><b>• Summarization:</b> Summarization techniques can be used as a form of feature extraction. By generating concise summaries of the original text, we can distill the most important points and reduce the input length, making it easier for the LLM to process and understand the core information.</div><br>\n",
    "<div style=\"margin-left: 20px;\"><b>• Retrieval-Augmented Generation (RAG):</b> This approach involves retrieving relevant information from the corpus before passing it to the language model. Techniques such as semantic search or keyword-based search (e.g., BM25) can be used to identify and extract the most pertinent sections of text. The retrieved content is then fed into the LLM, ensuring that the model receives focused and contextually relevant information for prediction.</div>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "By applying either summarization or retrieval-augmented generation, we can significantly improve the efficiency and effectiveness of language models when dealing with large and complex corpora. This targeted approach helps prevent information overload and allows the model to generate more accurate and meaningful outputs.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c20019",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b288bc03",
   "metadata": {},
   "source": [
    "## Summarization + Multi-label classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5b37af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
