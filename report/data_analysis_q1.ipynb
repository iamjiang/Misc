{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ea407cf",
   "metadata": {},
   "source": [
    "# TR Data Challenge 2023 - Data Analysis\n",
    "\n",
    "This notebook reads and analyzes the TRDataChallenge2023.txt file containing legal documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe05c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ab5d71",
   "metadata": {},
   "source": [
    "## 1. Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f3c527",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tr_data(file_path='/mnt/d/TR-Project/TRDataChallenge2023.txt'):\n",
    "    \"\"\"\n",
    "    Load TR data from the text file.\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    \n",
    "    print(f\"Loading data from: {file_path}\")\n",
    "    file_size_mb = os.path.getsize(file_path) / (1024*1024)\n",
    "    print(f\"File size: {file_size_mb:.2f} MB\")\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for i, line in enumerate(file):\n",
    "            if line.strip():\n",
    "                try:\n",
    "                    doc = json.loads(line.strip())\n",
    "                    documents.append(doc)\n",
    "                    \n",
    "                    if (i + 1) % 5000 == 0:\n",
    "                        print(f\"Loaded {i + 1} lines...\")\n",
    "                        \n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "    \n",
    "    print(f\"Total documents loaded: {len(documents)}\")\n",
    "    return documents\n",
    "\n",
    "# Load the data\n",
    "documents = load_tr_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02610a3",
   "metadata": {},
   "source": [
    "## 2. Explore Raw Data Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509219a7",
   "metadata": {},
   "source": [
    "#### explore data structure in terminal before reading it</br>\n",
    "```bash\n",
    "head -n 1 TRDataChallenge2023.txt | python3 -m json.tool\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690c2cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the structure of the first document\n",
    "if documents:\n",
    "    print(\"Keys in first document:\")\n",
    "    print(list(documents[0].keys()))\n",
    "    \n",
    "    print(\"\\nFirst document:\")\n",
    "    print(json.dumps(documents[0], indent=2)[:1000] + \"...\")  # Show first 1000 chars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb3d096",
   "metadata": {},
   "source": [
    "## 3. Convert to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccfd83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe(documents):\n",
    "    \"\"\"\n",
    "    Convert documents to a pandas DataFrame.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    \n",
    "    for doc in documents:\n",
    "        # Basic document info\n",
    "        row = {\n",
    "            'document_id': doc.get('documentId', ''),\n",
    "            'postures': ', '.join(doc.get('postures', [])),\n",
    "            'num_postures': len(doc.get('postures', [])),\n",
    "            'num_sections': len(doc.get('sections', [])),\n",
    "        }\n",
    "        \n",
    "        # Extract text content\n",
    "        all_text = []\n",
    "        section_headers = []\n",
    "        \n",
    "        for section in doc.get('sections', []):\n",
    "            header = section.get('headtext', '')\n",
    "            if header:\n",
    "                section_headers.append(header)\n",
    "            \n",
    "            paragraphs = section.get('paragraphs', [])\n",
    "            all_text.extend(paragraphs)\n",
    "        \n",
    "        row['section_headers'] = ' | '.join(section_headers)\n",
    "        row['full_text'] = ' '.join(all_text)\n",
    "        row['text_length'] = len(row['full_text'])\n",
    "        row['word_count'] = len(row['full_text'].split())\n",
    "        row['num_paragraphs'] = len(all_text)\n",
    "        row['num_headers'] = len(section_headers)\n",
    "        \n",
    "        data.append(row)\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Create DataFrame\n",
    "df = create_dataframe(documents)\n",
    "print(f\"DataFrame shape: {df.shape}\")\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feae6443",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "\n",
    "text = (\n",
    "    \"Examining the posture values in the dataframe reveals this is a classic \"\n",
    "    \"multi-label classification problem. Each document can be assigned multiple \"\n",
    "    \"posture labels simultaneously.\"\n",
    ")\n",
    "\n",
    "display(HTML(f'<span style=\"font-size: 150%;\">{text}</span>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db6b985",
   "metadata": {},
   "source": [
    "## 4. Data Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb06eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "print(\"Dataset Overview:\")\n",
    "print(df.info())\n",
    "print(\"\\nDescriptive Statistics:\")\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_columns', None)\n",
    "print()\n",
    "print(df.describe().round(2))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e773de6a",
   "metadata": {},
   "source": [
    "### 4.1 Multi-Label Postures Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad29e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Count number of documents per number of postures\n",
    "posture_counts = df['num_postures'].value_counts().sort_index()\n",
    "plt.bar(posture_counts.index, posture_counts.values)\n",
    "plt.title('Distribution of Number of Postures per Document')\n",
    "plt.xlabel('Number of Postures')\n",
    "plt.ylabel('Number of Documents')\n",
    "plt.show()\n",
    "\n",
    "# Most common postures\n",
    "all_postures = []\n",
    "for postures in df['postures']:\n",
    "    if postures:\n",
    "        all_postures.extend([p.strip() for p in postures.split(',')])\n",
    "\n",
    "posture_freq = pd.Series(all_postures).value_counts().head(10)\n",
    "plt.figure(figsize=(12, 6))\n",
    "posture_freq.plot(kind='bar')\n",
    "plt.title('Top 10 Most Common Postures')\n",
    "plt.xlabel('Posture')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9f2d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "_counts = df['num_postures'].value_counts(dropna=False)\n",
    "_pct = df['num_postures'].value_counts(dropna=False,normalize=True) \n",
    "\n",
    "pd.DataFrame({\n",
    "    'count': _counts,\n",
    "    'percentage': _pct\n",
    "}).sort_index().style.format({'count':'{:,}','percentage':'{:.2%}'}).set_caption(\"Distribution of num_postures\")\\\n",
    "    .set_table_styles([{'selector': 'caption','props': [('color', 'red'),('font-size', '15px')]}])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cfc5a7",
   "metadata": {},
   "source": [
    "<span style=\"font-weight: bold; font-size: 18px;\">**Multi-Label Posture Distribution Analysis**<br>\n",
    "\n",
    "Based on the distribution charts examining posture frequency and prevalence patterns, the following key insights emerge:\n",
    "\n",
    "**Label Distribution Characteristics:**\n",
    "\n",
    "<div style=\"margin-left: 20px;\"> • <strong>Modal Distribution:</strong> The majority of documents exhibit 1-2 posture labels, indicating a relatively sparse multi-label structure</div></br>\n",
    "\n",
    "<div style=\"margin-left: 20px;\"> • <strong>Label Complexity:</strong> Documents can contain up to 4+ concurrent posture labels, representing the complexity of multi-label density</div></br>\n",
    "\n",
    "<div style=\"margin-left: 20px;\"> • <strong>Unlabeled Data:</strong> Approximately 1,000 documents lack posture annotations (num_postures=0), requiring exclusion from the training dataset to prevent noise injection</div></br>\n",
    "\n",
    "<div style=\"margin-left: 20px;\"> • <strong>Severe Label Imbalance:</strong> \"On Appeal\" and \"Appellate Review\" dominate the dataset as the most prevalent posture labels, while numerous categories exhibit extremely low frequency, creating a highly imbalanced multi-label distribution that poses significant challenges for model training</div></br>\n",
    "\n",
    "<div style=\"margin-left: 20px;\"> • <strong>Class Imbalance Mitigation Strategies:</strong> </br>\n",
    "\n",
    "The substantial disparity between dominant and rare categories necessitates implementation of specialized techniques such as:</br>\n",
    "\n",
    "  <div style=\"margin-left: 40px;\">- Undersampling of overrepresented classes</div></br>\n",
    "\n",
    "  <div style=\"margin-left: 40px;\">- Class-weighted loss functions to amplify learning signals from minority categories</div></br>\n",
    "\n",
    "  <div style=\"margin-left: 40px;\">- Focal loss or other imbalance-aware optimization approaches</div></br>\n",
    "\n",
    "</div>\n",
    "\n",
    "<div style=\"margin-left: 20px;\"> • <strong>Training Implications:</strong> Without proper handling of this imbalance, the model may develop bias toward frequent categories while failing to adequately learn rare but potentially important legal postures</div>\n",
    "\n",
    "**Data Quality Implications:**\n",
    "The identified unlabeled instances necessitate data preprocessing to ensure training dataset integrity and model performance optimization.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd36d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the labels - convert postures to a list format\n",
    "def prepare_labels(postures_str):\n",
    "    \"\"\"Convert posture string to list of postures\"\"\"\n",
    "    if pd.isna(postures_str) or postures_str == '':\n",
    "        return []\n",
    "    return [p.strip() for p in postures_str.split(',') if p.strip()]\n",
    "\n",
    "df['posture_list'] = df['postures'].apply(prepare_labels)\n",
    "\n",
    "# Remove documents with no postures\n",
    "df_temp = df[df['posture_list'].apply(len) > 0].copy()\n",
    "print(\"{:<30}{:<20,}\".format(\"Documents with postures: \", len(df_temp)))\n",
    "print(\"{:<30}{:<20}\".format(\"Documents without postures: \", len(df)-len(df_temp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32cbb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze posture distribution\n",
    "all_postures_ml = []\n",
    "for postures in df_temp['posture_list']:\n",
    "    all_postures_ml.extend(postures)\n",
    "\n",
    "posture_counts = pd.Series(all_postures_ml).value_counts()\n",
    "print(f\"\\nTotal unique postures: {len(posture_counts)}\")\n",
    "print()\n",
    "print(f\"Most common postures:\")\n",
    "print(posture_counts.head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9fdc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to most common postures (those appearing in at least 100 documents)\n",
    "min_frequency = 100\n",
    "common_postures = posture_counts[posture_counts >= min_frequency].index.tolist()\n",
    "print(f\"\\nPostures with >= {min_frequency} occurrences: {len(common_postures)}\")\n",
    "print(common_postures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17915f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter documents to only include those with common postures\n",
    "def filter_common_postures(posture_list, common_postures):\n",
    "    \"\"\"Keep only postures that are in the common_postures list\"\"\"\n",
    "    return [p for p in posture_list if p in common_postures]\n",
    "\n",
    "df_temp['filtered_postures'] = df_temp['posture_list'].apply(\n",
    "    lambda x: filter_common_postures(x, common_postures)\n",
    ")\n",
    "\n",
    "# Remove documents that have no common postures after filtering\n",
    "df_ml = df_temp[df_temp['filtered_postures'].apply(len) > 0].copy()\n",
    "print(f\"Documents after filtering to common postures: {len(df_ml)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dca272a",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_postures_raw = len(set(itertools.chain.from_iterable(df['posture_list'])))\n",
    "unique_postures_temp = len(set(itertools.chain.from_iterable(df_temp['posture_list'])))\n",
    "unique_postures_ml = len(set(itertools.chain.from_iterable(df_ml['filtered_postures'])))\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"--\": [\"raw_document\", \"Removing Missing Postures\", \"Filter to Common Postures\"],\n",
    "    \"sample size\": [df.shape[0], df_temp.shape[0], df_ml.shape[0]],\n",
    "    \"Total unique postures\": [unique_postures_raw, unique_postures_temp, unique_postures_ml]\n",
    "}).style.format({'sample size':'{:,}'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fe93c1",
   "metadata": {},
   "source": [
    "### 4.2 Text Length Distribution Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549732d1",
   "metadata": {},
   "source": [
    "<span style=\"color: red; font-weight: bold; font-size: 18px;\"> **Text Length Analysis for Model Selection and Optimization**</br>\n",
    "\n",
    "Analyzing text length distribution is a critical preprocessing step that directly influences model architecture selection and performance optimization. When deploying Large Language Models (LLMs) for this multi-label classification task, context length becomes a pivotal consideration due to several key factors:\n",
    "\n",
    "**Context Length Constraints:** </br>Each LLM architecture has a predefined maximum context length (e.g., 512 tokens for BERT, 4096 for Longformer, 8192 for MordernBERT, 10000+ for many decoder models(Llama, Mistral, Qwen family etc)). Understanding our data's text length distribution enables informed model selection that balances computational efficiency with information preservation.\n",
    "\n",
    "**Resource Optimization:**</br> Oversized context windows lead to unnecessary computational overhead and increased memory consumption without performance gains. Conversely, insufficient context length results in critical information truncation, potentially degrading model accuracy.\n",
    "\n",
    "**Strategic Implications:** </br>By examining the statistical distribution of text lengths across our dataset, we can:\n",
    "\n",
    "Select appropriate model architectures with optimal context lengths\n",
    "Implement efficient text preprocessing strategies (chunking, summarization, or truncation)\n",
    "Estimate computational requirements and training costs\n",
    "Ensure minimal information loss during model input preparation\n",
    "This analysis forms the foundation for making data-driven decisions regarding model selection and hyperparameter configuration. </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01a43d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text length distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(df['text_length'], bins=50, alpha=0.7, edgecolor='black')\n",
    "plt.title('Distribution of Text Length')\n",
    "plt.xlabel('Text Length (characters)')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(df['word_count'], bins=50, alpha=0.7, edgecolor='black', color='orange')\n",
    "plt.title('Distribution of Word Count')\n",
    "plt.xlabel('Word Count')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cf68bf",
   "metadata": {},
   "source": [
    "<span style=\"font-weight: bold; font-size: 18px;\"> **Accurate Text Length Measurement Using Subword Tokenization**</br>\n",
    "\n",
    "The initial text length analysis employed a basic word-counting approach using whitespace as delimiters. However, this naive methodology fails to accurately represent how Large Language Models process and interpret textual input, leading to potential misestimation of context requirements.\n",
    "\n",
    "**Tokenization Discrepancy:**</br> LLMs do not process text at the word level as humans do. Instead, they utilize sophisticated subword tokenization algorithms including:\n",
    "\n",
    "<div style=\"margin-left: 20px;\">1. Byte Pair Encoding (BPE): Used by GPT models</div>\n",
    "<div style=\"margin-left: 20px;\">2. WordPiece: Employed by BERT variants</div>\n",
    "<div style=\"margin-left: 20px;\">3. SentencePiece: Utilized by T5 and many multilingual models</div>\n",
    "\n",
    "**Technical Implementation:**</br> To obtain precise context length measurements that align with actual LLM processing, we implement tokenization using ModernBERT, which employs BPE (Byte Pair Encoding) for subword segmentation. This approach provides several advantages:\n",
    "\n",
    "<div style=\"margin-left: 20px;\">1. Accurate Token Counting: Measures text length in the same units that the model processes</div>\n",
    "<div style=\"margin-left: 20px;\">2. Context Window Optimization: Enables precise estimation of how much text fits within model constraints</div>\n",
    "<div style=\"margin-left: 20px;\">3. Model-Agnostic Approach: The methodology can be adapted for different tokenizers (e.g., GPT-2/3/4 or many other LLMs' tokenizer) based on target model selection</div>\n",
    "\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f66367a",
   "metadata": {},
   "source": [
    "### text length distribution using subword tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e0f90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "model_path = \"answerdotai/ModernBERT-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc3e951",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "# text_df=df[\"full_text\"].to_frame(\"text\")\n",
    "text_df=df_ml.loc[:,[\"full_text\",\"postures\"]]\n",
    "hf=Dataset.from_pandas(text_df)\n",
    "\n",
    "def compute_lenth(example):\n",
    "    return {\"text_length\":len(example[\"input_ids\"])}\n",
    "hf=hf.map(lambda x: tokenizer(x[\"full_text\"]),batched=True)\n",
    "hf=hf.map(compute_lenth)\n",
    "df0=hf.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b464cd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text length distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.hist(df0['text_length'], bins=50, alpha=0.7, edgecolor='black')\n",
    "plt.title('Distribution of Text Length')\n",
    "plt.xlabel('Text Length : Subword Tokenization(BPE)')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7acc6f8",
   "metadata": {},
   "source": [
    "<span style=\"font-weight: bold; font-size: 18px;\">**Subword Tokenization Analysis Results and Model Selection**</br>\n",
    "\n",
    "The subword tokenization analysis reveals that the average text length across the corpus is 3,826 tokens. When considering ModernBERT as our target language model, which supports a maximum context length of 8,192 tokens, this configuration provides excellent coverage for our dataset.\n",
    "\n",
    "Key Findings:\n",
    "\n",
    "<div style=\"margin-left: 20px;\"> •  Average token length: 3,826 tokens per document</div>\n",
    "<div style=\"margin-left: 20px;\"> •  Model capacity: ModernBERT's 8,192-token context window</div>\n",
    "<div style=\"margin-left: 20px;\"> •  Coverage efficiency: Approximately 90% of documents (up to 8,143 tokens) can be processed without truncation</div>\n",
    "\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a9807c",
   "metadata": {},
   "source": [
    "## 5. split the data into training, validation and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c152c7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Multi-label Classification Setup\n",
    "\n",
    "# Create binary label matrix using MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "y_multilabel = mlb.fit_transform(df_ml['filtered_postures'])\n",
    "\n",
    "print(f\"Label matrix shape: {y_multilabel.shape}\")\n",
    "print(f\"Labels: {mlb.classes_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122f957d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare text data\n",
    "X_text = df_ml['full_text'].values\n",
    "\n",
    "# Split the data\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X_text, y_multilabel, \n",
    "    test_size=0.3, # 30% for temp (which will be split into val and test)\n",
    "    random_state=42, \n",
    "    stratify=None\n",
    ")\n",
    "\n",
    " # Split temp into validation and test (50-50 split of the 30%)\n",
    "# # This gives us 15% each\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp,\n",
    "    test_size=0.5,\n",
    "    random_state=42, \n",
    "    stratify=None\n",
    ")\n",
    "\n",
    "print(\"{:<18}{:<10,}{:<15.0%}\".format(\"Total samples:\",len(df_ml),1))\n",
    "print(\"{:<18}{:<10,}{:<15.0%}\".format(\"Training set:\",len(X_train),len(X_train)/len(df_ml)))\n",
    "print(\"{:<18}{:<10,}{:<15.0%}\".format(\"Validation set:\",len(X_val),len(X_val)/len(df_ml)))\n",
    "print(\"{:<18}{:<10,}{:<15.0%}\".format(\"Test set:\",len(X_test),len(X_test)/len(df_ml)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a79240b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check label distribution\n",
    "train_label_sums = y_train.sum(axis=0)\n",
    "val_label_sums = y_val.sum(axis=0)\n",
    "test_label_sums = y_test.sum(axis=0)\n",
    "\n",
    "print(\"\\nLabel distribution in training set:\")\n",
    "print()\n",
    "for i, label in enumerate(mlb.classes_):\n",
    "    print(f\"{label}: {train_label_sums[i]} ({train_label_sums[i]/len(y_train)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a589e835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check label distribution\n",
    "def label_dist(y_train, data_split=\"training\"):\n",
    "    train_label_sums = y_train.sum(axis=0)\n",
    "\n",
    "    # Get top 5 most frequent labels and their indices\n",
    "    top5_indices = train_label_sums.argsort()[-5:][::-1]\n",
    "    top5_labels = [mlb.classes_[i] for i in top5_indices]\n",
    "    top5_counts = train_label_sums[top5_indices]\n",
    "\n",
    "    print(f\"\\nTop 5 most frequent labels in {data_split} set:\")\n",
    "    print()\n",
    "    for idx, label in zip(top5_indices, top5_labels):\n",
    "        count = train_label_sums[idx]\n",
    "        percent = count / len(y_train) \n",
    "        # print(f\"{label}: {count} ({percent:.1f}%)\")\n",
    "        print(\"{:<50}{:<10,}{:<15.1%}\".format(f\"{label}:\", count, percent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becd7630",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dist(y_train, data_split=\"training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65aa9c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dist(y_val, data_split=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3977ee69",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dist(y_test, data_split=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69743260",
   "metadata": {},
   "source": [
    "## 6. Save Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0620897",
   "metadata": {},
   "outputs": [],
   "source": [
    "## save preprocess data\n",
    "saved_data=os.path.join(os.getcwd(), 'processed_data')\n",
    "os.makedirs(saved_data, exist_ok=True)\n",
    "# Save using pickle\n",
    "with open(os.path.join(saved_data,'train_arrays.pkl'), 'wb') as f:\n",
    "    pickle.dump({'X_train': X_train, 'y_train': y_train}, f)\n",
    "\n",
    "with open(os.path.join(saved_data,'val_arrays.pkl'), 'wb') as f:\n",
    "    pickle.dump({'X_val': X_val, 'y_val': y_val}, f)\n",
    "\n",
    "with open(os.path.join(saved_data,'test_arrays.pkl'), 'wb') as f:\n",
    "    pickle.dump({'X_test': X_test, 'y_test': y_test}, f)\n",
    "\n",
    "with open(os.path.join(saved_data,'class_name.pkl'), 'wb') as f:\n",
    "    pickle.dump({'class_name': mlb.classes_}, f)\n",
    "\n",
    "print(\"All arrays saved with pickle!\")\n",
    "\n",
    "# To load later:\n",
    "# with open(os.path.join(saved_data,'train_arrays.pkl'), 'rb') as f:\n",
    "#     train_data = pickle.load(f)\n",
    "#     X_train = train_data['X_train']\n",
    "#     y_train = train_data['y_train']\n",
    "#     label_train = train_data['label_train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b083d212",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
