{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e7a5b02",
   "metadata": {},
   "source": [
    "<span style=\"font-weight: bold; font-size: 18px;\">**Multi-Label Posture Classification: Model Development Strategy**<br><br>\n",
    "\n",
    "We propose a comparative evaluation of two complementary modeling approaches to address the multi-label posture prediction task, each offering distinct advantages for legal document classification.\n",
    "\n",
    "**Baseline Approach: Bag-of-Words Models**<br>\n",
    "\n",
    "Our initial baseline leverages traditional bag-of-words representations (TF-IDF, BM25) combined with multi-label classifiers, justified by several key factors:\n",
    "\n",
    "<div style=\"margin-left: 20px;\"><b>• Computational Efficiency:</b> Lightweight architecture enables rapid prototyping and establishes performance baselines without GPU requirements</div>\n",
    "<div style=\"margin-left: 20px;\"><b>• Statistical Robustness:</b> Word-frequency features provide interpretable, domain-agnostic representations suitable for legal terminology analysis</div>\n",
    "<div style=\"margin-left: 20px;\"><b>• Multi-Label Compatibility:</b> Well-established integration with multi-label algorithms (One-vs-Rest, Binary Relevance, Label Powerset)</div>\n",
    "<div style=\"margin-left: 20px;\"><b>• Baseline Establishment:</b> Provides interpretable performance benchmarks for evaluating more complex architectures</div>\n",
    "\n",
    "**Advanced Approach: Transformer-Based Models (ModernBERT)**<br>\n",
    "\n",
    "Our primary model leverages ModernBERT encoder architecture, specifically designed to address the limitations of traditional BERT for our use case:\n",
    "\n",
    "<div style=\"margin-left: 20px;\"><b>• Extended Context Coverage:</b> ModernBERT's 8,192-token context window accommodates ~90% of our corpus without truncation, preserving critical legal context that may span entire documents</div>\n",
    "\n",
    "<div style=\"margin-left: 20px;\"><b>• Contextual Understanding:</b> Unlike bag-of-words approaches, transformer architectures capture:\n",
    "  <div style=\"margin-left: 40px;\">- Long-range dependencies between legal arguments</div>\n",
    "  <div style=\"margin-left: 40px;\">- Positional relationships between procedural elements</div>\n",
    "  <div style=\"margin-left: 40px;\">- Semantic nuances distinguishing similar posture categories</div>\n",
    "</div>\n",
    "\n",
    "<div style=\"margin-left: 20px;\"><b>• Multi-Label Architecture:</b> The encoder's [CLS] token representation can be effectively coupled with multi-label classification heads, enabling simultaneous prediction of multiple postures</div>\n",
    "\n",
    "<div style=\"margin-left: 20px;\"><b>• Legal Domain Adaptation:</b> Pre-trained language understanding provides superior handling of complex legal terminology and document structure</div>\n",
    "\n",
    "**Comparative Justification:**<br>\n",
    "\n",
    "This dual-approach strategy enables comprehensive evaluation of feature representation impact on multi-label performance, ranging from traditional statistical methods to state-of-the-art contextual understanding, ultimately identifying the optimal balance between computational efficiency and classification accuracy for legal posture prediction.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec7a271",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install -r /mnt/d/TR-Project/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4273a31e",
   "metadata": {},
   "source": [
    "## Bag-of-word (TFIDF): Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8244ef62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from lightgbm import early_stopping, log_evaluation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, hamming_loss\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, f1_score, \n",
    "    roc_auc_score, average_precision_score,\n",
    "    hamming_loss, jaccard_score, accuracy_score\n",
    ")\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3df6176c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load Dataset for model training and evaluation ###\n",
    "data_path=os.path.join(os.getcwd(), 'processed_data')\n",
    "with open(os.path.join(data_path,'train_arrays.pkl'), 'rb') as f:\n",
    "    train_data = pickle.load(f)\n",
    "    X_train = train_data['X_train']\n",
    "    y_train = train_data['y_train']\n",
    "\n",
    "with open(os.path.join(data_path,'val_arrays.pkl'), 'rb') as f:\n",
    "    val_data = pickle.load(f)\n",
    "    X_val = val_data['X_val']\n",
    "    y_val = val_data['y_val']\n",
    "\n",
    "with open(os.path.join(data_path,'test_arrays.pkl'), 'rb') as f:\n",
    "    test_data = pickle.load(f)\n",
    "    X_test = test_data['X_test']\n",
    "    y_test = test_data['y_test']\n",
    "\n",
    "with open(os.path.join(data_path,'class_name.pkl'), 'rb') as f:\n",
    "    class_name_data = pickle.load(f)\n",
    "    class_name = class_name_data['class_name']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "122d8bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting TF-IDF vectorizer...\n",
      "TF-IDF matrix shape (train): (11597, 10000)\n",
      "TF-IDF matrix shape (val): (2485, 10000)\n",
      "TF-IDF matrix shape (test): (2486, 10000)\n",
      "Vocabulary size: 10000\n",
      "\n",
      "Sample features: ['00' '000' '000 00' '000 000' '001' '01' '010' '02' '020' '03' '030' '04'\n",
      " '040' '05' '06' '07' '08' '09' '10' '10 000']\n",
      "Last features: ['years prior' 'years prison' 'years supervised' 'yes' 'yes sir' 'yield'\n",
      " 'york' 'york city' 'york county' 'york law' 'york state' 'young'\n",
      " 'younger' 'youth' 'zba' 'zero' 'zone' 'zoning' 'zoning board'\n",
      " 'zoning ordinance']\n"
     ]
    }
   ],
   "source": [
    "# Create TF-IDF vectorizer\n",
    "# Using parameters optimized for legal text\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=10000,  # Limit features for computational efficiency\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 2),  # Include unigrams and bigrams\n",
    "    min_df=5,           # Ignore terms that appear in fewer than 5 documents\n",
    "    max_df=0.95,        # Ignore terms that appear in more than 95% of documents\n",
    "    sublinear_tf=True   # Apply sublinear scaling\n",
    ")\n",
    "\n",
    "print(\"Fitting TF-IDF vectorizer...\")\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_val_tfidf = tfidf.transform(X_val)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "print(f\"TF-IDF matrix shape (train): {X_train_tfidf.shape}\")\n",
    "print(f\"TF-IDF matrix shape (val): {X_val_tfidf.shape}\")\n",
    "print(f\"TF-IDF matrix shape (test): {X_test_tfidf.shape}\")\n",
    "print(f\"Vocabulary size: {len(tfidf.vocabulary_)}\")\n",
    "\n",
    "# Show some sample features\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "print(f\"\\nSample features: {feature_names[:20]}\")\n",
    "print(f\"Last features: {feature_names[-20:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3da6407",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Train_XGBoost(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"XGBoost classifier with validation-based early stopping for multi-label\"\"\"\n",
    "    \n",
    "    def __init__(self, **xgb_params):\n",
    "        self.xgb_params = xgb_params\n",
    "        self.models_ = []\n",
    "        self.n_classes_ = None\n",
    "        \n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        if len(y.shape) == 1:\n",
    "            y = y.reshape(-1, 1)\n",
    "        if X_val is not None and len(y_val.shape) == 1:\n",
    "            y_val = y_val.reshape(-1, 1)\n",
    "            \n",
    "        self.n_classes_ = y.shape[1]\n",
    "        self.models_ = []\n",
    "        \n",
    "        for i in tqdm(range(self.n_classes_), total=self.n_classes_, leave=True, position=0):\n",
    "            \n",
    "            y_single = y[:, i]\n",
    "            \n",
    "            # Skip if no positive samples\n",
    "            if y_single.sum() == 0:\n",
    "                self.models_.append(None)\n",
    "                continue\n",
    "            \n",
    "            model = xgb.XGBClassifier(**self.xgb_params)\n",
    "            \n",
    "            if X_val is not None and y_val is not None:\n",
    "                y_val_single = y_val[:, i]\n",
    "                model.fit(\n",
    "                    X, y_single,\n",
    "                    eval_set=[(X_val, y_val_single)],\n",
    "                    verbose=False\n",
    "                )\n",
    "            else:\n",
    "                model.fit(X, y_single)\n",
    "            \n",
    "            self.models_.append(model)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predictions = np.zeros((X.shape[0], self.n_classes_))\n",
    "        \n",
    "        for i, model in enumerate(self.models_):\n",
    "            if model is not None:\n",
    "                predictions[:, i] = model.predict(X)\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        probabilities = np.zeros((X.shape[0], self.n_classes_))\n",
    "        \n",
    "        for i, model in enumerate(self.models_):\n",
    "            if model is not None:\n",
    "                proba = model.predict_proba(X)\n",
    "                # Handle case where only one class is present\n",
    "                if proba.shape[1] == 1:\n",
    "                    probabilities[:, i] = 0  # All negative class\n",
    "                else:\n",
    "                    probabilities[:, i] = proba[:, 1]  # Positive class probability\n",
    "        \n",
    "        return probabilities\n",
    "\n",
    "class Train_LGBM(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"LightGBM classifier with validation-based early stopping for multi-label\"\"\"\n",
    "    \n",
    "    def __init__(self, **lgb_params):\n",
    "        self.lgb_params = lgb_params\n",
    "        self.models_ = []\n",
    "        self.n_classes_ = None\n",
    "        \n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        if len(y.shape) == 1:\n",
    "            y = y.reshape(-1, 1)\n",
    "        if X_val is not None and len(y_val.shape) == 1:\n",
    "            y_val = y_val.reshape(-1, 1)\n",
    "            \n",
    "        self.n_classes_ = y.shape[1]\n",
    "        self.models_ = []\n",
    "        \n",
    "        for i in tqdm(range(self.n_classes_), total=self.n_classes_, leave=True, position=0):\n",
    "            \n",
    "            y_single = y[:, i]\n",
    "            \n",
    "            # Skip if no positive samples\n",
    "            if y_single.sum() == 0:\n",
    "                self.models_.append(None)\n",
    "                continue\n",
    "            \n",
    "            model = lgb.LGBMClassifier(**self.lgb_params)\n",
    "            \n",
    "            if X_val is not None and y_val is not None:\n",
    "                y_val_single = y_val[:, i]\n",
    "                model.fit(\n",
    "                    X, y_single,\n",
    "                    eval_set=[(X_val, y_val_single)],\n",
    "                    callbacks=[\n",
    "                        lgb.early_stopping(10, verbose=False),\n",
    "                        lgb.log_evaluation(0)\n",
    "                    ]\n",
    "                )\n",
    "            else:\n",
    "                model.fit(X, y_single)\n",
    "            \n",
    "            self.models_.append(model)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predictions = np.zeros((X.shape[0], self.n_classes_))\n",
    "        \n",
    "        for i, model in enumerate(self.models_):\n",
    "            if model is not None:\n",
    "                predictions[:, i] = model.predict(X)\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        probabilities = np.zeros((X.shape[0], self.n_classes_))\n",
    "        \n",
    "        for i, model in enumerate(self.models_):\n",
    "            if model is not None:\n",
    "                proba = model.predict_proba(X)\n",
    "                # Handle case where only one class is present\n",
    "                if proba.shape[1] == 1:\n",
    "                    probabilities[:, i] = 0  # All negative class\n",
    "                else:\n",
    "                    probabilities[:, i] = proba[:, 1]  # Positive class probability\n",
    "        \n",
    "        return probabilities\n",
    "\n",
    "class Train_logistic(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"Logistic Regression classifier with validation monitoring for multi-label\"\"\"\n",
    "    \n",
    "    def __init__(self, **lr_params):\n",
    "        self.lr_params = lr_params\n",
    "        self.models_ = []\n",
    "        self.n_classes_ = None\n",
    "        self.validation_scores_ = []\n",
    "        \n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        if len(y.shape) == 1:\n",
    "            y = y.reshape(-1, 1)\n",
    "        if X_val is not None and len(y_val.shape) == 1:\n",
    "            y_val = y_val.reshape(-1, 1)\n",
    "            \n",
    "        self.n_classes_ = y.shape[1]\n",
    "        self.models_ = []\n",
    "        self.validation_scores_ = []\n",
    "        \n",
    "        for i in tqdm(range(self.n_classes_), total=self.n_classes_, leave=True, position=0):\n",
    "            \n",
    "            y_single = y[:, i]\n",
    "            \n",
    "            # Skip if no positive samples\n",
    "            if y_single.sum() == 0:\n",
    "                self.models_.append(None)\n",
    "                self.validation_scores_.append(0.0)\n",
    "                continue\n",
    "            \n",
    "            model = LogisticRegression(**self.lr_params)\n",
    "            model.fit(X, y_single)\n",
    "            \n",
    "            # Calculate validation score if validation data provided\n",
    "            if X_val is not None and y_val is not None:\n",
    "                y_val_single = y_val[:, i]\n",
    "                val_score = model.score(X_val, y_val_single)\n",
    "                self.validation_scores_.append(val_score)\n",
    "            else:\n",
    "                self.validation_scores_.append(None)\n",
    "            \n",
    "            self.models_.append(model)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predictions = np.zeros((X.shape[0], self.n_classes_))\n",
    "        \n",
    "        for i, model in enumerate(self.models_):\n",
    "            if model is not None:\n",
    "                predictions[:, i] = model.predict(X)\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        probabilities = np.zeros((X.shape[0], self.n_classes_))\n",
    "        \n",
    "        for i, model in enumerate(self.models_):\n",
    "            if model is not None:\n",
    "                proba = model.predict_proba(X)\n",
    "                # Handle case where only one class is present\n",
    "                if proba.shape[1] == 1:\n",
    "                    probabilities[:, i] = 0  # All negative class\n",
    "                else:\n",
    "                    probabilities[:, i] = proba[:, 1]  # Positive class probability\n",
    "        \n",
    "        return probabilities\n",
    "    \n",
    "    def get_validation_scores(self):\n",
    "        \"\"\"Return validation scores for each label\"\"\"\n",
    "        return self.validation_scores_\n",
    "\n",
    "class Train_RandomForest(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"Random Forest classifier with validation monitoring for multi-label\"\"\"\n",
    "    \n",
    "    def __init__(self, **rf_params):\n",
    "        self.rf_params = rf_params\n",
    "        self.models_ = []\n",
    "        self.n_classes_ = None\n",
    "        self.validation_scores_ = []\n",
    "        self.feature_importances_ = []\n",
    "        \n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        if len(y.shape) == 1:\n",
    "            y = y.reshape(-1, 1)\n",
    "        if X_val is not None and len(y_val.shape) == 1:\n",
    "            y_val = y_val.reshape(-1, 1)\n",
    "            \n",
    "        self.n_classes_ = y.shape[1]\n",
    "        self.models_ = []\n",
    "        self.validation_scores_ = []\n",
    "        self.feature_importances_ = []\n",
    "        \n",
    "        for i in tqdm(range(self.n_classes_), total=self.n_classes_, leave=True, position=0):\n",
    "            \n",
    "            y_single = y[:, i]\n",
    "            \n",
    "            # Skip if no positive samples\n",
    "            if y_single.sum() == 0:\n",
    "                self.models_.append(None)\n",
    "                self.validation_scores_.append(0.0)\n",
    "                self.feature_importances_.append(None)\n",
    "                continue\n",
    "            \n",
    "            model = RandomForestClassifier(**self.rf_params)\n",
    "            model.fit(X, y_single)\n",
    "            \n",
    "            # Store feature importances\n",
    "            self.feature_importances_.append(model.feature_importances_)\n",
    "            \n",
    "            # Calculate validation score if validation data provided\n",
    "            if X_val is not None and y_val is not None:\n",
    "                y_val_single = y_val[:, i]\n",
    "                val_score = model.score(X_val, y_val_single)\n",
    "                self.validation_scores_.append(val_score)\n",
    "            else:\n",
    "                self.validation_scores_.append(None)\n",
    "            \n",
    "            self.models_.append(model)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predictions = np.zeros((X.shape[0], self.n_classes_))\n",
    "        \n",
    "        for i, model in enumerate(self.models_):\n",
    "            if model is not None:\n",
    "                predictions[:, i] = model.predict(X)\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        probabilities = np.zeros((X.shape[0], self.n_classes_))\n",
    "        \n",
    "        for i, model in enumerate(self.models_):\n",
    "            if model is not None:\n",
    "                proba = model.predict_proba(X)\n",
    "                # Handle case where only one class is present\n",
    "                if proba.shape[1] == 1:\n",
    "                    probabilities[:, i] = 0  # All negative class\n",
    "                else:\n",
    "                    probabilities[:, i] = proba[:, 1]  # Positive class probability\n",
    "        \n",
    "        return probabilities\n",
    "    \n",
    "    def get_validation_scores(self):\n",
    "        \"\"\"Return validation scores for each label\"\"\"\n",
    "        return self.validation_scores_\n",
    "    \n",
    "    def get_feature_importances(self):\n",
    "        \"\"\"Return feature importances for each label\"\"\"\n",
    "        return self.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "043de98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_function_with_validation(X_train, y_train, X_val, y_val, model_type='lightgbm'):\n",
    "    \"\"\"\n",
    "    Enhanced training function with proper validation control for multi-label classification\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Training {model_type} with validation control...\")\n",
    "    print(f\"X_train shape: {X_train.shape}\")\n",
    "    print(f\"y_train shape: {y_train.shape}\")\n",
    "    print(f\"X_val shape: {X_val.shape}\")\n",
    "    print(f\"y_val shape: {y_val.shape}\")\n",
    "    \n",
    "    if model_type == 'lightgbm':\n",
    "        model = Train_LGBM(\n",
    "            random_state=42,\n",
    "            n_estimators=200,  # More estimators for early stopping\n",
    "            max_depth=6,\n",
    "            learning_rate=0.1,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            verbosity=-1,\n",
    "            early_stopping_rounds=10\n",
    "        )\n",
    "    elif model_type == 'xgboost':\n",
    "        model = Train_XGBoost(\n",
    "            random_state=42,\n",
    "            n_estimators=200,  # More estimators for early stopping\n",
    "            max_depth=6,\n",
    "            learning_rate=0.1,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            eval_metric='logloss',\n",
    "            verbosity=0,\n",
    "            early_stopping_rounds=10\n",
    "        )\n",
    "    elif model_type == 'logistic':\n",
    "        model = Train_logistic(\n",
    "            random_state=42,\n",
    "            max_iter=1000,\n",
    "            C=1.0,\n",
    "            solver='liblinear',\n",
    "            class_weight='balanced'  # Handle class imbalance\n",
    "        )\n",
    "    elif model_type == 'randomforest':\n",
    "        model = Train_RandomForest(\n",
    "            random_state=42,\n",
    "            n_estimators=100,\n",
    "            max_depth=10,\n",
    "            min_samples_split=5,\n",
    "            min_samples_leaf=2,\n",
    "            max_features='sqrt',\n",
    "            class_weight='balanced',  # Handle class imbalance\n",
    "            n_jobs=-1\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"Supported model types: 'lightgbm', 'xgboost', 'logistic', 'randomforest'\")\n",
    "    \n",
    "    # Fit with validation data\n",
    "    model.fit(X_train, y_train, X_val, y_val)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_val = model.predict(X_val)\n",
    "\n",
    "    # For ROC-AUC and PR-AUC, need probability estimates\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        y_prob_val = model.predict_proba(X_val)\n",
    "    else:\n",
    "        y_prob_val = None\n",
    "    \n",
    "    # Calculate metrics\n",
    "    # train_acc = accuracy_score(y_train, y_pred_train)\n",
    "    val_acc = accuracy_score(y_val, y_pred_val)\n",
    "    # train_f1 = f1_score(y_train, y_pred_train, average='micro')\n",
    "\n",
    "\n",
    "    val_f1_samples = f1_score(y_val, y_pred_val, average='samples', zero_division=0)\n",
    "    val_f1_micro = f1_score(y_val, y_pred_val, average='micro', zero_division=0)\n",
    "    val_f1_macro = f1_score(y_val, y_pred_val, average='macro', zero_division=0)\n",
    "    val_f1_weighted = f1_score(y_val, y_pred_val, average='weighted', zero_division=0)\n",
    "\n",
    "    val_jaccard_samples = jaccard_score(y_val, y_pred_val, average='samples', zero_division=0)\n",
    "    val_jaccard_macro = jaccard_score(y_val, y_pred_val, average='macro', zero_division=0)\n",
    "    val_jaccard_weighted = jaccard_score(y_val, y_pred_val, average='weighted', zero_division=0)\n",
    "\n",
    "    # Calculate hamming loss (lower is better)\n",
    "    train_hamming = hamming_loss(y_train, y_pred_train)\n",
    "    val_hamming = hamming_loss(y_val, y_pred_val)\n",
    "    \n",
    "    # Calculate overfitting gaps for different metrics\n",
    "    hamming_gap = val_hamming - train_hamming  # Note: val - train because lower hamming is better\n",
    "\n",
    "    if y_prob_val is not None:\n",
    "        try:\n",
    "            val_roc_auc_macro = roc_auc_score(y_val, y_prob_val, average=\"macro\")\n",
    "            val_roc_auc_micro = roc_auc_score(y_val, y_prob_val, average=\"micro\")\n",
    "            val_roc_auc_weighted = roc_auc_score(y_val, y_prob_val, average=\"weighted\")\n",
    "\n",
    "        except ValueError as e:\n",
    "            print(f\"Warning: ROC-AUC calculation failed: {e}\")\n",
    "            val_roc_auc_macro=0.0\n",
    "            val_roc_auc_micro=0.0\n",
    "            val_roc_auc_weighted=0.0\n",
    "        try:\n",
    "            val_pr_auc_macro = average_precision_score(y_val, y_prob_val, average=\"macro\")\n",
    "            val_pr_auc_micro = average_precision_score(y_val, y_prob_val, average=\"weighted\")\n",
    "            val_pr_auc_weighted = average_precision_score(y_val, y_prob_val, average=\"micro\")\n",
    "        except ValueError as e:\n",
    "            print(f\"Warning: PR-AUC calculation failed: {e}\")\n",
    "            val_pr_auc_macro=0.0\n",
    "            val_pr_auc_micro=0.0\n",
    "            val_pr_auc_weighted=0.0\n",
    "    else:\n",
    "        val_roc_auc_macro=0.0\n",
    "        val_roc_auc_micro=0.0\n",
    "        val_roc_auc_weighted=0.0\n",
    "        val_pr_auc_macro=0.0\n",
    "        val_pr_auc_micro=0.0\n",
    "        val_pr_auc_weighted=0.0\n",
    "    \n",
    "    print(f\"Training completed!\")\n",
    "    print(f\"Val Accuracy: {val_acc:.4f}\")\n",
    "    print(f\"Val F1 samples: {val_f1_samples:.4f}\")\n",
    "    print(f\"Val F1 macro: {val_f1_macro:.4f}\")\n",
    "    print(f\"Val F1 micro: {val_f1_micro:.4f}\")\n",
    "    print(f\"Val F1 weighted: {val_f1_weighted:.4f}\")\n",
    "    print(f\"Train Hamming Loss: {train_hamming:.4f}\")\n",
    "    print(f\"Val Hamming Loss: {val_hamming:.4f}\")\n",
    "    print(f\"Overfitting Gap (Hamming): {hamming_gap:.4f}\")\n",
    "    \n",
    "    return model, {\n",
    "        'val_accuracy': val_acc,\n",
    "        'val_f1_samples': val_f1_samples,\n",
    "        'val_f1_macro': val_f1_macro,\n",
    "        'val_f1_micro': val_f1_micro,\n",
    "        'val_f1_weighted': val_f1_weighted,\n",
    "        'val_hamming_loss': val_hamming,\n",
    "        \"val_jaccard_samples\":val_jaccard_samples,\n",
    "        \"val_jaccard_macro\":val_jaccard_macro,\n",
    "        \"val_jaccard_weighted\":val_jaccard_weighted,        \n",
    "        'val_roc_auc_macro': val_roc_auc_macro,\n",
    "        'val_roc_auc_micro': val_roc_auc_micro,\n",
    "        'val_roc_auc_weighted': val_roc_auc_weighted,\n",
    "        'val_pr_auc_macro': val_pr_auc_macro,\n",
    "        'val_pr_auc_micro': val_pr_auc_micro,\n",
    "        'val_pr_auc_weighted': val_pr_auc_weighted,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d9918bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Model Comparison with Validation Control\n",
    "\n",
    "def compare_all_models(X_train, y_train, X_val, y_val, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Train and compare all models with validation control\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"🚀 COMPREHENSIVE MODEL COMPARISON WITH VALIDATION CONTROL\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    models_to_test = ['logistic', 'randomforest', 'lightgbm', 'xgboost']\n",
    "    results = {}\n",
    "    \n",
    "    for model_type in models_to_test:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"🔧 Training {model_type.upper()} Model\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        try:\n",
    "            # Train model with validation\n",
    "            model, metrics = training_function_with_validation(\n",
    "                X_train, y_train, X_val, y_val, model_type=model_type\n",
    "            )\n",
    "            \n",
    "            # Test on unseen data\n",
    "            y_pred_test = model.predict(X_test)\n",
    "            # For ROC-AUC and PR-AUC, need probability estimates\n",
    "            if hasattr(model, \"predict_proba\"):\n",
    "                y_prob_test = model.predict_proba(X_test)\n",
    "            else:\n",
    "                y_prob_test = None\n",
    "        \n",
    "            test_acc = accuracy_score(y_test, y_pred_test)\n",
    "            test_f1_samples = f1_score(y_test, y_pred_test, average='samples', zero_division=0)\n",
    "            test_f1_micro = f1_score(y_test, y_pred_test, average='micro', zero_division=0)\n",
    "            test_f1_macro = f1_score(y_test, y_pred_test, average='macro', zero_division=0)\n",
    "            test_f1_weighted = f1_score(y_test, y_pred_test, average='weighted', zero_division=0)\n",
    "            test_hamming = hamming_loss(y_test, y_pred_test)\n",
    "\n",
    "            test_jaccard_samples = jaccard_score(y_test, y_pred_test, average='samples', zero_division=0)\n",
    "            test_jaccard_macro = jaccard_score(y_test, y_pred_test, average='macro', zero_division=0)\n",
    "            test_jaccard_weighted = jaccard_score(y_test, y_pred_test, average='weighted', zero_division=0)\n",
    "\n",
    "            if y_prob_test is not None:\n",
    "                try:\n",
    "                    test_roc_auc_macro = roc_auc_score(y_test, y_prob_test, average=\"macro\")\n",
    "                    test_roc_auc_micro = roc_auc_score(y_test, y_prob_test, average=\"micro\")\n",
    "                    test_roc_auc_weighted = roc_auc_score(y_test, y_prob_test, average=\"weighted\")\n",
    "\n",
    "                except ValueError as e:\n",
    "                    print(f\"Warning: ROC-AUC calculation failed: {e}\")\n",
    "                    test_roc_auc_macro=0.0\n",
    "                    test_roc_auc_micro=0.0\n",
    "                    test_roc_auc_weighted=0.0\n",
    "                try:\n",
    "                    test_pr_auc_macro = average_precision_score(y_test, y_prob_test, average=\"macro\")\n",
    "                    test_pr_auc_micro = average_precision_score(y_test, y_prob_test, average=\"weighted\")\n",
    "                    test_pr_auc_weighted = average_precision_score(y_test, y_prob_test, average=\"micro\")\n",
    "                except ValueError as e:\n",
    "                    print(f\"Warning: PR-AUC calculation failed: {e}\")\n",
    "                    test_pr_auc_macro=0.0\n",
    "                    test_pr_auc_micro=0.0\n",
    "                    test_pr_auc_weighted=0.0\n",
    "            else:\n",
    "                test_roc_auc_macro=0.0\n",
    "                test_roc_auc_micro=0.0\n",
    "                test_roc_auc_weighted=0.0\n",
    "                test_pr_auc_macro=0.0\n",
    "                test_pr_auc_micro=0.0\n",
    "                test_pr_auc_weighted=0.0            \n",
    "            \n",
    "            # Store all results\n",
    "            results[model_type] = {\n",
    "                'model': model,\n",
    "                'val_accuracy': metrics['val_accuracy'],\n",
    "                'test_accuracy': test_acc,\n",
    "                'val_f1_samples': metrics['val_f1_samples'],\n",
    "                'val_f1_micro': metrics['val_f1_micro'],\n",
    "                'val_f1_macro': metrics['val_f1_macro'],\n",
    "                'val_f1_weighted': metrics['val_f1_weighted'],\n",
    "                'test_f1_samples': test_f1_samples,\n",
    "                'test_f1_micro': test_f1_micro,\n",
    "                'test_f1_macro': test_f1_macro,\n",
    "                'test_f1_weighted': test_f1_weighted,\n",
    "                'val_hamming_loss': metrics['val_hamming_loss'],\n",
    "                'test_hamming_loss': test_hamming,\n",
    "                'val_jaccard_samples': metrics['val_jaccard_samples'],\n",
    "                'val_jaccard_macro': metrics['val_jaccard_macro'],\n",
    "                'val_jaccard_weighted': metrics['val_jaccard_weighted'],\n",
    "                'test_jaccard_samples': test_jaccard_samples,\n",
    "                'test_jaccard_macro': test_jaccard_macro,\n",
    "                'test_jaccard_weighted': test_jaccard_weighted,\n",
    "                'val_roc_auc_macro': metrics['val_roc_auc_macro'],\n",
    "                'val_roc_auc_micro': metrics['val_roc_auc_micro'],\n",
    "                'val_roc_auc_weighted': metrics['val_roc_auc_weighted'],\n",
    "                'val_pr_auc_macro': metrics['val_pr_auc_macro'],\n",
    "                'val_pr_auc_micro': metrics['val_pr_auc_micro'],\n",
    "                'val_pr_auc_weighted': metrics['val_pr_auc_weighted'],\n",
    "                'test_roc_auc_macro': test_roc_auc_macro,\n",
    "                'test_roc_auc_micro': test_roc_auc_micro,\n",
    "                'test_roc_auc_weighted': test_roc_auc_weighted,\n",
    "                'test_pr_auc_macro': test_pr_auc_macro,\n",
    "                'test_pr_auc_micro': test_pr_auc_micro,\n",
    "                'test_pr_auc_weighted': test_pr_auc_weighted,\n",
    "\n",
    "            }\n",
    "            \n",
    "            print(f\"✅ {model_type.upper()} completed successfully!\")\n",
    "            print(f\"   Test Accuracy: {test_acc:.4f}\")\n",
    "            print(f\"   test_f1_samples: {test_f1_samples:.4f}\")\n",
    "            print(f\"   test_f1_macro: {test_f1_macro:.4f}\")\n",
    "            print(f\"   test_f1_micro: {test_f1_micro:.4f}\")\n",
    "            print(f\"   test_f1_weighted: {test_f1_weighted:.4f}\")\n",
    "            print(f\"   Test Hamming Loss: {test_hamming:.4f}\")\n",
    "            print(f\"   test_jaccard_samples: {test_jaccard_samples:.4f}\")\n",
    "            print(f\"   test_jaccard_macro: {test_jaccard_macro:.4f}\")\n",
    "            print(f\"   test_jaccard_weighted: {test_jaccard_weighted:.4f}\")\n",
    "            print(f\"   test_roc_auc_macro: {test_roc_auc_macro:.4f}\")\n",
    "            print(f\"   test_roc_auc_micro: {test_roc_auc_micro:.4f}\")\n",
    "            print(f\"   test_roc_auc_weighted: {test_roc_auc_weighted:.4f}\")\n",
    "            print(f\"   test_pr_auc_macro: {test_pr_auc_macro:.4f}\")\n",
    "            print(f\"   test_pr_auc_micro: {test_pr_auc_micro:.4f}\")\n",
    "            print(f\"   test_pr_auc_weighted: {test_pr_auc_weighted:.4f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error training {model_type}: {str(e)}\")\n",
    "            results[model_type] = None\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce800481",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_model_results(results):\n",
    "    \"\"\"\n",
    "    Analyze and display comprehensive results\n",
    "    \"\"\"\n",
    "\n",
    "    # Filter successful results\n",
    "    successful_results = {k: v for k, v in results.items() if v is not None}\n",
    "    if not successful_results:\n",
    "        print(\"❌ No models trained successfully!\")\n",
    "        return\n",
    "\n",
    "    # Define column widths for perfect alignment\n",
    "    col_widths = {\n",
    "        \"Model\": 15,\n",
    "        \"Acc\": 9,\n",
    "        \"Ham\": 9,\n",
    "        \"f1_macro\": 13,\n",
    "        \"f1_weighted\": 16,\n",
    "        \"roc_auc_macro\": 18,\n",
    "        \"roc_auc_weighted\": 20,\n",
    "        \"pr_auc_macro\": 17,\n",
    "        \"pr_auc_weighted\": 19,\n",
    "        \"jaccard_macro\": 18,\n",
    "        \"jaccard_weighted\": 20\n",
    "    }\n",
    "\n",
    "    # Validation Set Table\n",
    "    print(\"\\n\" + \"=\"*120)\n",
    "    print(\"📊 Model Evaluation in Validation Set\")\n",
    "    print(\"=\"*120 + \"\\n\")\n",
    "\n",
    "    val_header = (\n",
    "        f\"{'Model':<{col_widths['Model']}} | \"\n",
    "        f\"{'Val Acc':<{col_widths['Acc']}} | \"\n",
    "        f\"{'Val Ham':<{col_widths['Ham']}} | \"\n",
    "        f\"{'val_f1_macro':<{col_widths['f1_macro']}} | \"\n",
    "        f\"{'val_f1_weighted':<{col_widths['f1_weighted']}} | \"\n",
    "        f\"{'val_roc_auc_macro':<{col_widths['roc_auc_macro']}} | \"\n",
    "        f\"{'val_roc_auc_weighted':<{col_widths['roc_auc_weighted']}} | \"\n",
    "        f\"{'val_pr_auc_macro':<{col_widths['pr_auc_macro']}} | \"\n",
    "        f\"{'val_pr_auc_weighted':<{col_widths['pr_auc_weighted']}} | \"\n",
    "        # f\"{'val_jaccard_macro':<{col_widths['jaccard_macro']}} | \"\n",
    "        # f\"{'val_jaccard_weighted':<{col_widths['jaccard_weighted']}}\"\n",
    "    )\n",
    "    print(val_header)\n",
    "    print(\"-\" * len(val_header))\n",
    "\n",
    "    for model_name, result in successful_results.items():\n",
    "        print(\n",
    "            f\"{model_name.upper():<{col_widths['Model']}} | \"\n",
    "            f\"{result['val_accuracy']:<{col_widths['Acc']}.4f} | \"\n",
    "            f\"{result['val_hamming_loss']:<{col_widths['Ham']}.4f} | \"\n",
    "            f\"{result['val_f1_macro']:<{col_widths['f1_macro']}.4f} | \"\n",
    "            f\"{result['val_f1_weighted']:<{col_widths['f1_weighted']}.4f} | \"\n",
    "            f\"{result['val_roc_auc_macro']:<{col_widths['roc_auc_macro']}.4f} | \"\n",
    "            f\"{result['val_roc_auc_weighted']:<{col_widths['roc_auc_weighted']}.4f} | \"\n",
    "            f\"{result['val_pr_auc_macro']:<{col_widths['pr_auc_macro']}.4f} | \"\n",
    "            f\"{result['val_pr_auc_weighted']:<{col_widths['pr_auc_weighted']}.4f} | \"\n",
    "            # f\"{result['val_jaccard_macro']:<{col_widths['jaccard_macro']}.4f} | \"\n",
    "            # f\"{result['val_jaccard_weighted']:<{col_widths['jaccard_weighted']}.4f}\"\n",
    "        )\n",
    "\n",
    "    print(\"\\n\" + \"=\"*120)\n",
    "    print(\"📊 Model Evaluation in Test Set\")\n",
    "    print(\"=\"*120 + \"\\n\")\n",
    "\n",
    "    test_header = (\n",
    "        f\"{'Model':<{col_widths['Model']}} | \"\n",
    "        f\"{'test Acc':<{col_widths['Acc']}} | \"\n",
    "        f\"{'test Ham':<{col_widths['Ham']}} | \"\n",
    "        f\"{'test_f1_macro':<{col_widths['f1_macro']}} | \"\n",
    "        f\"{'test_f1_weighted':<{col_widths['f1_weighted']}} | \"\n",
    "        f\"{'test_roc_auc_macro':<{col_widths['roc_auc_macro']}} | \"\n",
    "        f\"{'test_roc_auc_weighted':<{col_widths['roc_auc_weighted']}} | \"\n",
    "        f\"{'test_pr_auc_macro':<{col_widths['pr_auc_macro']}} | \"\n",
    "        f\"{'test_pr_auc_weighted':<{col_widths['pr_auc_weighted']}} | \"\n",
    "        # f\"{'test_jaccard_macro':<{col_widths['jaccard_macro']}} | \"\n",
    "        # f\"{'test_jaccard_weighted':<{col_widths['jaccard_weighted']}}\"\n",
    "    )\n",
    "    print(test_header)\n",
    "    print(\"-\" * len(test_header))\n",
    "\n",
    "    for model_name, result in successful_results.items():\n",
    "        print(\n",
    "            f\"{model_name.upper():<{col_widths['Model']}} | \"\n",
    "            f\"{result['test_accuracy']:<{col_widths['Acc']}.4f} | \"\n",
    "            f\"{result['test_hamming_loss']:<{col_widths['Ham']}.4f} | \"\n",
    "            f\"{result['test_f1_macro']:<{col_widths['f1_macro']}.4f} | \"\n",
    "            f\"{result['test_f1_weighted']:<{col_widths['f1_weighted']}.4f} | \"\n",
    "            f\"{result['test_roc_auc_macro']:<{col_widths['roc_auc_macro']}.4f} | \"\n",
    "            f\"{result['test_roc_auc_weighted']:<{col_widths['roc_auc_weighted']}.4f} | \"\n",
    "            f\"{result['test_pr_auc_macro']:<{col_widths['pr_auc_macro']}.4f} | \"\n",
    "            f\"{result['test_pr_auc_weighted']:<{col_widths['pr_auc_weighted']}.4f} | \"\n",
    "            # f\"{result['test_jaccard_macro']:<{col_widths['jaccard_macro']}.4f} | \"\n",
    "            # f\"{result['test_jaccard_weighted']:<{col_widths['jaccard_weighted']}.4f}\"\n",
    "        )\n",
    "    return successful_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c057bf7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Model Training...\n",
      "🚀 COMPREHENSIVE MODEL COMPARISON WITH VALIDATION CONTROL\n",
      "================================================================================\n",
      "\n",
      "============================================================\n",
      "🔧 Training LOGISTIC Model\n",
      "============================================================\n",
      "Training logistic with validation control...\n",
      "X_train shape: (11597, 10000)\n",
      "y_train shape: (11597, 27)\n",
      "X_val shape: (2485, 10000)\n",
      "y_val shape: (2485, 27)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [00:21<00:00,  1.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed!\n",
      "Val Accuracy: 0.4978\n",
      "Val F1 samples: 0.8097\n",
      "Val F1 macro: 0.6277\n",
      "Val F1 micro: 0.7936\n",
      "Val F1 weighted: 0.8241\n",
      "Train Hamming Loss: 0.0199\n",
      "Val Hamming Loss: 0.0269\n",
      "Overfitting Gap (Hamming): 0.0070\n",
      "✅ LOGISTIC completed successfully!\n",
      "   Test Accuracy: 0.4702\n",
      "   test_f1_samples: 0.7981\n",
      "   test_f1_macro: 0.6184\n",
      "   test_f1_micro: 0.7859\n",
      "   test_f1_weighted: 0.8188\n",
      "   Test Hamming Loss: 0.0281\n",
      "   test_jaccard_samples: 0.7203\n",
      "   test_jaccard_macro: 0.4815\n",
      "   test_jaccard_weighted: 0.7227\n",
      "   test_roc_auc_macro: 0.9772\n",
      "   test_roc_auc_micro: 0.9885\n",
      "   test_roc_auc_weighted: 0.9746\n",
      "   test_pr_auc_macro: 0.6688\n",
      "   test_pr_auc_micro: 0.8618\n",
      "   test_pr_auc_weighted: 0.8556\n",
      "\n",
      "============================================================\n",
      "🔧 Training RANDOMFOREST Model\n",
      "============================================================\n",
      "Training randomforest with validation control...\n",
      "X_train shape: (11597, 10000)\n",
      "y_train shape: (11597, 27)\n",
      "X_val shape: (2485, 10000)\n",
      "y_val shape: (2485, 27)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [00:15<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed!\n",
      "Val Accuracy: 0.5127\n",
      "Val F1 samples: 0.7891\n",
      "Val F1 macro: 0.3862\n",
      "Val F1 micro: 0.7894\n",
      "Val F1 weighted: 0.7666\n",
      "Train Hamming Loss: 0.0114\n",
      "Val Hamming Loss: 0.0245\n",
      "Overfitting Gap (Hamming): 0.0130\n",
      "✅ RANDOMFOREST completed successfully!\n",
      "   Test Accuracy: 0.5270\n",
      "   test_f1_samples: 0.7884\n",
      "   test_f1_macro: 0.3861\n",
      "   test_f1_micro: 0.7901\n",
      "   test_f1_weighted: 0.7671\n",
      "   Test Hamming Loss: 0.0242\n",
      "   test_jaccard_samples: 0.7232\n",
      "   test_jaccard_macro: 0.2972\n",
      "   test_jaccard_weighted: 0.6723\n",
      "   test_roc_auc_macro: 0.9641\n",
      "   test_roc_auc_micro: 0.9840\n",
      "   test_roc_auc_weighted: 0.9649\n",
      "   test_pr_auc_macro: 0.6091\n",
      "   test_pr_auc_micro: 0.8343\n",
      "   test_pr_auc_weighted: 0.8438\n",
      "\n",
      "============================================================\n",
      "🔧 Training LIGHTGBM Model\n",
      "============================================================\n",
      "Training lightgbm with validation control...\n",
      "X_train shape: (11597, 10000)\n",
      "y_train shape: (11597, 27)\n",
      "X_val shape: (2485, 10000)\n",
      "y_val shape: (2485, 27)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [02:41<00:00,  5.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed!\n",
      "Val Accuracy: 0.6149\n",
      "Val F1 samples: 0.8339\n",
      "Val F1 macro: 0.5954\n",
      "Val F1 micro: 0.8340\n",
      "Val F1 weighted: 0.8183\n",
      "Train Hamming Loss: 0.0016\n",
      "Val Hamming Loss: 0.0181\n",
      "Overfitting Gap (Hamming): 0.0165\n",
      "✅ LIGHTGBM completed successfully!\n",
      "   Test Accuracy: 0.6070\n",
      "   test_f1_samples: 0.8205\n",
      "   test_f1_macro: 0.5729\n",
      "   test_f1_micro: 0.8248\n",
      "   test_f1_weighted: 0.8096\n",
      "   Test Hamming Loss: 0.0190\n",
      "   test_jaccard_samples: 0.7675\n",
      "   test_jaccard_macro: 0.4471\n",
      "   test_jaccard_weighted: 0.7212\n",
      "   test_roc_auc_macro: 0.9621\n",
      "   test_roc_auc_micro: 0.9874\n",
      "   test_roc_auc_weighted: 0.9755\n",
      "   test_pr_auc_macro: 0.6197\n",
      "   test_pr_auc_micro: 0.8605\n",
      "   test_pr_auc_weighted: 0.8836\n",
      "\n",
      "============================================================\n",
      "🔧 Training XGBOOST Model\n",
      "============================================================\n",
      "Training xgboost with validation control...\n",
      "X_train shape: (11597, 10000)\n",
      "y_train shape: (11597, 27)\n",
      "X_val shape: (2485, 10000)\n",
      "y_val shape: (2485, 27)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [08:49<00:00, 19.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed!\n",
      "Val Accuracy: 0.6205\n",
      "Val F1 samples: 0.8357\n",
      "Val F1 macro: 0.5861\n",
      "Val F1 micro: 0.8368\n",
      "Val F1 weighted: 0.8162\n",
      "Train Hamming Loss: 0.0027\n",
      "Val Hamming Loss: 0.0176\n",
      "Overfitting Gap (Hamming): 0.0149\n",
      "✅ XGBOOST completed successfully!\n",
      "   Test Accuracy: 0.6219\n",
      "   test_f1_samples: 0.8273\n",
      "   test_f1_macro: 0.5815\n",
      "   test_f1_micro: 0.8331\n",
      "   test_f1_weighted: 0.8133\n",
      "   Test Hamming Loss: 0.0179\n",
      "   test_jaccard_samples: 0.7767\n",
      "   test_jaccard_macro: 0.4584\n",
      "   test_jaccard_weighted: 0.7255\n",
      "   test_roc_auc_macro: 0.9797\n",
      "   test_roc_auc_micro: 0.9920\n",
      "   test_roc_auc_weighted: 0.9780\n",
      "   test_pr_auc_macro: 0.6835\n",
      "   test_pr_auc_micro: 0.8706\n",
      "   test_pr_auc_weighted: 0.9148\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "print(\"Starting Model Training...\")\n",
    "all_results = compare_all_models(X_train_tfidf, y_train, X_val_tfidf, y_val, X_test_tfidf, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "194778cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting comprehensive model comparison...\n",
      "\n",
      "\n",
      "========================================================================================================================\n",
      "📊 Model Evaluation in Validation Set\n",
      "========================================================================================================================\n",
      "\n",
      "Model           | Val Acc   | Val Ham   | val_f1_macro  | val_f1_weighted  | val_roc_auc_macro  | val_roc_auc_weighted | val_pr_auc_macro  | val_pr_auc_weighted | \n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "LOGISTIC        | 0.4978    | 0.0269    | 0.6277        | 0.8241           | 0.9800             | 0.9765               | 0.6757            | 0.8560              | \n",
      "RANDOMFOREST    | 0.5127    | 0.0245    | 0.3862        | 0.7666           | 0.9655             | 0.9676               | 0.6136            | 0.8480              | \n",
      "LIGHTGBM        | 0.6149    | 0.0181    | 0.5954        | 0.8183           | 0.9522             | 0.9753               | 0.6355            | 0.8891              | \n",
      "XGBOOST         | 0.6205    | 0.0176    | 0.5861        | 0.8162           | 0.9778             | 0.9788               | 0.6918            | 0.9184              | \n",
      "\n",
      "========================================================================================================================\n",
      "📊 Model Evaluation in Test Set\n",
      "========================================================================================================================\n",
      "\n",
      "Model           | test Acc  | test Ham  | test_f1_macro | test_f1_weighted | test_roc_auc_macro | test_roc_auc_weighted | test_pr_auc_macro | test_pr_auc_weighted | \n",
      "---------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "LOGISTIC        | 0.4702    | 0.0281    | 0.6184        | 0.8188           | 0.9772             | 0.9746               | 0.6688            | 0.8556              | \n",
      "RANDOMFOREST    | 0.5270    | 0.0242    | 0.3861        | 0.7671           | 0.9641             | 0.9649               | 0.6091            | 0.8438              | \n",
      "LIGHTGBM        | 0.6070    | 0.0190    | 0.5729        | 0.8096           | 0.9621             | 0.9755               | 0.6197            | 0.8836              | \n",
      "XGBOOST         | 0.6219    | 0.0179    | 0.5815        | 0.8133           | 0.9797             | 0.9780               | 0.6835            | 0.9148              | \n"
     ]
    }
   ],
   "source": [
    "print(\"Starting comprehensive model comparison...\")\n",
    "print()\n",
    "final_analysis = analyze_model_results(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8ec67ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Suppose your dictionary is named 'results'\n",
    "# Remove non-serializable objects (like model instances) before saving\n",
    "results_to_save = {}\n",
    "for k, v in all_results.items():\n",
    "    results_to_save[k] = {key: value for key, value in v.items() if key != 'model'}\n",
    "\n",
    "# Save to JSON file\n",
    "with open('TFIDF-model.json', 'w') as f:\n",
    "    json.dump(results_to_save, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12de3fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load from JSON file\n",
    "# with open('TFIDF-model.json', 'r') as f:\n",
    "#     loaded_results = json.load(f)\n",
    "\n",
    "# print(loaded_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a7c4d3",
   "metadata": {},
   "source": [
    "## Transformers Encoder Model (MordenBERT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516998b2",
   "metadata": {},
   "source": [
    "### you need GPU to run the following script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "48e3714a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from datasets import Dataset, DatasetDict\n",
    "from datasets import Sequence, Value\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import EarlyStoppingCallback\n",
    "from transformers import TrainerCallback\n",
    "from transformers import set_seed\n",
    "import evaluate\n",
    "import argparse\n",
    "from functools import partial\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, f1_score, \n",
    "    roc_auc_score, average_precision_score,\n",
    "    hamming_loss, jaccard_score, accuracy_score\n",
    ")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c148b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.random.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d27045a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets_from_arrays(X_train, y_train, X_val=None, y_val=None, X_test=None, y_test=None):\n",
    "    \"\"\"\n",
    "    Convert arrays into HuggingFace datasets format with specified structure\n",
    "    \n",
    "    Returns:\n",
    "        DatasetDict with features:\n",
    "        - dataset[\"train\"][\"text\"]: text data\n",
    "        - dataset[\"train\"][\"labels\"]: multi-label arrays\n",
    "        - dataset[\"val\"][\"text\"]: validation text data (if provided)\n",
    "        - dataset[\"val\"][\"labels\"]: validation labels (if provided)\n",
    "        - dataset[\"test\"][\"text\"]: test text data (if provided)\n",
    "        - dataset[\"test\"][\"labels\"]: test labels (if provided)\n",
    "    \"\"\"\n",
    "    # Create training dataset\n",
    "    train_dict = {\n",
    "        \"text\": X_train.tolist() if hasattr(X_train, 'tolist') else list(X_train),\n",
    "        \"labels\": y_train.tolist() if hasattr(y_train, 'tolist') else list(y_train)\n",
    "    }\n",
    "    \n",
    "    datasets_dict = {\n",
    "        \"train\": Dataset.from_dict(train_dict)\n",
    "    }\n",
    "    \n",
    "    # Add validation dataset if provided\n",
    "    if X_val is not None and y_val is not None:\n",
    "        val_dict = {\n",
    "            \"text\": X_val.tolist() if hasattr(X_val, 'tolist') else list(X_val),\n",
    "            \"labels\": y_val.tolist() if hasattr(y_val, 'tolist') else list(y_val)\n",
    "        }\n",
    "        datasets_dict[\"val\"] = Dataset.from_dict(val_dict)\n",
    "    \n",
    "    # Add test dataset if provided\n",
    "    if X_test is not None and y_test is not None:\n",
    "        test_dict = {\n",
    "            \"text\": X_test.tolist() if hasattr(X_test, 'tolist') else list(X_test),\n",
    "            \"labels\": y_test.tolist() if hasattr(y_test, 'tolist') else list(y_test)\n",
    "        }\n",
    "        datasets_dict[\"test\"] = Dataset.from_dict(test_dict)\n",
    "\n",
    "    # Create DatasetDict\n",
    "    dataset = DatasetDict(datasets_dict)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def preprocess_function(examples,tokenizer,max_length):\n",
    "    \"\"\"\n",
    "    Proper tokenization function for multi-label classification.\n",
    "    Ensures all outputs are compatible with HuggingFace Trainer.\n",
    "    \"\"\"\n",
    "    # Handle batch vs single example\n",
    "    if isinstance(examples['text'], str):\n",
    "        texts = [examples['text']]\n",
    "        labels = [examples['labels']]\n",
    "    else:\n",
    "        texts = examples['text']\n",
    "        labels = examples['labels']\n",
    "    \n",
    "    # Tokenize the texts\n",
    "    tokenized = tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        padding=True,  # Will be handled by data collator\n",
    "        max_length=max_length,  # Adjust based on your model's limit\n",
    "        return_tensors=None  # Don't return tensors yet, let data collator handle it\n",
    "    )\n",
    "    \n",
    "    # Ensure labels are float32 for BCEWithLogitsLoss\n",
    "    if isinstance(labels[0], (list, np.ndarray)):\n",
    "        tokenized['labels'] = [np.array(label, dtype=np.float32).tolist() for label in labels]\n",
    "    else:\n",
    "        tokenized['labels'] = [np.array(labels, dtype=np.float32).tolist()]\n",
    "    \n",
    "    return tokenized\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"Sigmoid activation function\"\"\"\n",
    "    return 1/(1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "00721051",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_evaluation(y_true, y_pred_proba, y_pred_binary=None, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation for multi-label classification with all averaging methods\n",
    "    \n",
    "    Args:\n",
    "        y_true: Ground truth binary labels (n_samples, n_labels)\n",
    "        y_pred_proba: Predicted probabilities (n_samples, n_labels)\n",
    "        y_pred_binary: Predicted binary labels (n_samples, n_labels), optional\n",
    "        threshold: Threshold for converting probabilities to binary (default: 0.5)\n",
    "    \n",
    "    Returns:\n",
    "        dict: Comprehensive metrics including all averaging methods\n",
    "    \"\"\"\n",
    "    if y_pred_binary is None:\n",
    "        y_pred_binary = (y_pred_proba >= threshold).astype(int)\n",
    "    \n",
    "    metrics = {}\n",
    "    \n",
    "    try:\n",
    "        # SAMPLES AVERAGE (per-sample then average across samples)\n",
    "        # metrics['precision_samples'] = precision_score(y_true, y_pred_binary, average='samples', zero_division=0)\n",
    "        # metrics['recall_samples'] = recall_score(y_true, y_pred_binary, average='samples', zero_division=0)\n",
    "        metrics['f1_samples'] = f1_score(y_true, y_pred_binary, average='samples', zero_division=0)\n",
    "        \n",
    "        # MICRO AVERAGE (global aggregation)\n",
    "        # metrics['precision_micro'] = precision_score(y_true, y_pred_binary, average='micro', zero_division=0)\n",
    "        # metrics['recall_micro'] = recall_score(y_true, y_pred_binary, average='micro', zero_division=0)\n",
    "        metrics['f1_micro'] = f1_score(y_true, y_pred_binary, average='micro', zero_division=0)\n",
    "        \n",
    "        # MACRO AVERAGE (unweighted average across labels)\n",
    "        # metrics['precision_macro'] = precision_score(y_true, y_pred_binary, average='macro', zero_division=0)\n",
    "        # metrics['recall_macro'] = recall_score(y_true, y_pred_binary, average='macro', zero_division=0)\n",
    "        metrics['f1_macro'] = f1_score(y_true, y_pred_binary, average='macro', zero_division=0)\n",
    "        \n",
    "        # WEIGHTED AVERAGE (weighted by support)\n",
    "        # metrics['precision_weighted'] = precision_score(y_true, y_pred_binary, average='weighted', zero_division=0)\n",
    "        # metrics['recall_weighted'] = recall_score(y_true, y_pred_binary, average='weighted', zero_division=0)\n",
    "        metrics['f1_weighted'] = f1_score(y_true, y_pred_binary, average='weighted', zero_division=0)\n",
    "        \n",
    "        # ACCURACY METRICS\n",
    "        metrics['accuracy'] = accuracy_score(y_true, y_pred_binary)\n",
    "        metrics['hamming_loss'] = hamming_loss(y_true, y_pred_binary)\n",
    "        \n",
    "        # JACCARD (IoU) METRICS \n",
    "        metrics['jaccard_samples'] = jaccard_score(y_true, y_pred_binary, average='samples', zero_division=0)\n",
    "        metrics['jaccard_macro'] = jaccard_score(y_true, y_pred_binary, average='macro', zero_division=0)\n",
    "        metrics['jaccard_weighted'] = jaccard_score(y_true, y_pred_binary, average='weighted', zero_division=0)\n",
    "        \n",
    "        # ROC-AUC METRICS (using probabilities)\n",
    "        try:\n",
    "            metrics['roc_auc_micro'] = roc_auc_score(y_true, y_pred_proba, average='micro')\n",
    "            metrics['roc_auc_macro'] = roc_auc_score(y_true, y_pred_proba, average='macro')\n",
    "            metrics['roc_auc_weighted'] = roc_auc_score(y_true, y_pred_proba, average='weighted')\n",
    "            metrics['roc_auc_samples'] = roc_auc_score(y_true, y_pred_proba, average='samples')\n",
    "        except ValueError as e:\n",
    "            print(f\"Warning: ROC-AUC calculation failed: {e}\")\n",
    "            metrics['roc_auc_micro'] = 0.0\n",
    "            metrics['roc_auc_macro'] = 0.0\n",
    "            metrics['roc_auc_weighted'] = 0.0\n",
    "            metrics['roc_auc_samples'] = 0.0\n",
    "        \n",
    "        # PR-AUC METRICS (using probabilities)\n",
    "        try:\n",
    "            metrics['pr_auc_micro'] = average_precision_score(y_true, y_pred_proba, average='micro')\n",
    "            metrics['pr_auc_macro'] = average_precision_score(y_true, y_pred_proba, average='macro')\n",
    "            metrics['pr_auc_weighted'] = average_precision_score(y_true, y_pred_proba, average='weighted')\n",
    "            metrics['pr_auc_samples'] = average_precision_score(y_true, y_pred_proba, average='samples')\n",
    "        except ValueError as e:\n",
    "            print(f\"Warning: PR-AUC calculation failed: {e}\")\n",
    "            metrics['pr_auc_micro'] = 0.0\n",
    "            metrics['pr_auc_macro'] = 0.0\n",
    "            metrics['pr_auc_weighted'] = 0.0\n",
    "            metrics['pr_auc_samples'] = 0.0\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in comprehensive_evaluation: {e}\")\n",
    "        # Return minimal metrics if calculation fails\n",
    "        metrics = {\n",
    "            'precision_micro': 0.0, 'recall_micro': 0.0, 'f1_micro': 0.0,\n",
    "            'precision_macro': 0.0, 'recall_macro': 0.0, 'f1_macro': 0.0,\n",
    "            'accuracy': 0.0, 'hamming_loss': 1.0\n",
    "        }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Enhanced compute_metrics function for transformers Trainer using comprehensive evaluation\n",
    "    \"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    # Apply sigmoid to get probabilities\n",
    "    predictions_proba = sigmoid(predictions)\n",
    "    \n",
    "    # Convert to binary predictions using threshold 0.5\n",
    "    predictions_binary = (predictions_proba > 0.5).astype(int)\n",
    "    \n",
    "    # Ensure labels are integers\n",
    "    labels = labels.astype(int)\n",
    "    \n",
    "    # Use comprehensive evaluation\n",
    "    metrics = comprehensive_evaluation(\n",
    "        y_true=labels,\n",
    "        y_pred_proba=predictions_proba,\n",
    "        y_pred_binary=predictions_binary,\n",
    "        threshold=0.5\n",
    "    )\n",
    "    \n",
    "    # Return metrics with eval_ prefix for Trainer compatibility\n",
    "    return {\n",
    "        # Primary metrics for monitoring\n",
    "        'eval_accuracy': metrics['accuracy'],\n",
    "        'eval_hamming_loss': metrics['hamming_loss'],\n",
    "        'eval_f1_macro': metrics['f1_macro'],\n",
    "        # 'eval_f1_samples': metrics['f1_samples'],\n",
    "        'eval_f1_weighted': metrics['f1_weighted'],\n",
    "\n",
    "        # # Precision metrics\n",
    "        # 'eval_precision_micro': metrics['precision_micro'],\n",
    "        # 'eval_precision_macro': metrics['precision_macro'],\n",
    "        # 'eval_precision_samples': metrics['precision_samples'],\n",
    "        # 'eval_precision_weighted': metrics['precision_weighted'],\n",
    "        \n",
    "        # # Recall metrics\n",
    "        # 'eval_recall_micro': metrics['recall_micro'],\n",
    "        # 'eval_recall_macro': metrics['recall_macro'],\n",
    "        # 'eval_recall_samples': metrics['recall_samples'],\n",
    "        # 'eval_recall_weighted': metrics['recall_weighted'],\n",
    "        \n",
    "        # ROC-AUC metrics\n",
    "        # 'eval_roc_auc_micro': metrics['roc_auc_micro'],\n",
    "        'eval_roc_auc_macro': metrics['roc_auc_macro'],\n",
    "        'eval_roc_auc_weighted': metrics['roc_auc_weighted'],\n",
    "        # 'eval_roc_auc_samples': metrics['roc_auc_samples'],\n",
    "        \n",
    "        # PR-AUC metrics\n",
    "        # 'eval_pr_auc_micro': metrics['pr_auc_micro'],\n",
    "        'eval_pr_auc_macro': metrics['pr_auc_macro'],\n",
    "        'eval_pr_auc_weighted': metrics['pr_auc_weighted'],\n",
    "        # 'eval_pr_auc_samples': metrics['pr_auc_samples'],\n",
    "        \n",
    "        # Jaccard metrics\n",
    "        # 'eval_jaccard_samples': metrics['jaccard_samples'],\n",
    "        'eval_jaccard_macro': metrics['jaccard_macro'],\n",
    "        'eval_jaccard_weighted': metrics['jaccard_weighted'],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "52aedb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClearCUDACacheCallback(TrainerCallback):\n",
    "    def on_step_end(self,args,state,control,**kwargs):\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    def on_evaluate(self,args,state,control,**kwargs):\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "class EarlyStoppingCallback(TrainerCallback):\n",
    "    def __init__(self,patience=1):\n",
    "        super().__init__()\n",
    "        self.patience=patience\n",
    "        self.best_loss = float(\"inf\")\n",
    "        self.early_stop_count = 0\n",
    "\n",
    "    def on_evaluate(self, args, state, control, **kwargs):\n",
    "        # Access the evaluation loss\n",
    "        eval_loss = kwargs[\"metrics\"][\"eval_loss\"]\n",
    "        if eval_loss < self.best_loss:\n",
    "            self.best_loss = eval_loss\n",
    "            self.early_stop_count = 0\n",
    "        else:\n",
    "            self.early_stop_count += 1\n",
    "        if self.early_stop_count >= self.patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            control.should_training_stop = True\n",
    "            \n",
    "class LoggingCallback(TrainerCallback):\n",
    "    def __init__(self,log_file):\n",
    "        super().__init__()\n",
    "        self.log_file=log_file\n",
    "        self.last_train_loss = None\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        logs = logs or {}\n",
    "\n",
    "        # Extract metrics\n",
    "        epoch = round(float(state.epoch), 2) if state.epoch is not None else None\n",
    "        step=int(state.global_step)\n",
    "        # train_loss = logs.get(\"loss\")\n",
    "        # if train_loss is not None:\n",
    "        #     train_loss = round(float(train_loss), 4)\n",
    "        #     self.last_train_loss = train_loss\n",
    "        # elif self.last_train_loss is not None:\n",
    "        #     train_loss = self.last_train_loss\n",
    "        # else:\n",
    "        #     train_loss = \"N/A\"  # or skip logging this time\n",
    "        eval_loss = logs.get(\"eval_loss\")\n",
    "        eval_accuracy = logs.get(\"eval_accuracy\")\n",
    "        eval_hamming_loss = logs.get(\"eval_hamming_loss\")\n",
    "        eval_jaccard_weighted = logs.get(\"eval_jaccard_weighted\")\n",
    "\n",
    "        # Round losses to 4 decimal places if present\n",
    "        # train_loss = round(float(train_loss), 4) if train_loss is not None else None\n",
    "        eval_loss = round(float(eval_loss), 4) if eval_loss is not None else None\n",
    "        eval_accuracy = round(float(eval_accuracy), 4) if eval_accuracy is not None else None\n",
    "        eval_hamming_loss = round(float(eval_hamming_loss), 4) if eval_hamming_loss is not None else None\n",
    "        eval_jaccard_weighted = round(float(eval_jaccard_weighted), 4) if eval_jaccard_weighted is not None else None\n",
    "\n",
    "        # Prepare log line: epoch, train_loss, eval_loss, eval_accuracy, eval_hamming_loss, eval_jaccard_weighted\n",
    "        log_line = (\n",
    "            f\"Epoch: {epoch} | \"\n",
    "            f\"Step: {step} | \"\n",
    "            f\"Val Loss: {eval_loss} | \"\n",
    "            f\"Val Acc: {eval_accuracy} | \"\n",
    "            f\"Val Hamming Loss: {eval_hamming_loss} | \"\n",
    "            f\"Val Jaccard Weighted: {eval_jaccard_weighted}\\n\"\n",
    "        )\n",
    "\n",
    "        # Write to log file\n",
    "        with open(self.log_file, \"a\") as f:\n",
    "            f.write(log_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b9d6f970",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    dataset = create_datasets_from_arrays(X_train, y_train, X_val, y_val, X_test, y_test)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model_path)\n",
    "    print(\"{:<25}{:<15,}\".format(\"Maximal context length:\",tokenizer.model_max_length))\n",
    "    print(\"{:<25}{:<15,}\".format(\"Vocabulary size :\",tokenizer.vocab_size))\n",
    "\n",
    "    # Apply the tokenization function\n",
    "\n",
    "    encode_function = partial(preprocess_function,tokenizer=tokenizer, max_length=args.max_context_length)\n",
    "    \n",
    "    tokenized_dataset = dataset.map(\n",
    "        encode_function,\n",
    "        batched=True,\n",
    "        remove_columns=['text'],  # Remove the problematic text column\n",
    "        desc=\"Tokenizing dataset\"\n",
    "    )\n",
    "    \n",
    "    # Define the proper feature type for multi-label classification\n",
    "    label_feature = Sequence(Value(\"float32\"), length=len(class_name))\n",
    "    \n",
    "    # Cast the labels column to float32 for all splits\n",
    "    for split_name in tokenized_dataset.keys():\n",
    "        tokenized_dataset[split_name] = tokenized_dataset[split_name].cast_column(\"labels\", \n",
    "                                                                                  label_feature)\n",
    "    print(f\"Features: {list(tokenized_dataset['train'].features.keys())}\")\n",
    "\n",
    "\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "    \n",
    "    class2id = {class_:id for id, class_ in enumerate(class_name)}\n",
    "    id2class = {id:class_ for class_, id in class2id.items()}\n",
    "    \n",
    "    \n",
    "    model = AutoModelForSequenceClassification.from_pretrained(args.model_path, \n",
    "                                                               num_labels=len(class_name),\n",
    "                                                               id2label=id2class, \n",
    "                                                               label2id=class2id,\n",
    "                                                               problem_type = \"multi_label_classification\"\n",
    "                                                              )\n",
    "    \n",
    "    \n",
    "    print()\n",
    "    \n",
    "    # Verify model is properly configured for multi-label classification\n",
    "    print(\"🤖 Model Configuration Verification:\")\n",
    "    print(f\"  Model type: {type(model).__name__}\")\n",
    "    print(f\"  Number of labels: {model.config.num_labels}\")\n",
    "    print(f\"  Expected labels: {len(class_name)}\")\n",
    "    \n",
    "    # Check if model configuration matches our data\n",
    "    if model.config.num_labels != len(class_name):\n",
    "        print(f\"⚠️ WARNING: Model expects {model.config.num_labels} labels, but data has {len(class_name)}\")\n",
    "        print(\"  This might cause issues during training\")\n",
    "    else:\n",
    "        print(f\"✅ Model configuration matches data: {len(class_name)} labels\")\n",
    "    \n",
    "    # Verify model parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"\\n📊 Model Parameters:\")\n",
    "    print(f\"  Total parameters: {total_params:,}\")\n",
    "    print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "    # Fix tokenizer parallelism warning\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        # Output and logging\n",
    "        output_dir=args.output_dir,\n",
    "        # logging_dir=\"./logs\",\n",
    "        # logging_steps=100,\n",
    "        logging_strategy=\"no\", ## we customize logging intead of using the built-in logging\n",
    "        \n",
    "        # Learning parameters\n",
    "        learning_rate=2e-5,\n",
    "        lr_scheduler_type=\"linear\",  # Linear decay\n",
    "        warmup_ratio=0.1,  # 10% warmup\n",
    "        weight_decay=0.01,\n",
    "        \n",
    "        # Batch sizes (adjust based on GPU memory)\n",
    "        per_device_train_batch_size=args.train_batch,\n",
    "        per_device_eval_batch_size=args.eval_batch,\n",
    "        gradient_accumulation_steps=args.gradient_accumulation_step,  # Effective batch size = 4 * 12 = 48\n",
    "\n",
    "        # Training epochs and evaluation\n",
    "        num_train_epochs=args.num_epochs,  # Increased for better convergence\n",
    "        eval_strategy=\"steps\",  # More frequent evaluation\n",
    "        eval_steps=100,  # Evaluate every 100 steps\n",
    "        \n",
    "        # 🎯 OPTIMAL METRICS FOR MULTI-LABEL CLASSIFICATION\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=100,\n",
    "        save_total_limit=3,  # Keep only 3 best checkpoints\n",
    "        load_best_model_at_end=True,\n",
    "        \n",
    "        # 🔥 RECOMMENDED: Use Hamming Loss for multi-label problems\n",
    "        metric_for_best_model=\"eval_hamming_loss\",  # Primary metric: lower is better\n",
    "        greater_is_better=False,  # Hamming loss: lower = better performance\n",
    "        \n",
    "        # Alternative good options:\n",
    "        # metric_for_best_model=\"eval_f1_micro\",     # Current choice - also excellent\n",
    "        # metric_for_best_model=\"eval_jaccard_samples\", # IoU metric - good for multi-label\n",
    "        \n",
    "        # Memory and performance optimization\n",
    "        dataloader_pin_memory=False,  # Disable to avoid forking issues\n",
    "        dataloader_num_workers=0,     # Disable multiprocessing\n",
    "        remove_unused_columns=False,  # Keep all columns for multi-label\n",
    "        \n",
    "        # Mixed precision for faster training (if GPU supports it)\n",
    "        fp16=True,  # Enable if using compatible GPU\n",
    "        \n",
    "        # Reproducibility\n",
    "        seed=args.seed,\n",
    "        data_seed=args.seed,\n",
    "        \n",
    "        # Report metrics\n",
    "        report_to=None,  # Disable wandb/tensorboard if not needed\n",
    "        run_name=\"multi_label_posture_classification\",\n",
    "    )\n",
    "    # # Early stopping callback for overfitting control\n",
    "    # early_stopping = EarlyStoppingCallback(\n",
    "    #     early_stopping_patience=3,  # Stop if no improvement for 3 evaluations\n",
    "    #     early_stopping_threshold=0.001  # Minimum improvement threshold\n",
    "    # )\n",
    "    \n",
    "    # Initialize trainer with enhanced configuration (using processing_class)\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset[\"train\"],\n",
    "        eval_dataset=tokenized_dataset[\"val\"],\n",
    "        processing_class=tokenizer,  # Updated parameter name\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(patience=3),\n",
    "                   ClearCUDACacheCallback(),\n",
    "                   LoggingCallback(log_file=os.path.join(args.output_dir,\"training_logs.txt\"))],  # Add early stopping callback\n",
    "    )    \n",
    "\n",
    "    print(\"\\n🎯 Starting training...\")\n",
    "    trainer.train()\n",
    "    print(\"✅ Training completed successfully!\")\n",
    "\n",
    "    val_results = trainer.evaluate()\n",
    "\n",
    "    # Display key metrics\n",
    "    key_metrics = [\n",
    "        'eval_accuracy', 'eval_hamming_loss', 'eval_f1_micro', 'eval_f1_macro', \n",
    "        'eval_f1_weighted','eval_f1_samples', 'eval_jaccard_macro', \n",
    "        'eval_jaccard_weighted'\n",
    "    ]\n",
    "\n",
    "    with open(os.path.join(args.output_dir,\"val_logs.txt\"), \"w\") as f:\n",
    "        for metric in key_metrics:\n",
    "            if metric in val_results:\n",
    "                line = f\"{metric}: {val_results[metric]:.4f}\"\n",
    "                print(f\"   {line}\")\n",
    "                f.write(line + \"\\n\")\n",
    "\n",
    "    if \"test\" in tokenized_dataset:\n",
    "        print(\"\\n🎯 Test Set Evaluation:\")\n",
    "        test_results = trainer.evaluate(eval_dataset=tokenized_dataset[\"test\"])\n",
    "        with open(os.path.join(args.output_dir,\"test_logs.txt\"), \"w\") as f:\n",
    "            for metric in key_metrics:\n",
    "                if metric in test_results:\n",
    "                    line = f\"{metric}: {test_results[metric]:.4f}\"\n",
    "                    print(f\"   {line}\")\n",
    "                    f.write(line + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e7689b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximal context length:  8,192          \n",
      "Vocabulary size :        50,280         \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f979efa11fd148fb8508b03e270d9b22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing dataset:   0%|          | 0/11597 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4351e6b860614fe9819334b13ddac4d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing dataset:   0%|          | 0/2485 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8db7f93d4f84b3bbaa3f9909f4ef480",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing dataset:   0%|          | 0/2486 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b95f67e7a9c641619f0742c3b10a27fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/11597 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d91d609da2434d569c9cebf39180ba61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/2485 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdc7787e60c44207b1aeac2beb001a22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/2486 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: ['labels', 'input_ids', 'attention_mask']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ModernBertForSequenceClassification were not initialized from the model checkpoint at answerdotai/ModernBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🤖 Model Configuration Verification:\n",
      "  Model type: ModernBertForSequenceClassification\n",
      "  Number of labels: 27\n",
      "  Expected labels: 27\n",
      "✅ Model configuration matches data: 27 labels\n",
      "\n",
      "📊 Model Parameters:\n",
      "  Total parameters: 149,625,627\n",
      "  Trainable parameters: 149,625,627\n",
      "\n",
      "🎯 Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1210' max='1210' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1210/1210 31:50, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Hamming Loss</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "      <th>Roc Auc Macro</th>\n",
       "      <th>Roc Auc Weighted</th>\n",
       "      <th>Pr Auc Macro</th>\n",
       "      <th>Pr Auc Weighted</th>\n",
       "      <th>Jaccard Macro</th>\n",
       "      <th>Jaccard Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.108869</td>\n",
       "      <td>0.324346</td>\n",
       "      <td>0.036277</td>\n",
       "      <td>0.098176</td>\n",
       "      <td>0.544194</td>\n",
       "      <td>0.757261</td>\n",
       "      <td>0.889413</td>\n",
       "      <td>0.177906</td>\n",
       "      <td>0.664161</td>\n",
       "      <td>0.077208</td>\n",
       "      <td>0.453510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.075353</td>\n",
       "      <td>0.500201</td>\n",
       "      <td>0.024547</td>\n",
       "      <td>0.191859</td>\n",
       "      <td>0.679791</td>\n",
       "      <td>0.874292</td>\n",
       "      <td>0.951123</td>\n",
       "      <td>0.344305</td>\n",
       "      <td>0.774936</td>\n",
       "      <td>0.150330</td>\n",
       "      <td>0.598609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.068367</td>\n",
       "      <td>0.543260</td>\n",
       "      <td>0.022938</td>\n",
       "      <td>0.347228</td>\n",
       "      <td>0.738232</td>\n",
       "      <td>0.925669</td>\n",
       "      <td>0.962463</td>\n",
       "      <td>0.446651</td>\n",
       "      <td>0.801689</td>\n",
       "      <td>0.268993</td>\n",
       "      <td>0.651860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.061599</td>\n",
       "      <td>0.554930</td>\n",
       "      <td>0.021149</td>\n",
       "      <td>0.355828</td>\n",
       "      <td>0.737189</td>\n",
       "      <td>0.940895</td>\n",
       "      <td>0.966702</td>\n",
       "      <td>0.521841</td>\n",
       "      <td>0.819050</td>\n",
       "      <td>0.280129</td>\n",
       "      <td>0.660274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.059241</td>\n",
       "      <td>0.574648</td>\n",
       "      <td>0.020762</td>\n",
       "      <td>0.451176</td>\n",
       "      <td>0.761073</td>\n",
       "      <td>0.946890</td>\n",
       "      <td>0.968905</td>\n",
       "      <td>0.581514</td>\n",
       "      <td>0.837406</td>\n",
       "      <td>0.352403</td>\n",
       "      <td>0.675321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.056793</td>\n",
       "      <td>0.591147</td>\n",
       "      <td>0.019674</td>\n",
       "      <td>0.480668</td>\n",
       "      <td>0.784267</td>\n",
       "      <td>0.951841</td>\n",
       "      <td>0.970376</td>\n",
       "      <td>0.598930</td>\n",
       "      <td>0.840041</td>\n",
       "      <td>0.381261</td>\n",
       "      <td>0.701362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.055119</td>\n",
       "      <td>0.592354</td>\n",
       "      <td>0.019510</td>\n",
       "      <td>0.527601</td>\n",
       "      <td>0.786582</td>\n",
       "      <td>0.958111</td>\n",
       "      <td>0.971808</td>\n",
       "      <td>0.616069</td>\n",
       "      <td>0.848114</td>\n",
       "      <td>0.406824</td>\n",
       "      <td>0.699286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.054728</td>\n",
       "      <td>0.610865</td>\n",
       "      <td>0.018735</td>\n",
       "      <td>0.536085</td>\n",
       "      <td>0.799105</td>\n",
       "      <td>0.956394</td>\n",
       "      <td>0.970696</td>\n",
       "      <td>0.629109</td>\n",
       "      <td>0.848413</td>\n",
       "      <td>0.420717</td>\n",
       "      <td>0.710891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.054200</td>\n",
       "      <td>0.615694</td>\n",
       "      <td>0.018451</td>\n",
       "      <td>0.541879</td>\n",
       "      <td>0.806194</td>\n",
       "      <td>0.954730</td>\n",
       "      <td>0.970917</td>\n",
       "      <td>0.642234</td>\n",
       "      <td>0.852992</td>\n",
       "      <td>0.424259</td>\n",
       "      <td>0.720777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.053624</td>\n",
       "      <td>0.621328</td>\n",
       "      <td>0.018213</td>\n",
       "      <td>0.587482</td>\n",
       "      <td>0.818676</td>\n",
       "      <td>0.956112</td>\n",
       "      <td>0.971402</td>\n",
       "      <td>0.637450</td>\n",
       "      <td>0.852427</td>\n",
       "      <td>0.461486</td>\n",
       "      <td>0.730777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.054034</td>\n",
       "      <td>0.615292</td>\n",
       "      <td>0.018213</td>\n",
       "      <td>0.576042</td>\n",
       "      <td>0.812174</td>\n",
       "      <td>0.955955</td>\n",
       "      <td>0.970973</td>\n",
       "      <td>0.640449</td>\n",
       "      <td>0.852279</td>\n",
       "      <td>0.453131</td>\n",
       "      <td>0.725022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.053865</td>\n",
       "      <td>0.617706</td>\n",
       "      <td>0.018213</td>\n",
       "      <td>0.577485</td>\n",
       "      <td>0.814890</td>\n",
       "      <td>0.954598</td>\n",
       "      <td>0.970856</td>\n",
       "      <td>0.639752</td>\n",
       "      <td>0.852321</td>\n",
       "      <td>0.452617</td>\n",
       "      <td>0.727075</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Training completed successfully!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='622' max='311' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [311/311 00:30]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered\n",
      "   eval_accuracy: 0.6213\n",
      "   eval_hamming_loss: 0.0182\n",
      "   eval_f1_macro: 0.5875\n",
      "   eval_f1_weighted: 0.8187\n",
      "   eval_jaccard_macro: 0.4615\n",
      "   eval_jaccard_weighted: 0.7308\n",
      "\n",
      "🎯 Test Set Evaluation:\n",
      "Early stopping triggered\n",
      "   eval_accuracy: 0.6102\n",
      "   eval_hamming_loss: 0.0190\n",
      "   eval_f1_macro: 0.5631\n",
      "   eval_f1_weighted: 0.8102\n",
      "   eval_jaccard_macro: 0.4437\n",
      "   eval_jaccard_weighted: 0.7220\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    parser = argparse.ArgumentParser(description='Fine-tune MordenBERT')\n",
    "\n",
    "    parser.add_argument(\"--seed\",  type=int,default=42)\n",
    "    parser.add_argument(\"--data_path\", type=str, default='processed_data')\n",
    "    parser.add_argument(\"--output_dir\", type=str, default='model_output')\n",
    "    parser.add_argument('--model_path', type=str, default=\"answerdotai/ModernBERT-base\")\n",
    "    parser.add_argument('--train_batch', type=int, default=4)\n",
    "    parser.add_argument('--eval_batch', type=int, default=8)\n",
    "    parser.add_argument('--gradient_accumulation_step', type=int, default=12)\n",
    "    parser.add_argument('--num_epochs', type=int, default=5)\n",
    "    parser.add_argument('--max_context_length', type=int, default=512)\n",
    "    \n",
    "    args, _= parser.parse_known_args()\n",
    "\n",
    "    seed_everything(args.seed)\n",
    "\n",
    "    ### Load Dataset for model training and evaluation ###\n",
    "    data_path=os.path.join(os.getcwd(), args.data_path)\n",
    "    with open(os.path.join(data_path,'train_arrays.pkl'), 'rb') as f:\n",
    "        train_data = pickle.load(f)\n",
    "        X_train = train_data['X_train']\n",
    "        y_train = train_data['y_train']\n",
    "    \n",
    "    with open(os.path.join(data_path,'val_arrays.pkl'), 'rb') as f:\n",
    "        val_data = pickle.load(f)\n",
    "        X_val = val_data['X_val']\n",
    "        y_val = val_data['y_val']\n",
    "    \n",
    "    with open(os.path.join(data_path,'test_arrays.pkl'), 'rb') as f:\n",
    "        test_data = pickle.load(f)\n",
    "        X_test = test_data['X_test']\n",
    "        y_test = test_data['y_test']\n",
    "    \n",
    "    with open(os.path.join(data_path,'class_name.pkl'), 'rb') as f:\n",
    "        class_name_data = pickle.load(f)\n",
    "        class_name = class_name_data['class_name']\n",
    "\n",
    "    main(args)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a12253",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bada12bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a304a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20ff6c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2614be9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ba87f2c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TR-Project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
